<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>必装软件</title>
      <link href="/2025/06/software_must.html"/>
      <url>/2025/06/software_must.html</url>
      
        <content type="html"><![CDATA[<h1 id="必装软件"><a href="#必装软件" class="headerlink" title="必装软件"></a>必装软件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Core Temp setup</span><br><span class="line">Snipaste</span><br><span class="line">WindowTop</span><br><span class="line">qbittorrent</span><br><span class="line">Simplenote</span><br><span class="line">Everything</span><br><span class="line">AutoHotkey</span><br><span class="line">Simplenote</span><br><span class="line">Xmind</span><br></pre></td></tr></table></figure><h2 id="办公软件"><a href="#办公软件" class="headerlink" title="办公软件"></a>办公软件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">office</span><br><span class="line">visio</span><br><span class="line">福昕</span><br><span class="line">edge 火狐 谷歌 星火 </span><br><span class="line">Zotero </span><br><span class="line">Pycharm anaconda VScode Matlab </span><br><span class="line">Comsol</span><br><span class="line">typora typora代码块默认语言 Pingo Github desktop中文版 Clash.Verge</span><br></pre></td></tr></table></figure><h2 id="娱乐软件"><a href="#娱乐软件" class="headerlink" title="娱乐软件"></a>娱乐软件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">播放器</span><br><span class="line">百度云 阿里云 <span class="number">123</span>云盘 onedrive</span><br><span class="line">Todesk 贝锐蒲公英 </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>第五卷</title>
      <link href="/2024/11/Di_wu_juan.html"/>
      <url>/2024/11/Di_wu_juan.html</url>
      
        <content type="html"><![CDATA[<h1 id="第五卷"><a href="#第五卷" class="headerlink" title="第五卷"></a>第五卷</h1><h2 id="出版说明"><a href="#出版说明" class="headerlink" title="出版说明"></a>出版说明</h2><p>伟大的领袖和导师毛泽东主席的著作，是马克思列宁主<br>义的不朽文献。根据中共中央的决定，《毛泽东选集》第五卷现在出版了，以后各卷也将陆续出版。<br>过去出版的《毛泽东选集》第一卷至第四卷，是新民主主义革命时期的重要著作。第五卷和以后各卷，是社会主义革命和社会主义建设时期的重要著作。<br>在中华人民共和国成立以后的新的历史时期，毛泽东同<br>志坚持马克思列宁主义普遍真理和革命具体实践相结合的一贯原则，领导我党和我国人民，在进行社会主义革命和社会主义建设的斗争中，在反对高饶、彭德怀、刘少奇、林彪、王张江姚的修正主义路线的斗争中，在反对帝国主义和各国反动派的斗争中，在反对以苏修叛徒集团为中心的现代修正主义的斗争中，继承、捍卫和发展了马克思列宁主义。这个时期，毛泽东同志在理论上最伟大的贡献，就是系统地总结了我国的和国际的无产阶级专政的历史经验，运用唯物辩证法的对立统一这个基本观点，分析了社会主义社会的矛盾、阶级和阶级斗争，从而揭示了社会主义社会的发展规律，创立了无产阶级专政下继续革命的伟大理论。毛泽东同志关于无产阶级革命和无产阶级专政的这种新思想、新结论，在哲学、政治经济学和科学社会主义方面，极大地丰富了马克思列宁主义的理论宝库。它不仅为我国人民指明了巩固无产阶级专政，防止资本主义复辟，建设社会主义的根本道路，而且具有伟大的深远的世界意义。<br>《毛泽东选集》第五卷是一九四九年九月到一九五七年的重要著作。毛泽东同志关于在生产资料所有制的社会主义改造基本完成以后无产阶级和资产阶级、社会主义道路和资本主义道路的斗争还长期存在的科学论断，关于正确区分和处理社会主义社会中敌我矛盾和人民内部矛盾这两类不同性质的矛盾的学说，关于无产阶级专政下继续革命的伟大理论，关于社会主义建设总路线的基本思想，就是在这卷著作中首次提出的。以后，特别是在无产阶级文化大革命中，毛泽东同志根据革命实践经验，不断地充实和发展了这些光辉思想。<br>毛泽东同志是当代最伟大的马克思列宁主义者。毛泽东<br>思想是我党我军和我国人民团结战斗、继续革命的胜利旗帜，是国际无产阶级和各国革命人民的共同财富。毛泽东同志的思想和学说是永存的。<br>收入选集的毛泽东同志在社会主义革命和社会主义建设<br>时期的著作，有一部分公开发表过，有一部分没有公开发表过，包括毛泽东同志起草的文件、手稿和讲话的正式记录。讲话记录在编辑时作了必要的技术性的整理。<br><code>中共中央毛泽东主席著作编辑出版委员会</code><br>一九七七年三月一日</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>英文写作</title>
      <link href="/2024/07/english_learn.html"/>
      <url>/2024/07/english_learn.html</url>
      
        <content type="html"><![CDATA[<hr><h1 id="英文写作"><a href="#英文写作" class="headerlink" title="英文写作"></a>英文写作</h1><hr><p><code>中文润色</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">作为一名中文学术论文写作改进助理，你的任务是改进所提供文本的拼写、语法、清晰、简洁和整体可读性，同时分解长句，减少重复，并提供改进建议。请先提供文本的更正版本，然后在markdown表格中列出修改的内容，并给出修改的理由:</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>英文润色</code>：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Below is a paragraph from an academic paper. Polish the writing to meet the academic style,improve the spelling, grammar, clarity, concision and overall readability. When necessary, rewrite the whole sentence. Furthermore, list all modification and explain the reasons to do so in markdown table. Paragraph ：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>学术-中文转英文</code>:</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I want you to act as a scientific English-Chinese translator, I will provide you with some paragraphs in one language and your task is to accurately and academically translate the paragraphs only into the other language. Do not repeat the original provided paragraphs after translation. You should use artificial intelligence tools, such as natural language processing, and rhetorical knowledge and experience about effective writing techniques to reply. I&#x27;ll give you my paragraphs as follows, tell me what language it is written in, and then translate：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>查找语法错误</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Help me ensure that the grammar <span class="keyword">and</span> the spelling <span class="keyword">is</span> correct. Do <span class="keyword">not</span> <span class="keyword">try</span> to polish the text, <span class="keyword">if</span> no mistake <span class="keyword">is</span> found, tell me that this paragraph <span class="keyword">is</span> good.If you find grammar <span class="keyword">or</span> spelling mistakes, please <span class="built_in">list</span> mistakes you find <span class="keyword">in</span> a two-column markdown table,put the original text the first column,put the corrected text <span class="keyword">in</span> the second column <span class="keyword">and</span> highlight the key words you fixed.Finally, please provide the proofreaded text.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>英-中互译</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">你是经验丰富的翻译，请把以下学术文章段落翻译成中文，并同时充分考虑中文的语法、清晰、简洁和整体可读性，必要时，你可以修改整个句子的顺序以确保翻译后的段落符合中文的语言习惯。你需要翻译的文本如下:</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I want you to act <span class="keyword">as</span> a scientific English-Chinese translator, I will provide you <span class="keyword">with</span> some paragraphs <span class="keyword">in</span> one language,<span class="keyword">and</span> your task <span class="keyword">is</span> to accurately <span class="keyword">and</span> academically translate the paragraphs only into the other language.Do <span class="keyword">not</span> repeat the original provided paragraphs after translation.You should use artificial intelligence tools, such <span class="keyword">as</span> natural language processing, <span class="keyword">and</span> rhetorical knowledge <span class="keyword">and</span> experience about effective writing techniques to reply.I<span class="string">&#x27;ll give you my paragraphs as follows, tell me what language it is written in, and then translate:</span></span><br><span class="line"><span class="string">    </span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 英文写作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KAN网络</title>
      <link href="/2024/07/kan_model.html"/>
      <url>/2024/07/kan_model.html</url>
      
        <content type="html"><![CDATA[<hr><h1 id="AI大讲堂：深度学习要变天？专业拆解【KAN网络】"><a href="#AI大讲堂：深度学习要变天？专业拆解【KAN网络】" class="headerlink" title="AI大讲堂：深度学习要变天？专业拆解【KAN网络】"></a>AI大讲堂：深度学习要变天？专业拆解【KAN网络】</h1><p><img src="https://s2.loli.net/2024/06/20/Nz5EFfrxpJPMbgy.png" alt="image-20240620161210708"></p><p>人话讲论文系列：KAN模型（上）.pdf</p><p>MLP本质回顾</p><p>$f(x)&#x3D;wx+b$</p><h2 id="KAN网络为什么牛逼？"><a href="#KAN网络为什么牛逼？" class="headerlink" title="KAN网络为什么牛逼？"></a>KAN网络为什么牛逼？</h2><p>Kolmogorov-Arnold Networks顾名思义基于柯尔莫果洛夫-阿诺尔德表示定理。是由这两个俄罗斯数学家1957年提出的如何用一组较简单的函数来表示任何一个多变量的连续函数。</p><p>$f(\mathbf{x})&#x3D;\sum_{q&#x3D;1}^{2n+1}\Phi_q\left(\sum_{p&#x3D;1}^n\phi_{q,p}(x_p)\right)$</p><p>想象一下，你有一个非常复杂的配方，需要各种各样的原料和步骤来制作一道菜。柯尔莫果洛夫阿诺尔德表示定理告诉我们，无论这个配方多么复杂，我们总能找到一种方法，通过一些简单的基本步骤（这里是一些基本的函数）来重现这道菜的味道。</p><p>在上面的式子中，输入是×，$\phi q$,$p(xp)$是基本的一元函数，就像是青椒西红柿基本原料的处理。内层求和就是放到一起。$\phi q$是外层的函数，各自接受内层求和的结果作为输入。外层的求和∑表示整个函数灯是子函数中q的和。用图来表示就相当于一个两层的神经网络，区别在于一没了线性组合，而是直接对输入进行激活：二来这些激活函数不是固定的，而是可以学习的。</p><p><img src="https://s2.loli.net/2024/06/20/nJIUH6pt3bCAkDZ.png" alt="image-20240620163842529"></p><ol><li>learnable activation functions on deges</li><li>sum opearation on nodes</li></ol><p>和MLP每层统一进行非线性空间变换相比，这相当于对每个坐标轴单独进行非线性变换，然后再组合形成多维度空间。（画个简图，先组合再变形和先单个变形再简单组<br>合的区别）</p><p>公式写成向量的形式就是</p><p>$\mathrm{KAN}(\mathbf{x})&#x3D;(\mathbf{\Phi}<em>{3}\circ\mathbf{\Phi}</em>{2}\circ\mathbf{\Phi}_{1})(\mathbf{x})$</p><p>$\mathrm{MLP}(\mathbf{x})&#x3D;(\mathbf{W}<em>{3}\circ\sigma</em>{2}\circ\mathbf{W}<em>{2}\circ\sigma</em>{1}\circ\mathbf{W}_{1})(\mathbf{x})$</p><p>对比MLP，没有了激活函数和参数矩阵的嵌套关系，而直接是非线性函数φ的嵌套。<br>对于多层网络，这相当于下面的结构：</p><p><img src="https://s2.loli.net/2024/06/20/M15Itl2zyxoS3ua.png" alt="image-20240620164411364"></p><p>注意这里所有的非线性函数Φ都采用同样的函数结构，只是用不同的参数来控制其形状。具体<br>来说，文章选择了数值分析中的样条函数<code>spline</code>。这个英语单词spline来源于可变形的样条<br>工具，那是一种在造船和工程制图时用来画出光滑形状的工具。</p><p><img src="https://s2.loli.net/2024/06/20/s5JUluEXC48aP1e.png" alt="image-20240620164659721"></p><p>样条函数的研究始于20世纪中叶，到了60年代它与计算机铺助设计相结合，在外形设计方面得到成功的应用。样条理论已成为函数逼近的有力工具。</p><p>对比MLP和KAN,最大的区别就是变固定的非线性激活+线性参数学习为直接对参数化的非线性激活函数的学习。因为参数本身的复杂度，显然单个spie函数的学习难度要比线性函数难，但KANs通常允许比MLPs更小的计算图，也就是实现同样效果，需要的网络规模更小。例如，文章展示了在解偏微分方程(PDE)的过程中，一个2层宽度为10的KAN比一个4层宽度为100的MLP具有更高的准确度（均方误差10^-7对比10^-5)并且具有更高的参数效率(参数数量100对比10000)。</p><p>到这里为止，你一定好奇，这DEA不复杂啊，难道以前没人想到，有，但是卡壳在都坚持使用原始的二层宽度为(2+1)的表示方法，并没有机会利用更现代的技术（例如，反向传播）来训川练网络。KAN模型的贡献就在于通过进一步简化推广到任意宽度和深度，同时通过广泛的实证实验论证了在A!+科学方面的效果，而且具备很好的准确性和可解释性。这就牛逼了啊，深度学习最大的问题就是个黑盒子，训练网络像是炼丹。大模型越弄越大，很可能一条道走到黑就进死胡同了。好比芯片的摩尔定律。现在出现了量子芯片，原理上就不同，从而有可能实现根本性的变革。当然，原来的各种网络结构还能平替重做一遍，有没有感觉一片Al新大陆向你招手了。我一直劝大家别太短视，成天只盯着transformer,大模型兜兜转转，撑死了也是井中之蛙。</p><p>好，咱们接着走进技术细节，来把KAN网络扒光。</p><p><img src="https://s2.loli.net/2024/06/20/7vwgAEYTnoNkxsK.png" alt="image-20240620165132329"></p><p>整篇文章用了这样一张图来装逼，大部分都是废话，关键词就是仁：<code>数学上有据可依</code>，用实<br>验分别证明了<code>准确性</code>和<code>可解释性</code>。</p><h2 id="四、KAN的架构细节"><a href="#四、KAN的架构细节" class="headerlink" title="四、KAN的架构细节"></a>四、KAN的架构细节</h2><h2 id="4-1详细解释"><a href="#4-1详细解释" class="headerlink" title="4.1详细解释"></a>4.1详细解释</h2><p>整个网络架构原理看图一目了然。很多个这种类以四分之三个周期的正弦函数组合起来就能拟<br>合任意形状的函数。换句话说，用B-spline这一种激活函数两次求和就够了。</p><p><img src="https://s2.loli.net/2024/06/20/hTgqElxaGNvCJAS.png" alt="image-20240620165816809"></p><p>图中展示的结构中，使用了两种尺度或分辩率的组合：粗粒度和细粒度网格，在保持<br>计算效率的同时，更加精确地捕捉和适应函数的变化。这种基础结构其实并不是很难<br>想到，以前就有了，但难点是怎么把它变深，否则单靠这么点玩意儿是不能逼近复杂<br>函数的。这就是本文的主要贡献了。</p><p>要构建深层KAN,首先需要回答：“什么是KAN层？”简单说，它就是一个一维函数矩<br>阵。</p><p>$\Phi&#x3D;{\phi_{q,p}},\quad p&#x3D;1,2,\cdots,n_{\mathrm{in}},\quad q&#x3D;1,2\cdots,n_{\mathrm{out}},$</p><p>这样以来，前面的两层KAN网络第一层输入有n个，输出为2n+1个，第二层输入为<br>2+1,最终输出为1个。想更深就简单堆叠好了，和MLP一个球样。简单说就是找<br>层输入输出之间的转移矩阵</p><p>$\mathbf{x}<em>{l+1}&#x3D;\underbrace{\begin{pmatrix}\phi</em>{l,1,1}(\cdot)&amp;\phi_{l,1,2}(\cdot)&amp;\cdots&amp;\phi_{l,1,n_l}(\cdot)\\phi_{l,2,1}(\cdot)&amp;\phi_{l,2,2}(\cdot)&amp;\cdots&amp;\phi_{l,2,n_l}(\cdot)\\vdots&amp;\vdots&amp;&amp;\vdots\\phi_{l,n_{l+1},1}(\cdot)&amp;\phi_{l,n_{l+1},2}(\cdot)&amp;\cdots&amp;\phi_{l,n_{l+1},n_l}(\cdot)\end{pmatrix}}_{\Phi_l}\mathbf{x}_l,$</p><p>这里<code>l</code>是层编号，右边为输入，左边为输出。看上面左图就大致明白对应关系，输入为2个，因此第二层是2×2+1&#x3D;5个。，订就是每条边上的激活函数，也就是非线性变<br>换。相当于每个×都有5个分身，然后再分别组合。其中i用来标记当前层的节点，而j用来标记下一层的节点。每个节点x_,i的输出通过激活函数中_，，j处理后，贡献到所有下一层的x+1,的计算中。对应上面左图，输入层2个节点，第二层5个节点，因此矩阵为5*2。矩阵的第一列表示×0,1对应的5个激活函数，第二列对应X0,2的，然后两两组合。</p><p>因此，这里需要强调的是KAN网络层节点数不是随便搞的，由输入节点个数确定2n+1个，然后所需要的参数或者连接数为(2n+1)*n,明显比全连接少了不少，看图就知道。</p><p>进而把多层函数级联关系写成矩阵形式就是:</p><p>$\mathrm{KAN}(\mathbf{x})&#x3D;(\boldsymbol{\Phi}<em>{L-1}\circ\boldsymbol{\Phi}</em>{L-2}\circ\cdots\circ\boldsymbol{\Phi}_1\circ\boldsymbol{\Phi}_0)\mathbf{x}.\quad(2.7)$</p><p>或者展开的形式</p><p>$f(\mathbf{x})&#x3D;\sum_{i_{L-1}&#x3D;1}^{n_{L-1}}\phi_{L-1,i_{L},i_{L-1}}\left(\sum_{i_{L-2}&#x3D;1}^{n_{L-2}}\cdots\left(\sum_{i_{2}&#x3D;1}^{n_{2}}\phi_{2,i_{3},i_{2}}\left(\sum_{i_{1}&#x3D;1}^{n_{1}}\phi_{1,i_{2},i_{1}}\left(\sum_{i_{0}&#x3D;1}^{n_{0}}\phi_{0,i_{1},i_{0}}(x_{i_{0}})\right)\right)\right)\cdots\right),$</p><p>再小结一下，原始的两层KAN网络的形状是<code>[n,2n+1,1]</code>,现在变成了<code>多层级联</code>。这里貌似进一步又放开了2+1的结构限制，甚至隐层节点数可以自由发挥。</p><p><img src="https://s2.loli.net/2024/06/20/qiENmtzk954Mr76.png" alt="image-20240620170705266"></p><p>比如这个图中显然隐层个数不是2+1，但依然是各自对应位置的求和，而不是全连<br>接。<br><code>执行细节</code><br>1.<code>残差激活函数</code>：我们包含了一个基础函数b(x)（类似于残差连接中的函数），使<br>得激活函数(x)是基础函数b(x)和样条函数的总和：</p><p>$\phi(x)&#x3D;w(b(x)+\mathrm{spline}(x)).$</p><p>我们通常设置</p><p>$b(x)&#x3D;\text{silu}(x)&#x3D;\frac x{1+e^{-x}}$</p><p>样条函数通常参数化为B样条的线性组合：</p><p>$\mathrm{spline}(x)&#x3D;\sum_ic_iB_i(x)$</p><p>其中c:是可训练的。原则上w是多余的，因为它可以被吸收进b(x)和spline(c).<br>然而，我们仍然包括这个w因子以更好地控制激活函数的整体大小。</p><p>SiLU(Sigmoid Linear Unit)是一种神经网络激活函数，也被称为Swish函数。这个函数由一篇Google Brain的论文首次提出，并因其在某些任务上表现出的优异性能而受到关注。你可以认为它就是sigmoid函数的一种变体。</p><p>2.假设层宽相等，L层，每层N个节点。<br>2.每个样条函数的阶数通常为k&#x3D;3,在G个区间上G+1个网格点。“G个区间”指的是样条函数的分段定义的区间数。</p><p>那么总共大约有O(N2L(G+k)或O(N2LG)个参数。相比之下，具有深度L和宽度N的多层感知机(MLP)只需要O(N2L)个参数，这看起来比KAN更有效率。</p><p>也就是说单看计算复杂度好像KAN还不如MLP简单，但是幸运的是，KANs通常需要<br>比MLPs小得多的NN,这不仅节省了参数，而且还提高了泛化能力，并且有助于解<br>释性。<br>换句话理解，就是借助spline样条函数的表达能力，无需很多节点就能实现比较强的<br>表达能力，因此总的来说，可以比MLP节省不少参数量。</p><h2 id="4-2逼近能力和缩放定律的讨论"><a href="#4-2逼近能力和缩放定律的讨论" class="headerlink" title="4.2逼近能力和缩放定律的讨论"></a>4.2逼近能力和缩放定律的讨论</h2><p>文章花了一页的篇幅推导证明了定理</p><p>$\begin{aligned}&amp;\textbf{Theorem 2.1 (Approximation theory, KAT). Let x}&#x3D;(x_{1},x_{2},\cdots,x_{n}).\textit{Suppose that a function}\&amp;f(\mathbf{x})admitsarepresentation\end{aligned}$</p><p>$f&#x3D;(\Phi_{L-1}\circ\Phi_{L-2}\circ\cdots\circ\Phi_{1}\circ\Phi_{0})\mathbf{x},\quad(2.14)$</p><p>as in Eq.(2.7),where each one of the ii are (k+1)-times continuously differentiable.Then<br>there exists a constant C depending on f and its representation,such that we have the following<br>approximation bound in terms of the grid size G:there exist k-th order B-spline functions<br>such that for any0≤m≤k,we have the bound</p><p>$|f-(\Phi_{L-1}^G\circ\Phi_{L-2}^G\circ\cdots\circ\Phi_1^G\circ\Phi_0^G)\mathbf{x}|_{C^m}\leq CG^{-k-1+m}.\quad(2.15)$</p><p>这部分讲的不是人能听懂的话，看不懂很正常。简单说，就是从数学上证明可以通过构建多层的B样条函数网络来有效逼近复杂函数。尽管增加网络的深度和复杂度，KANs能够通过细致的网格划分来逼近高维函数，而不会受到维数灾难的影响，也就是在高维空间中，数据的稀疏性和处理复杂度急剧增加的问题。而残差率不依赖于维度，因此战胜了维数灾难！</p><p>再来看看所谓的缩放定律。注意这里的缩放定律与大模型领域的不同。后者是说模型<br>大小（如参数数量）的增加，模型的性能（例如在语言任务中的准确性）通常会提<br>高，并且有时这种提升的速度可以用某些数学关系（如幂律关系）来描述C&#x3D;6ND。这<br>里更偏重于理论和数学上的分析，当然背景相似，都是讨论随着参数数量的增加，模<br>型表现的提升。这部分内容基本上也可以暂时略过，主要就是简要对比了几种理论，<br>关注于如何通过理论来指导实际的神经网络设计，以实现更有效的学习和泛化能力。<br>后面还有讨论，这里暂时可以忽略。<br>好，我们接下来重点看看KAN准确性和可解释性的改进。</p><h2 id="4-3如何提升准确性？"><a href="#4-3如何提升准确性？" class="headerlink" title="4.3如何提升准确性？"></a>4.3如何提升准确性？</h2><p><code>MLPs</code>通过增加模型的宽度和深度可以提高性能，但这种方法效率低下，因为需要独立地训练不同大小的模型。</p><p><code>KANs</code>:开始可以用较少的参数训川练，然后通过简单地细化其<br>样条网格来增加参数，无需重新训练整个模型。<br>基本原理就是通过将样条函数(splines)I旧的粗网格转换为更细的网格，并对应地调<br>整参数，无需从头开始训练就能扩展现有的KAN模型。这种技术称为“网格扩展”<br>(grid extension)</p><p>$\text{Fitting }f(x,y)&#x3D;\exp(\sin(\pi x)+y^2)$</p><p><img src="https://s2.loli.net/2024/06/20/RorngTDAOxVIh8d.png" alt="image-20240620172527644"></p><p>文章用了一个小例子来证明这一点。用KAN网络逼近一个函数。上图中横轴的每个”gid-x”标签代表了在特定训练步骤时进行网格细化的时点。每次这样的标记出现，都意味着网格点数量在这个步骤有所增加，从而使模型能够更细致地逼近目标函数，这通常会导致误差的下降。表明网格点的增加直接影响了模型的学习效果，提高了逼近目标函数的精度。左右图表示了两种不同结构的网络。</p><p>下面两个图分别展示了测试误差随网格大小变化（左下图）和训练时间随网格大小的变化（右下图）。结论就是误差loss随网格大小grid size G在不同的规模上显示出不同的缩放关系；训练时间随网格大小增加而增长，特别是在网格非常大时（接近1000),训练时间急剧上升。<br>这些观测结果支持了文章中关于KANs利用网格扩展可以有效提高精度而无需重新训练整个模型的说法，同时也提示了在选择网格大小时可能需要在模型精度和训练效<br>率之间做出权衡。简单说，网格太密了也不好，太费时。</p><h2 id="4-4如何提升可解释性？"><a href="#4-4如何提升可解释性？" class="headerlink" title="4.4如何提升可解释性？"></a>4.4如何提升可解释性？</h2><p>尽管上面介绍了KN的不少好处，但遇到实际问题时该怎么设计网络结构依然是个玄学。因此需要有种方法能自动发现这种结构。本文提出的方法是使用稀疏正则化和剪枝技术从较大的KAN开始训练，剪枝后的KAN比未剪枝的KAN更易解释。为了使KAN达到最大的可解释性，本文提出了几种简化技术，并提供了一个示例，说明用户如何与KAN进行交互以增强可解释性。</p><p><img src="https://s2.loli.net/2024/06/20/KkWZLdjAPtIqSpM.png" alt="Figure 2.4:An example of how to do symbolic regression with KAN."></p><p>1.稀疏化：使用数据集训练一个KAN模型，使其能够尽可能地拟合目标函数。MLP通常使用L1正则化来促进权重的稀疏性，L1正则化倾向于推动权重值向零收缩，特别是那些对模型输出影响不大的权重。权重矩阵的“稀疏化“可以降低模型的复杂性，减少模型的存储需求和计算负担，因为只需要处理非零权重；还能提高模型的泛化能力，减少过拟合的风险<br>2.剪枝：在稀疏化后，进一步通过剪枝技术移除那些不重要的连接和神经元。<br>3.设定特定激活函数：根据剪枝后各神经元的特性，手动设置或调整特定神经元的激活函数</p><p> 4.训练仿射参数：在调整了激活函数后，对模型中的剩余参数进行再次训练，优化这<br>些参数以最好地拟合数据。<br>5.符号化：最终，模型将输出一个符号公式，这个公式是对原始目标函数的一个近似<br>表示，但通常会更简洁、更易于理解和分析。</p><h2 id="五、实验论证"><a href="#五、实验论证" class="headerlink" title="五、实验论证"></a>五、实验论证</h2><p>文章花了很大篇幅汇报详细的模拟实验结果，这也是本文引起轰动的重要原因之一。主要包括<br>准确性和可解释两部分。</p><h3 id="5-1KAN的准确性"><a href="#5-1KAN的准确性" class="headerlink" title="5.1KAN的准确性"></a>5.1KAN的准确性</h3><p><img src="https://s2.loli.net/2024/06/20/sSwV2BfGxU6EdpJ.png" alt="image-20240620173526320"></p><p>比较了KAN与MLP在逼近5个典型函数上的性能，横轴是参数量，纵轴为均方根误差(RMSE)。总的来说，KAN和MLP随着参数数量的增加，RMSE都在下降。</p><p>在大多数情况下，KAN（蓝色线）比相同深度的MLP具有更低的RMSE,尤其是在参数数量较少时。这表明KANs在参数利用效率上可能更高。MLP在参数数量增加后，<br>性能提升逐渐放缓并迅速达到平台期，这可能是因为MLP对于这些类型的函数拟合存在固有的限制。KAN在多个测试案例中都接近或跟随理论曲线。</p><p>这表明KANs在处理复杂函数和高维数据时可能是更优的选择，具有更好的扩展性和效率。这种性能优势特别重要，当我们需要从有限的数据中学习复杂的模式时，如在<br>物理建模、声音处理或图像处理等任务中。当然目前还都是比较理论化的实验数据。</p><p>接着对比了KAN和MLP在高难度的特殊函数拟合任务上的性能，结论类似。随参数量增多KAN（蓝色）表现稳定，越来越好，而MLP（黄色）出现平台期。KANs在维持低误差的同时，表现出更好的参数效率和泛化能力。这一点对于设计高效且精确的机器学习模型来说是极其重要的，特别是在资源受限或对精度要求极高的应用中。</p><p><img src="https://s2.loli.net/2024/06/20/dFPt1Vrz29T4s3G.png" alt="image-20240620173830055"></p><p>这些函数涉及椭圆积分和贝塞尔函数的特殊数学函数。</p><p>然后又提供了在解决偏微分方程上的例子。对比MLP,KANs在相同参数数量下实现<br>了更低的误差</p><p><img src="https://s2.loli.net/2024/06/20/ux8ymLRZTwblgP3.png" alt="image-20240620173956201"></p><p>接着，讨论了连续学习问题，顶行展示了用于回归任务的一维数据，包含五个高斯峰数据按阶段顺序呈现，每个阶段只展示一部分数据峰。中间和底部的行分别展示了KAN和MLP在五个学习阶段的拟合结果。KANs能够在新增数据的学习中保持之前学到的知识，而MLP表现出严重的<code>灾难性遗忘</code>，即新学的信息<code>严重干扰了旧知识的记忆</code>。</p><p><img src="https://s2.loli.net/2024/06/20/pqwW3xzu5thG6oN.png" alt="image-20240620174047112"></p><p>这种能力在机器学习中显然很重要。</p><h2 id="5-2KAN的可解释性"><a href="#5-2KAN的可解释性" class="headerlink" title="5.2KAN的可解释性"></a>5.2KAN的可解释性</h2><h3 id="5-2-1蓝督学习的例子"><a href="#5-2-1蓝督学习的例子" class="headerlink" title="5.2.1蓝督学习的例子"></a>5.2.1蓝督学习的例子</h3><p>借助前面提升模型可解释性的小技巧，包括稀疏化、剪枝等，KAN网络最终形成的网络结构<br>不仅能够实现数学函数的拟合，而且其形式本身能反映出被拟合函数的内在结构。</p><p><img src="https://s2.loli.net/2024/06/20/9lJRSaifOWwEPCH.png" alt="Figure 4.1:KANs are interepretable for simple symbolic tasks"></p><p>以第一个图为例<br>函数：f(c,y)&#x3D;xy<br>解释：<br>图中的结构利用了恒等式$2xy&#x3D;(x+y)^2-x^2-y^2$来计算乘法。这说明<br>KAN通过结合基本运算(加法、平方)来实现复杂的乘法操作，展示了KAN如何通<br>过基本的数学操作构造更复杂的函数。<br>X和y各自经过线性函数求和，然后平方，同时再减去X和y的平方。</p><p>因此可以看出，KAN模型的牛逼之处在于两点：首先，不仅仅在于自身的模型结构，MLP是<br>先组合再非线性激活，KAN是先非线性激活再组合：其次，KAN的训川练能实现自身结构上的<br>优化，有点自组织的味道了。</p><h4 id="5-2-2非监督学习的例子"><a href="#5-2-2非监督学习的例子" class="headerlink" title="5.2.2非监督学习的例子"></a>5.2.2非监督学习的例子</h4><p>上面是个监督学习的例子，用来逼近合成数据。再来看一个非监督学习的例子。</p><p><img src="https://s2.loli.net/2024/06/20/J6XR4tsPvdw2DOH.png" alt="Figure 4.2:Unsupervised learning of a toy task.KANs can identify groups of dependent variables,i.e..(1,2,3)and (4,6)in this case."></p><p>在无监督学习中，目标是识别数据中变量之间的依赖关系，而不是预测输出。KANs通过修改其结构，能够识别哪些输入变量是相互依赖的。左图(seed-0)和右图(seed&#x3D;2024)显示了相同的数据集但不同的初始化种子如何导致KAN学到不同的依赖关系结构。<br>KAN通过其灵活的网络结构提供了一种强大的工具来探索这些关系，从而增强了摸型的解释性和应用的广泛性。</p><h4 id="5-2-3数学领域的应用示例"><a href="#5-2-3数学领域的应用示例" class="headerlink" title="5.2.3数学领域的应用示例"></a>5.2.3数学领域的应用示例</h4><p>用KAN来处理结点理论问题。结点理论(Knot Theory)是拓扑学的一个分支，专门研<br>究数学中的结。</p><p>图ā显示使用17个变量的网络结构实现了81.6%的测试准确率。仅使用3个最重要的变量精简后的模型达到了78.2%的测试准确率。图(c)通过饼图展示了三个变量对预测结<br>果的贡献比例。</p><p><img src="https://s2.loli.net/2024/06/20/j6JQtLf9F4AklCD.png" alt="Figure 4.3:Knot dataset,supervised mode.With KANs,we rediscover Deepmind&#39;s results that signature ismainly dependent on meridinal translation(real and imaginary parts)."></p><p>通过优化输入变量的选择，KAN能够在保持较高准确性的同时，显著减少模型复杂<br>度。这一点对于希望减少计算资源消耗同时提高模型解释性的应用场景尤为重要。<br>也就是说，KAN的训练算法能够某种程度上实现网络结构的自我选择和优化。<br>如果说上面的例子还是监督学习，只能用于验证，那么利用无监督学习KAN还能用来发现新的数据结构，这就是是下面的例子。</p><p><img src="https://s2.loli.net/2024/06/20/UjNoA9zTrCl7Oyk.png" alt="Figure 4.4:Knot dataset,unsupervised mode.With KANs,we rediscover three mathematical relations in theknot dataset."></p><p>图a重新发现签名依赖性；图(b)展示了KAN如何在不需要任何先验知识的情况下，通过自学习过程重新发现尖点体积定义。图c)中数据点围绕着直线y&#x3D;x&#x2F;2分布，表明y和x之间存在直接的线性关系。y始终小于或等于x的一半，相当于重新发现一个不等式。这种自发现的能力说明了KAN可以作为一种探索数据中隐藏模式的强大工具。</p><h4 id="5-2-4物理领域的应用示例"><a href="#5-2-4物理领域的应用示例" class="headerlink" title="5.2.4物理领域的应用示例"></a>5.2.4物理领域的应用示例</h4><p>接着，本文继续用KAN来探索和解释物理模型中的动力学边界，尤其是在量子系统的安德森局域化现象中的应用。</p><p><img src="https://s2.loli.net/2024/06/20/elUFm2J63df8iLt.png" alt="Figure 4.5:Results for the Mosaic Model.Top:phase diagram.Middle and Bottom:KANs can obtain bothqualitative intuition (bottom)and extract quantitative results(middle).5is the golden ratio."></p><p>这个图展示了把KAN应用于两个具体的物理模型。<br>顶部：显示了模型的相图，描绘了系统参数变化下的物理状态，具体是啥咱也不用懂。<br>中部：展示了系统的特征尺寸随系统参数变化的行为，这有助于量化电子状态的局部化程度。具体是啥也不用懂。<br>底部：提供了对应的KAN结构，展示了网络如何通过学习输入数据（系统参数）来输出对应的物理行为（如局部化状态），并突出了关键的网络节点和连接，这有助于<br>理解模型中最重要的物理量。反正就是说结构能有助于理解。<br>那既然KAN训练本身能自动选择结构，而结构又能有一定物理意义。那这种结构优化能人为控制吗？于是文章又举了一个这样的例子：自动和手动模式的比较。</p><p><img src="https://s2.loli.net/2024/06/20/jXifSbVEyF2to7c.png" alt="Figure 4.6:Human-KAN collaboration to discover mobility edges of GAAM and MAAM.The human user canchoose to be lazy (using the auto mode)or more involved (using the manual mode).More details in text."></p><p>自动模式下，KANs完全基于数据自动构建和优化网络结构，无需人工干预。这可以<br>快速得到一个工作模型，但可能缺乏针对复杂问题的深入定制。在手动模式下，用户<br>可以更深入地介入模型的构建过程，例如通过手动设置网络层和激活函数来探索数据<br>中的特定特征或关系。这种方法虽然更耗时，但可以更精确地调整模型以适应特定的<br>科学问题。通过结合KANs的自动化能力和用户的专业知识，可以有效地发掘和解释<br>物理系统中的关键现象，尤其是在处理难以用传统方法解析的复杂系统时。</p><h2 id="六、MLP和KAN到底该怎么选？"><a href="#六、MLP和KAN到底该怎么选？" class="headerlink" title="六、MLP和KAN到底该怎么选？"></a>六、MLP和KAN到底该怎么选？</h2><p><img src="https://s2.loli.net/2024/06/20/ARo68nEGHqmKXPI.png" alt="Figure 6.1:Should I use KANs or MLPs?"></p><p>关于这个问题，主要看想要的是什么？如果效率优先，也就是最右边这条支路，选MLP,因<br>为目前，KANs训练速度较慢是其主要瓶颈，通常比MLPs慢10倍。但如果想要小模<br>型，KAN更好。如果可解释性优先，选中间，那么KAN牛逼。如果准确性优先，最左边，KAN也更牛逼。<br>尽管KAN显示了不错的前景，但毕竟刚开始，还很<code>不足</code>。这包括：<br><code>数学方面</code>，其实对Kolmogorov-Arnold表示定理做了很多简化，而且这个定理本身并没有考虑深层情况，也许增加深度概念后数学基础更强。<br><code>算法方面</code>，还没有充分探索KANs的架构设计和训练方法，以改进模型准确性。比如<br>替换spline激活函数，也许更好。效率方面还有待提升。同时，是不是可以与MLP混合使用也值得考虑。</p><p>七、代码实现<br>来到Github上看代码，文件夹包括了论文和代码。后者在kan文件夹。<br><a href="https://github.com/KindXiaoming/pykan">https://github.com/KindXiaoming/pykan</a></p><p>打开代码文件夹，目录结构很简洁。<br>KAN.py是主要的文件，包含了定义KAN模型的主要类或函数，负责模型的建立和训<br>练流程。<br>KANLayer..py定义了KAN模型中使用的自定义层，包括模型的核心组件，如特殊的激<br>活层或其他处理层，这些是构成KAN的基础。<br>L-BFGS.py包含了适用于KANs训练的L-BFGS优化器的实现。<br>Symbolic_.KANLayer..py实现了一种特殊的KAN层，用于处理符号计算或增强模型的解<br>释性。将数据或激活函数转换为符号形式，以便进行更深入的分析或解释。<br>spline.py包含实现样条函数的代码</p><p>我们着重看下KAN.py和KANLayer.py两个文件。这个KAN.py文件定义了一个KAN类，继承自torch.nn.Module,主要用于实现一个基于卷积的神经网络模型。</p><p><code>属性(Attributes)</code><br>·biases:使用nn.Linear（）’定义的偏置列表。<br>·act_fun:使用KANLayer’定义的激活函数层列表。<br>·depth:网络的深度。<br>·width:每层的神经元数量。<br>grid:网格间隔数。<br>·k:分段多项式的阶数。<br>·base.fun:基函数，用于定义激活函数的基础形式<br>·symbolic..fun:使用Symbolic_.KANLayer定义的符号激活函数层列表。<br>symbolic..enabled:标志位，用于控制是否计算符号前端以节省时间。<br><code>方法(Methods)</code></p><p>iitO:初始化KAN模型，可以设定网络宽度、网格数、多项式阶数、噪声比例、基<br>函数等。<br>·forward0:前向传播函数，根据输入数据通过网络计算输出。<br>·train（）:训练模型，包括设定优化器、损失函数、正则化方法等。<br>prue(0:对模型进行剪枝，移除不重要的连接或节点以简化模型和减少计算量。<br>pot0:绘制网络结构图，显示网络各层连接和激活函数的详细信息。<br>symbolic.formula0:提取和返回网络的符号表达式，用于分析网络结构的数学形式。</p><p><code>代码逻辑流程</code><br>1.初始化：在类的构造函数中初始化网络结构，包括层的设置、氵<br>激活函数的初始化<br>等。<br>2.前向传播：定义前向传播逻辑，<br>如何通过每一层和每个激活函数计算输出。<br>3.训练过程：<br>设置训练参数，执行训川练循环，包括前向传播、损失计算、反向传播和<br>参数更新。<br>4.剪枝和可视化：提供方法对网络进行剪枝并可视化网络结构，以便分析和优化。<br>5.符号表达式提取：提取网络的数学符号表达式，用于深入理解网络的工作原理。<br>再来看看<code>KANLayer.py</code>文件，</p><h2 id="八、小结"><a href="#八、小结" class="headerlink" title="八、小结"></a>八、小结</h2><p>1.<code>MLP的硬伤</code>：我们回归了MLP的核心原理，线性组合+非线性激活。深层次化网络后，反<br>向传播求导数时单一激活函数的连乘积会产生很多问题，而且全连接网络导致参数效率低<br>下。<br>2.<code>KAN的原理</code>：用单一架构的参数化可学习非线性激活函数直接组合，实现非线性空间变<br>换。模型表征能力大大提升。<br>3.<code>KAN训练算法</code>：通过grid extension,也就是激活函数分辩率提升，以及稀疏化、剪枝等<br>结构自优化技巧，实现了准确性和可解释性的提升。能够在参数量大大减少的情况下实现<br>相同甚至更有的拟合效果。<br>4.<code>实验验证</code>：仿真实验提供了有效的量化的效果证明，展示了非常有前景的方向，但目前显<br>然还比较初级。不过，提供了一条新的道路。</p><p>俗话说，天下大事，分久必合，合久必分。深度学习的理论也已经十几年一成不变了。这就相当于建筑学上的砖混结构，曾经称霸数十年。当出现钢筋混凝土浇筑结构的时候，整个建造的方法就会发生翻天覆地的变化。KAN网络无疑展示出了能够替换传统的多层感知机结构的强大实力。也让众多的研究者看到了深度学习乃至于整个人工智能新的一大片蓝海。这也是为什<br>么能够让大家像打了鸡血一样兴奋起来的原因。以前不知道idea,现在借助这个结构可以重做一遍。<br>如果今天的分享对你有所帮助，欢迎三连支持。我是梗直哥。学好A!不迷路。只说人话，专治好奇。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">conda create -n kan620 python=<span class="number">3.9</span></span><br><span class="line">conda activate kan620</span><br><span class="line"></span><br><span class="line">conda install pytorch==<span class="number">1.10</span><span class="number">.0</span> torchvision==<span class="number">0.11</span><span class="number">.0</span> torchaudio==<span class="number">0.10</span><span class="number">.0</span> cudatoolkit=<span class="number">11.3</span> -c pytorch -c conda-forge</span><br><span class="line"><span class="comment"># 其他配套安装</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install torchtext==<span class="number">0.11</span><span class="number">.0</span></span><br><span class="line">pip install spacy</span><br><span class="line">pip install pyitcast</span><br><span class="line">pip install pandas</span><br><span class="line">pip install matplotlib</span><br><span class="line">pip install AEML</span><br><span class="line">pip install tensorboard</span><br><span class="line">pip install distutils</span><br><span class="line"></span><br><span class="line">pip uninstall setuptools</span><br><span class="line">pip install setuptools==<span class="number">59.5</span><span class="number">.0</span> //需要比你之前的低 </span><br><span class="line">pip install scikit-learn</span><br><span class="line">pip install seaborn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">禁用 conda env remove --name &lt;env_name&gt;</span><br><span class="line">pip install -r H:\PyTorch\4StockMarketPrediction\requirements.txt\requirement.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记五：Subword Tokenization（子词分词器）</title>
      <link href="/2024/04/Transformer5.html"/>
      <url>/2024/04/Transformer5.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer学习笔记五：Subword-Tokenization（子词分词器）"><a href="#Transformer学习笔记五：Subword-Tokenization（子词分词器）" class="headerlink" title="Transformer学习笔记五：Subword Tokenization（子词分词器）"></a>Transformer学习笔记五：Subword Tokenization（子词分词器）</h1><p>关于Transformer的笔记，预计出如下几篇：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/454482273">Positional Encoding （位置编码），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/455399791">Self-attention（自注意力机制），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/456863215">Batch Norm &amp; Layer Norm（批量标准化&#x2F;层标准化），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/459065530">ResNet（残差网络），点击跳转</a></li><li>Subword Tokenization（子词分词法）</li><li>组装：Transformer</li></ol><p><img src="https://s2.loli.net/2024/04/29/3hEN5SdK7AZqLvn.png" alt="绿色的刹车(Skids)和棕色的挡泥板(Mudflap)是《变形金钢2》里的汽车人双胞胎，但它们最讨厌别人说彼此长得像。在电影里负责吐槽搞笑讲骚话。在《变2》中，双胞胎+大黄蜂+前第七区被迫退休且没有拿到退休金的特工西蒙组成了一个奇妙的组合，成功阻止了霸天虎下的巨型机器龙（大概是这么个东西）夺取太阳能量的野心。凭借较小的身体，双胞胎在机器龙的嘴巴里来回穿梭，给了它不少的苦头。分词器对于自然语言模型来说，也有些相似双胞胎（这里笔者为了把它们关联上开始硬扯）：小、常被用但经常被忽略其本身、是任务开始的第一环。那就一起来看看它们吧！"></p><p>这篇笔记是Transformer系列的最后一个零件，讲述将语料处理成入模token的方法。笔记结构如下：</p><p>一、封面故事</p><p>二、分词器的组成</p><p>三、分词器的类别</p><ul><li>2.1 Word-level：基于空格的分词器</li><li>2.2 Character-level：基于字符的分词器</li><li>2.3 Subword-level：基于子词的分词器</li></ul><p>四、Byte Pair Encoding (BPE)</p><ul><li>3.1 BPE的训练步骤（Trainer）</li><li>3.2 BPE的编码步骤（Encoder）</li><li>3.3 BPE的解码步骤（Decoder）</li></ul><p>五、WordPiece</p><ul><li>4.1 WordPiece的训练步骤（Trainer）</li><li>4.2 WordPiece的编码和解码步骤</li></ul><p>六、Unigram Language Model (ULM)</p><ul><li>5.1 ULM的训练步骤（Trainer）</li><li>5.2 ULM的编码和解码步骤</li></ul><h2 id="一、分词器的组成"><a href="#一、分词器的组成" class="headerlink" title="一、分词器的组成"></a>一、分词器的组成</h2><p>如前所说，分词器的作用是将预料拆分成能入模的token。</p><p>分词器一般包含以下几个部分：</p><ul><li><strong>Trainer（分词训练器&#x2F;算法）</strong>：给定训练数据，用Trainer封装的分词算法，生成词表</li><li><strong>Vocabulary（词表）</strong>：词表是分词的标准。基本上每个语言任务都需要维护一个词表，词表可以是根据当前训练数据生成的，也可以直接加载前人预训练好的词表。</li><li><strong>Encoder（编码器）</strong>：拥有词表后，给定语料，编码器把语料按词表拆分token，并记录token在词表中的索引，拥有该索引则可将token表示成one-hot向量入模，再做后续的embedding等优化操作。</li><li><strong>Decoder（解码器）</strong>：给定一串索引，将索引还原成最初的语料形式。</li></ul><p>我们用HuggingFace中提供的Bert分词器做一个例子，在Bert中使用分词算法是WordPiece（在这篇笔记里会介绍）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sequence = <span class="string">&quot;A Titan RTX has 24GB of VRAM&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenized_sequence = tokenizer.tokenize(sequence) <span class="comment"># 用定义的分词器对语料分词</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(tokenized_sequence)</span><br><span class="line">[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;Titan&#x27;</span>, <span class="string">&#x27;R&#x27;</span>, <span class="string">&#x27;##T&#x27;</span>, <span class="string">&#x27;##X&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;24&#x27;</span>, <span class="string">&#x27;##GB&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;V&#x27;</span>, <span class="string">&#x27;##RA&#x27;</span>, <span class="string">&#x27;##M&#x27;</span>]</span><br></pre></td></tr></table></figure><p>分词器将语料拆分为多个token。以<code>&#39;R&#39;, &#39;##T&#39;, &#39;##X&#39;</code>为例，这里是分词器在说：“我的词表里没有<code>RTX</code>这个词，只有<code>R,T,X</code>三个字母，因此我只好把它们按词表拆开。但我知道<code>RTX</code>代表一个我不认识的整体，因此我用<code>##</code>把它们连接起来，团魂不灭！</p><p>我们也可以一步到位，把上面的文字token转变为在词表中的索引，其中101，102是Bert中输入的特殊标志符号<code>CLS</code>和<code>SEP</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>inputs = tokenizer(sequence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoded_sequence = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(encoded_sequence)</span><br><span class="line">[<span class="number">101</span>, <span class="number">138</span>, <span class="number">18696</span>, <span class="number">155</span>, <span class="number">1942</span>, <span class="number">3190</span>, <span class="number">1144</span>, <span class="number">1572</span>, <span class="number">13745</span>, <span class="number">1104</span>, <span class="number">159</span>, <span class="number">9664</span>, <span class="number">2107</span>, <span class="number">102</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>decoded_sequence = tokenizer.decode(encoded_sequence)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(decoded_sequence)</span><br><span class="line">[CLS] A Titan RTX has 24GB of VRAM [SEP] </span><br></pre></td></tr></table></figure><h2 id="二、分词器的类别"><a href="#二、分词器的类别" class="headerlink" title="二、分词器的类别"></a>二、分词器的类别</h2><h3 id="2-1-Word-level：基于空格的分词器"><a href="#2-1-Word-level：基于空格的分词器" class="headerlink" title="2.1 Word-level：基于空格的分词器"></a>2.1 Word-level：基于空格的分词器</h3><p>早期的做法，是按空格拆分单词，将一个单词做为一个token纳入词表，因此也说是word-level维度的。若语料里出现不在词表中的token，也称**OOV(Out Of Vocabulary)**，则此时常用<code>&lt;UNK&gt;</code>(Unknown)这个特殊符号来代替。这个分词器存在以下几个问题：</p><ul><li>大词表问题。词表大小和一门语言词汇量大小密切相关，当词表太大时，一来增加模型的存储负担。二来，<strong>token的one-hot向量也会非常长，增加后续embedding的训练参数</strong>。</li><li>罕见词难表示。</li><li>词汇冗余。例如love, loving, loved其实拥有相似的含义，但在词表中作为独立词存在。</li></ul><h3 id="2-2-Character-level：基于字符的分词器"><a href="#2-2-Character-level：基于字符的分词器" class="headerlink" title="2.2 Character-level：基于字符的分词器"></a>2.2 Character-level：基于字符的分词器</h3><p>每个字符作为一个词。例如英语中只有26个字符，那词表大小就只有26个。但这个level上依然存在问题：</p><ul><li>单个字符本身缺少语义。</li><li>输入序列变长。从原来的一句话变成若干个字符</li></ul><h3 id="2-3-Subword-level：基于子词的分词器"><a href="#2-3-Subword-level：基于子词的分词器" class="headerlink" title="2.3 Subword-level：基于子词的分词器"></a>2.3 Subword-level：基于子词的分词器</h3><p>子词分词器类似于借助词根词源来学习一系列单词。例如<code>transformer = trans + form + er</code>，<code>transfer = trans + fer</code>。借助这些共通的基本单元，就可以构造更多的词汇。每一个单元就是词表的组成。当然，我们的分词器是基于统计意义的分词器，不是基于语言学的。只是当它看过大量训练数据以后，它能学习到语言学上的含义。主要的子词分词器有三类：BPE，WordPiece和ULM。</p><h2 id="三、Byte-Pair-Encoding-BPE"><a href="#三、Byte-Pair-Encoding-BPE" class="headerlink" title="三、Byte Pair Encoding (BPE)"></a>三、Byte Pair Encoding (BPE)</h2><p>Transformer中，在对WMT 2014 English-German数据集进行处理的时候，就用到了BPE。Sennrich et al. 2015最早将BPE用在Neural Machine Translation领域上，这个算法是目前最流行的NLP分词方法之一（GPT-2采用的也是该方法）。</p><h3 id="3-1-BPE的训练步骤（Trainer）"><a href="#3-1-BPE的训练步骤（Trainer）" class="headerlink" title="3.1 BPE的训练步骤（Trainer）"></a>3.1 BPE的训练步骤（Trainer）</h3><ul><li>收集训练语料，并确定期望的subword大小，也就是词表大小</li><li>将语料中每一个单词的末尾添上特殊字符<code>&lt;/w&gt;</code>，该字符表示单词的结束。举例来说，同样是<code>er</code>，它在<code>lower</code>和在<code>era</code>中的意义就不一样，这就是为什么要加特殊字符。</li><li>统计单词在整个训练语料中的出现频率。例如单词low在训练语料中出现了5次，则记为<code>&#39;low&lt;/w&gt;&#39;: 5</code></li><li>把单词拆分成字符，即<code>&#39;low&lt;/w&gt;&#39;: 5</code>变成<code>l o w &lt;/w&gt;&#39;: 5</code></li><li>统计所有连续字符对的出现频率。假设我们有<code>l o w &lt;/w&gt;&#39;: 5</code>和<code>&#39;h o w &lt;/w&gt;&#39;: 3</code>，则此时有<code>(&#39;l&#39;, &#39;o&#39;): 5, (&#39;o&#39;, &#39;w&#39;): 5+3 = 8, (&#39;h&#39;, &#39;o&#39;): 3</code>。取出现频率最高的字符对将它们合并。例如<code>(&#39;o&#39;, &#39;w&#39;)</code>出现频率最高，则此时变成<code>&#39;l ow&lt;/w&gt;&#39;: 5, &#39;h ow&lt;/w&gt;&#39;:3</code>，此时的词表为<code>&#39;l&#39;, &#39;ow&#39;, &#39;h&#39;</code>。</li><li>重复上一步，直到词表达到预定的大小，或者所有连续字符对的最高出现频率为1</li></ul><p>沿用Sennrich et al. 2015在论文中给出的例子<code>‘low&lt;/w&gt;’:5 ‘lower&lt;/w&gt;’:2 ‘newest&lt;/w&gt;’:6 ‘widest&lt;/w&gt;’:3</code>，这里展示一个demo来看下算法运作的流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stats</span>(<span class="params">vocab: <span class="built_in">dict</span></span>)-&gt;<span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从当前训练数据中，计算所有连续字符对出现的频率</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab: 当前训练数据，key为当前每个单词的形式，value为对应频率</span></span><br><span class="line"><span class="string">               形式诸如：&#123;&#x27;l o w &lt;/w&gt;&#x27;: 5, &#x27;l o w e r &lt;/w&gt;&#x27;: 2, &#x27;n e w e s t &lt;/w&gt;&#x27;: 6, &#x27;w i d e s t &lt;/w&gt;&#x27;: 3&#125;</span></span><br><span class="line"><span class="string">    Return</span></span><br><span class="line"><span class="string">        pairs：所有连续字符对出现的频率，key为连续字符对，value为频率</span></span><br><span class="line"><span class="string">               形式诸如：&#123;(&#x27;l&#x27;, &#x27;o&#x27;): 7, (&#x27;o&#x27;, &#x27;w&#x27;): 7, (&#x27;w&#x27;, &#x27;&lt;/w&gt;&#x27;): 5, </span></span><br><span class="line"><span class="string">                        (&#x27;w&#x27;, &#x27;e&#x27;): 8, (&#x27;e&#x27;, &#x27;r&#x27;): 2, (&#x27;r&#x27;, &#x27;&lt;/w&gt;&#x27;): 2, </span></span><br><span class="line"><span class="string">                        (&#x27;n&#x27;, &#x27;e&#x27;): 6, (&#x27;e&#x27;, &#x27;w&#x27;): 6, (&#x27;e&#x27;, &#x27;s&#x27;): 9, </span></span><br><span class="line"><span class="string">                        (&#x27;s&#x27;, &#x27;t&#x27;): 9, (&#x27;t&#x27;, &#x27;&lt;/w&gt;&#x27;): 9, (&#x27;w&#x27;, &#x27;i&#x27;): 3, </span></span><br><span class="line"><span class="string">                        (&#x27;i&#x27;, &#x27;d&#x27;): 3, (&#x27;d&#x27;, &#x27;e&#x27;): 3&#125;)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols)-<span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_vocab</span>(<span class="params">pair:<span class="built_in">tuple</span>, v_in:<span class="built_in">tuple</span></span>)-&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用找出的频率最大的字符对（pair）去对当前训练数据做合并</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pair: 当前频率最大的子词对，形式诸如(&#x27;e&#x27;, &#x27;s&#x27;)</span></span><br><span class="line"><span class="string">        v_in: 当前训练数据</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        v_out: 合并结果。例如用&#x27;es&#x27;替换&#x27;e s&#x27;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&#x27; &#x27;</span>.join(pair)) <span class="comment"># 对字符串中所有可能被解释为正则运算的字符进行转义。例如&#x27;e s&#x27;变为&#x27;e\ s&#x27;</span></span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>) <span class="comment"># 创建正则匹配对象</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&#x27;&#x27;</span>.join(pair), word) <span class="comment"># 对word中满足正则匹配的部分，用&#x27;&#x27;.join(pair)代替</span></span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_token</span>(<span class="params">token: <span class="built_in">str</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算一个token的长度。主要是将特殊标记&#x27;&lt;/w&gt;&#x27;当成一个字符来处理</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token[-<span class="number">4</span>:] == <span class="string">&#x27;&lt;/w&gt;&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(token[:-<span class="number">4</span>]) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens</span>(<span class="params">vocab:<span class="built_in">dict</span></span>)-&gt;<span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据当前训练数据生成当下的bpe词表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bpe_vocab = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        <span class="keyword">for</span> symbol <span class="keyword">in</span> word.split():</span><br><span class="line">            bpe_vocab[symbol] += freq</span><br><span class="line">    bpe_vocab = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(bpe_vocab.items(), key=<span class="keyword">lambda</span> x: len_token(x[<span class="number">0</span>]), reverse = <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bpe_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_bpe_vocab</span>(<span class="params">v_in:<span class="built_in">dict</span>, num_merges: <span class="built_in">int</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成bpe词表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        v_in: 原始训练数据</span></span><br><span class="line"><span class="string">        num_merges: 合并次数，也是预设的最终的词表大小</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        bpe_vocab: bpe词表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    v_out = v_in.copy()</span><br><span class="line">    bpe_vocab = get_tokens(v_in)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;执行BPE前：&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练数据为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(v_in))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;初始化词表为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(bpe_vocab.keys()))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;初始化词表长度为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(bpe_vocab)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        pairs = get_stats(v_out)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            best = <span class="built_in">max</span>(pairs, key = pairs.get)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;所有词汇已不可拆分，停止生成词表&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> pairs[best] &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;字符对出现频率小于2，停止生成词表&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        v_out = merge_vocab(best, v_out)</span><br><span class="line">        bpe_vocab = get_tokens(v_out)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter出现频率最大的连续字符对为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(best))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter合并后的新训练数据为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(v_out))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter新生成的词表为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(bpe_vocab.keys()))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter新生成的词表长度为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(bpe_vocab)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bpe_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    num_merges = <span class="number">7</span></span><br><span class="line">    bpe_tokens = generate_bpe_vocab(vocab, num_merges)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="https://s2.loli.net/2024/04/29/rLYpjk8D6cFgv1w.webp" alt="运行结果"></p><h3 id="3-2-BPE的编码步骤（Encoder）"><a href="#3-2-BPE的编码步骤（Encoder）" class="headerlink" title="3.2 BPE的编码步骤（Encoder）"></a>3.2 BPE的编码步骤（Encoder）</h3><p>训练步骤结束后，我们得到一个词表，Encoder阶段要解决的事情，就是给定一个语料，基于词表对该语料进行分词，具体步骤如下：</p><ul><li>将词表中的token按长度从长到短进行排序（上述代码已经实现了这一点）</li><li>对语料中的每一个单词，遍历排序好的词表，查看当前的token是否是该单词的子字符串。如果是，则输出当前token，并对剩余的字符串继续匹配。</li><li>当遍历完一次词表过后，如果仍有字符串在词表中找不到匹配，就把这些字符串用特殊符号输出，比如<code>&lt;UNK&gt;</code></li></ul><p>以上述代码生成的词表<code>[&#39;est&lt;/w&gt;&#39;, &#39;low&#39;, &#39;new&#39;, &#39;&lt;/w&gt;&#39;, &#39;e&#39;, &#39;r&#39;, &#39;w&#39;, &#39;i&#39;, &#39;d&#39;]</code>为例，假设有一单词为<code>hiest</code>，则该单词最终将被切分为<code>&lt;UNK&gt; i est&lt;/w&gt;</code>。</p><h3 id="3-3-BPE的解码步骤（Decoder）"><a href="#3-3-BPE的解码步骤（Decoder）" class="headerlink" title="3.3 BPE的解码步骤（Decoder）"></a>3.3 BPE的解码步骤（Decoder）</h3><p>当相邻的token间没有特殊符号（例如空格&#x2F;终止符），则表示这两个token同属一个单词，将它们直接拼接即可。否则需要在两个token间添加分隔符。</p><h2 id="四、WordPiece"><a href="#四、WordPiece" class="headerlink" title="四、WordPiece"></a>四、WordPiece</h2><p>Bert模型在做分词的时候用的就是WordPiece方法。WordPiece方法和BPE总体很相似，不同的是，在选择合并字符时，BPE采用的标准是“频率最高”，而WordPiece则是“概率最大”。具体来说，WordPiece在每次合并时将选择一对相邻的子字符，这对子字符满足：当合并它们时，它们对语料库语言模型的概率提升最大。<br>来具体看一下算法。</p><h3 id="4-1-WordPiece的训练步骤（Trainer）"><a href="#4-1-WordPiece的训练步骤（Trainer）" class="headerlink" title="4.1 WordPiece的训练步骤（Trainer）"></a>4.1 WordPiece的训练步骤（Trainer）</h3><p>假设句子$S&#x3D;(t_1,t_2,\ldots,t_n)$ ，其中$t_i$表示第i个子词（初始状态是字符，经过一定的合并步骤以后变成子词，所以这里统一用 𝑡 来表示）。假设各个子词之间相互独立，则有：</p><p>$logP(S)&#x3D;\sum_{i&#x3D;1}^nlogP(t_i)$</p><p>对于某两个相邻位置x和y，若将对应位置的子词合并，记合并后的子词为z，则合并前后语言模型概率的变化为：</p><p>$$logP(t_z)-(logP(t_x)+logP(t_y))&#x3D;log(\frac{P(t_z)}{P(t_x)P(t_y)})$$ </p><p>则对于这个句子，我们要做的就是找到使得$log(\frac{P(t_z)}{P(t_x)P(t_y)})$ 最大的那个相邻$(t_x,t_y)$ 对，这个最大对在语言模型上具有较强的关联性，因此我们选择合并它们。</p><p>Encoder和Decoder的方法和BPE相似，因此这里略去。同时也注意到，WordPiece和BPE一样，都没有对不同的分词结果进行比较。</p><h2 id="五、Unigram-Language-Model-ULM"><a href="#五、Unigram-Language-Model-ULM" class="headerlink" title="五、Unigram Language Model (ULM)"></a>五、Unigram Language Model (ULM)</h2><p>总结BPE和WordPiece，我们发现它们都是通过一定的方法，生成一个最终的词表，然后对于某个单词或句子，根据这个最终的词表，生成<strong>唯一</strong>的划分结果。</p><p>但是，<strong>这个划分结果一定就是最好的吗</strong>？假设通过BPE或WordPiece，生成了一张词表<code>est&lt;/w&gt;, st&lt;/w&gt;, w, e, s, t</code>，那么对于单词<code>west</code>，它有三种划分方式：<code>w est&lt;/w&gt;</code>，<code>w e st&lt;/w&gt;</code>和<code>w e s t</code>。根据前两个算法的编码规则，我们选择了第一个，但是也许在后续做训练任务时，其实第三个的表现更好呢？</p><p>因此，ULM就作为一种改进办法出现了。概括来说，它的改进思路是：</p><ul><li>首先拥有一种大词表</li><li>基于这张词表，对所有语料的所有子词划分结果做一个总体评分。然后从词表中<strong>剔除掉那些对总体评分贡献最小的子词</strong></li><li>重复第二步，不断丢弃贡献度小的子词，直到词表达到预设大小</li><li>生成最终的词表。基于该词表，一个语料仍可能有多种划分，通过某种方式，选择其中最优的划分。</li></ul><p>基于这个改进思想，来详细看下算法过程。</p><h3 id="5-1-ULM的训练步骤（Trainer）"><a href="#5-1-ULM的训练步骤（Trainer）" class="headerlink" title="5.1 ULM的训练步骤（Trainer）"></a>5.1 ULM的训练步骤（Trainer）</h3><ul><li>初始化一个大词表。这个词表可以是语料中所有字符 + 常见的子字符串，也可以用BPE进行初始化。</li><li>基于该词表，对所有语料的所有子词划分结果做一个总体评价。具体的实现方式是：</li></ul><p>设语料库中有 |𝐷| 条语料。 $X(s)$ 表示语料库中的第 𝑠 条语料。 $\mathcal{S}(X^{(s)})$ 表示语料 𝑋(𝑠) 所有划分结果的集合。 $\vec{x}$表示其中一种划分结果，满足$\vec{x}\in\mathcal{S}(X^{(s)})$  。同时，我们<strong>假设词表中每一个子词相互独立（Unigram Language Model的重要前提）。</strong>则我们的<strong>评价标准</strong>$\mathcal{L}$定义为：预料库中的所有语料的所有分词组合形成的概率相加，即：</p><p>$$\mathcal{L}&#x3D;\sum_{s&#x3D;1}^{|D|}log(P(X^{(s)}))&#x3D;\sum_{s&#x3D;1}^{|D|}log(\sum_{\vec{x}\in\mathcal{S}(X^{(s)})}P(\vec{x}))$$</p><p>其中，假设某个划分结果$\vec{x}$中含有m个子词，即$\vec{x}&#x3D;(x_1,x_2,\ldots,x_m)$ ，那么有：</p><p>$$P(\vec{x})&#x3D;\prod_{i&#x3D;1}^mP(x_i)$$ </p><p>基于当前词表，通过EM算法，我们可以求得$$P(x_i)$$的概率，也就是每个子词在整个语料库上的概 率。进而就可以求得$\mathcal{L}$  。</p><ul><li>从当前词表中删除对 𝐿贡献度小的子词。具体实现方式是：</li></ul><p>对于每个字词，计算当该子词从词表中移除时，总的$\mathcal{L}$ 降低了多少，这个降低记为该字词的 𝑙𝑜𝑠𝑠 。将 𝑙𝑜𝑠𝑠 从小到大进行排序，<strong>丢弃掉一定比例的</strong> 𝑙𝑜𝑠𝑠 <strong>最小的子词</strong>。可以理解为，这些子词对整个语料库的影响不大。同时，<strong>单字符的子词不要丢弃，这是为了避免OOV的情况</strong>。</p><ul><li>重复步骤2和3，直到词表达到预设大小。同时保留最终计算得到的 𝑃(𝑥𝑖) 结果。这个结果将在encoder阶段使用。</li></ul><h3 id="5-2-ULM的编码步骤（Encoder）"><a href="#5-2-ULM的编码步骤（Encoder）" class="headerlink" title="5.2 ULM的编码步骤（Encoder）"></a>5.2 ULM的编码步骤（Encoder）</h3><p>编码步骤里，将基于最终生成的词表，找到一个语料最优的划分方式，设这个最有的划分方式为$$\vec x^*$$，则我们的目的其实是：</p><p>$$\vec{x}^*&#x3D;argmax_{\vec{x}\in\mathcal{S}(X^{(s)})}P(\vec{x})$$</p><p>根据训练阶段保留下来的$P(x_i)$，我们可以用**维特比算法(viterbi)**来得到最终最优的划分。<br>解码步骤和前面两个分词算法一样，这里就不再提了。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电脑环境配置</title>
      <link href="/2024/04/5212.html"/>
      <url>/2024/04/5212.html</url>
      
        <content type="html"><![CDATA[<h1 id="笔记本电脑配置环境"><a href="#笔记本电脑配置环境" class="headerlink" title="笔记本电脑配置环境"></a>笔记本电脑配置环境</h1><h2 id="教程："><a href="#教程：" class="headerlink" title="教程："></a>教程：</h2><p><a href="https://blog.csdn.net/sjjsbsbbs/article/details/123556348">3050显卡驱动安装+配置pytorch的cuda环境_3050驱动_血狼傲骨的博客-CSDN博客</a></p><p>相关型号：GeForce RTX 3050 Laptop GPU</p><p>文件信息：</p><ol><li>cuda_11.6.1_511.65_windows.exe</li><li>536.23-notebook-win10-win11-64bit-international-dch-whql.exe</li><li>cudnn-windows-x86_64-8.8.1.3_cuda11-archive.zip</li></ol><h2 id="添加镜像源："><a href="#添加镜像源：" class="headerlink" title="添加镜像源："></a>添加镜像源：</h2><p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\</a></p><h2 id="Transformer-推荐安装环境的版本"><a href="#Transformer-推荐安装环境的版本" class="headerlink" title="Transformer 推荐安装环境的版本"></a><code>Transformer</code> 推荐安装环境的版本</h2><p>Pytorch：<code>1.10.0</code> torchtext：<code>0.11.0 </code> Python：<code>3.7</code></p><p><code>3.7</code>版本</p><h2 id="cuda-11-3-笔记本-大电脑安装过程"><a href="#cuda-11-3-笔记本-大电脑安装过程" class="headerlink" title="cuda 11.3 笔记本-大电脑安装过程"></a>cuda 11.3 笔记本-大电脑安装过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">conda create -n trans370 python=<span class="number">3.7</span></span><br><span class="line">conda activate trans370</span><br><span class="line"></span><br><span class="line">conda install pytorch==<span class="number">1.10</span><span class="number">.0</span> torchvision==<span class="number">0.11</span><span class="number">.0</span> torchaudio==<span class="number">0.10</span><span class="number">.0</span> cudatoolkit=<span class="number">11.3</span> -c pytorch -c conda-forge</span><br><span class="line"><span class="comment"># 其他配套安装</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install torchtext==<span class="number">0.11</span><span class="number">.0</span></span><br><span class="line">pip install spacy</span><br><span class="line">pip install pyitcast</span><br><span class="line">pip install pandas</span><br><span class="line">pip install matplotlib</span><br><span class="line">pip install AEML</span><br><span class="line">pip install tensorboard</span><br><span class="line">pip install distutils</span><br><span class="line"></span><br><span class="line">pip uninstall setuptools</span><br><span class="line">pip install setuptools==<span class="number">59.5</span><span class="number">.0</span> //需要比你之前的低 </span><br><span class="line">pip install scikit-learn</span><br><span class="line">pip install seaborn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">禁用 conda env remove --name &lt;env_name&gt;</span><br><span class="line">pip install -r H:\PyTorch\4StockMarketPrediction\requirements.txt\requirement.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>测试是否安装成功</code> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"><span class="literal">True</span> <span class="comment"># 测试成功</span></span><br></pre></td></tr></table></figure><p><code>陈硕</code>版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">conda create -n torch_chen2 python=<span class="number">3.8</span></span><br><span class="line">conda activate torch_chen2</span><br><span class="line"></span><br><span class="line">pip3 install torch==<span class="number">1.10</span><span class="number">.0</span>+cu113 torchvision==<span class="number">0.11</span><span class="number">.1</span>+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -i https://pypi.douban.com/simple</span><br><span class="line">pip install xlrd==<span class="number">1.2</span><span class="number">.0</span></span><br><span class="line">pip install xlrd==<span class="number">4.64</span><span class="number">.1</span></span><br><span class="line">cudnn-windows-x86_64-<span class="number">8.8</span><span class="number">.0</span><span class="number">.121</span>_cuda12-archive.<span class="built_in">zip</span></span><br><span class="line">cuda_12<span class="number">.0</span><span class="number">.0_527</span><span class="number">.41</span>_windows.exe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install matplotlib==<span class="number">3.2</span><span class="number">.0</span> -i https://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conda create –n py35 python=<span class="number">3.5</span></span><br><span class="line">activate py35</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置路径</span></span><br><span class="line">https://blog.csdn.net/m0_67313306/article/details/<span class="number">124204890</span></span><br><span class="line">conda create -n torch1<span class="number">.11</span> python=<span class="number">3.8</span></span><br><span class="line">conda activate torch1<span class="number">.11</span></span><br><span class="line">conda install pytorch==<span class="number">1.8</span><span class="number">.1</span> torchvision==<span class="number">0.9</span><span class="number">.1</span> torchaudio==<span class="number">0.8</span><span class="number">.1</span> cudatoolkit=<span class="number">10.2</span> -c pytorch</span><br><span class="line">pip install xlrd==<span class="number">1.2</span><span class="number">.0</span></span><br><span class="line">pip install matplotlib==<span class="number">3.2</span><span class="number">.0</span> -i https://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="小米手机地址"><a href="#小米手机地址" class="headerlink" title="小米手机地址"></a>小米手机地址</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ftp://<span class="number">192.168</span><span class="number">.137</span><span class="number">.134</span>:<span class="number">2121</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 配置环境 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>陈硕--Transformer-Attention Is All You Need</title>
      <link href="/2024/04/Tranformer_chen.html"/>
      <url>/2024/04/Tranformer_chen.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer-Attention-Is-All-You-Need"><a href="#Transformer-Attention-Is-All-You-Need" class="headerlink" title="Transformer-Attention Is All You Need"></a>Transformer-Attention Is All You Need</h1><p>为什么提出Transformer,它到底是何方神圣？</p><p>它到底是如何表现高的性能，是否值得去研究？</p><ol><li>(1) Transformerself-attention</li><li>(2) 位置编码</li><li>(3) 网络编码模块</li><li>(4) 网络解码模块</li></ol><p><img src="https://s2.loli.net/2024/04/18/HXW1EfiBJrboDQd.png" alt="image-20240418120642326"></p><p><strong>为什么引入Transformer?</strong><br>Transformer模型使用了Self-Attention机制，不采用RNN顺序结构，使得模型可以并行化训练，<br>而且能够拥有全局信息。<br>说白了：<strong>并行加速计算+学习全局信息</strong></p><p><img src="https://s2.loli.net/2024/04/18/WnyJMDTkHBFsIRv.png" alt="image-20240418121136374"></p><p>CNN缺点：只能考虑有限的内容<br>CNN优点：并行化处理</p><p><img src="https://s2.loli.net/2024/04/18/hwjCQJVX4BmAlNK.png" alt="image-20240418121404965"></p><p>如何既能并行又能学习到全局信息？→Self-Attention</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404181215168.png" alt="image-20240418121554091"></p><p>为什么提出Transformer,它到底是何方神圣？</p><p>它到底是如何表现高的性能，是否值得去研究？</p><ol><li>(1)Transformerself-sttention</li><li>(2)位置编码</li><li>(3)网络编码模块</li><li>(4)网络解码模块</li></ol><p>具体如何实现Self-Attention?(q、k、v计算)</p><h2 id="系统框架"><a href="#系统框架" class="headerlink" title="系统框架"></a>系统框架</h2><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404181216192.png" alt="image-20240418121659147"></p><hr><p>具体如何实现Self-Attention?(q、k、v计算)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">q:q：query (to <span class="keyword">match</span> others) 查询</span><br><span class="line">qi Waai</span><br><span class="line">k:k：key (to be matched) 匹配</span><br><span class="line">ki =Wkai</span><br><span class="line">v:v：information to be extracted 信息</span><br><span class="line">vi=Wval</span><br></pre></td></tr></table></figure><p>q：查询</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222223323.png" alt="image-20240422222349223"></p><p>如何获得qkv</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222226415.png" alt="image-20240422222610310"></p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d is the dim of g and k</span><br></pre></td></tr></table></figure><p>Self-attention</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222227526.png" alt="image-20240422222739442"></p><p>Considering the whole sequence</p><p>数学推导并行计算机制！</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222229282.png" alt="image-20240422222944188"></p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222231757.png" alt="image-20240422223107651"></p><p>(ignore vd for simplicity)</p><p>步骤①、②、③</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222233627.png" alt="image-20240422223322500"></p><p>QKV如何计算<br><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222235648.png" alt="image-20240422223526570"></p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404222236256.png" alt="image-20240422223638154"></p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>p24:06</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231053905.png" alt="image-20240423105345813"></p><p>Positional Encoding !!!!!<br>位置编码嵌入与词嵌入相加？<br>$$W\cdot x_p^i&#x3D;[W^I,W^P]\cdot\left[x^i\backslash\text{p}^i\right]&#x3D;W^I\cdot x^i+W^P\cdot p^i&#x3D;a^i+e^i$$<br>位置编码获取方式：可以通过训练得到，也可以使用公式得到；<br>$$\begin{matrix}\text{[Sin(x),cos(x)}&amp;\text{Sin(2x),cos(2x)….]}\end{matrix}$$</p><p>$$PE\left(1\right)&#x3D;\left[\sin\left(1&#x2F;10000^{0&#x2F;512}\right),\cos\left(1&#x2F;10000^{0&#x2F;512}\right),\sin\left(1&#x2F;10000^{2&#x2F;512}\right),\cos\left(1&#x2F;10000^{2&#x2F;512}\right),\ldots\right]$$</p><p>P27:35</p><p>为什么提出Transformer,它到底是何方神圣？<br>它到底是如何表现高的性能，是否值得去研究？</p><ol><li>(1)Transformerself-sttention</li><li>(2)位置编码</li><li>(3)网络编码模块</li><li>(4)网络解码模块</li></ol><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231059315.png" alt="image-20240423105957216"></p><p>Add表示残差连接<br>(Residual Connection)用于防止网络退化<br>Norm表示Layer Normalization,用于对每一层的激活值进行归一化</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231102283.png" alt="image-20240423110208180"></p><h3 id="Encoder-30"><a href="#Encoder-30" class="headerlink" title="Encoder 30"></a>Encoder 30</h3><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231102352.png" alt="image-20240423110228255"></p><p>接着是一个Feed Forward的前馈网络和一个Add&amp;Norm Layer.<br>所以，这一个绿色的block的前2个Layer操作的表达式为：<br>O1 Layer Normalization(I+Multi-head Self-Attention(I))<br>这一个绿色的block的后2个Layer操作的表达式为：<br>2&#x3D;Layer Normalization(1+Feed Forward Network(O))<br>Block(I)&#x3D;02<br>所以Transformer的Encoder的整体操作为：<br>Encoder(I)&#x3D;Block(…Block(Block)（）)<br>quadN times</p><h3 id="Dencoder-31"><a href="#Dencoder-31" class="headerlink" title="Dencoder 31"></a>Dencoder 31</h3><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231104724.png" alt="image-20240423110410631"></p><p>新加的attention多加了一个mask,因为训练时的output都是Ground Truth,这样可以确保预测第i个位置时不会接触到未来的信息。</p><p><img src="https://s2.loli.net/2024/06/15/d2lNJsHW6MIb4Aq.png" alt="image-20240615191518467"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二章 Transformer</title>
      <link href="/2024/04/c3df.html"/>
      <url>/2024/04/c3df.html</url>
      
        <content type="html"><![CDATA[<h2 id="2-2-输入部分实现-P4"><a href="#2-2-输入部分实现-P4" class="headerlink" title="2.2 输入部分实现 P4"></a>2.2 输入部分实现 P4</h2><ul><li><p>学习目标</p><ul><li><p>了解文本嵌入层和位置编码的作用.</p></li><li><p>掌握文本嵌入层和位置编码的实现过程.</p></li></ul></li></ul><hr><ul><li>输入部分包含:<ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108193413280.png" alt="image-20231108193413280"></p><h3 id="文本嵌入层的作用"><a href="#文本嵌入层的作用" class="headerlink" title="文本嵌入层的作用"></a>文本嵌入层的作用</h3><ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li></ul><hr><ul><li><p>pytorch 0.3.0及其必备工具包的安装:</p><h3 id="安装版本"><a href="#安装版本" class="headerlink" title="安装版本"></a>安装版本</h3></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用pip安装的工具包包括pytorch-0.3.0, numpy, matplotlib, seaborn</span></span><br><span class="line">pip install http://download.pytorch.org/whl/cu80/torch-<span class="number">0.3</span><span class="number">.0</span>.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib seaborn</span><br><span class="line"></span><br><span class="line"><span class="comment"># MAC系统安装, python版本&lt;=3.6</span></span><br><span class="line">pip install torch==<span class="number">0.3</span><span class="number">.0</span>.post4 numpy matplotlib seaborn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda create -n postcoding python=<span class="number">3.0</span></span><br><span class="line">conda activate postcoding</span><br><span class="line"></span><br><span class="line">pip3 install torch==<span class="number">1.10</span><span class="number">.0</span>+cu113 torchvision==<span class="number">0.11</span><span class="number">.1</span>+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -i https://pypi.douban.com/simple</span><br><span class="line">pip install xlrd==<span class="number">1.2</span><span class="number">.0</span></span><br><span class="line">pip install xlrd==<span class="number">4.64</span><span class="number">.1</span></span><br><span class="line">pip install matplotlib==<span class="number">3.2</span><span class="number">.0</span> -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure><ul><li>文本嵌入层的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必备的工具包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层, </span></span><br><span class="line"><span class="comment"># 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学计算工具包</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch中变量封装函数Variable.</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span></span><br><span class="line"><span class="comment"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment"># 最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;可以将其理解为该层的前向传播逻辑，所有层中都会有此函数</span></span><br><span class="line"><span class="string">           当传给该类的实例化对象参数时, 自动调用该类函数</span></span><br><span class="line"><span class="string">           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)<span class="comment"># sqrt(self.d_model)缩放作用</span></span><br></pre></td></tr></table></figure><p><code>P5</code></p><ul><li>nn.Embedding演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>embedding(<span class="built_in">input</span>)</span><br><span class="line">tensor([[[-<span class="number">0.0251</span>, -<span class="number">1.6902</span>,  <span class="number">0.7172</span>],</span><br><span class="line">         [-<span class="number">0.6431</span>,  <span class="number">0.0748</span>,  <span class="number">0.6969</span>],</span><br><span class="line">         [ <span class="number">1.4970</span>,  <span class="number">1.3448</span>, -<span class="number">0.9685</span>],</span><br><span class="line">         [-<span class="number">0.3677</span>, -<span class="number">2.7265</span>, -<span class="number">0.1685</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.4970</span>,  <span class="number">1.3448</span>, -<span class="number">0.9685</span>],</span><br><span class="line">         [ <span class="number">0.4362</span>, -<span class="number">0.4004</span>,  <span class="number">0.9400</span>],</span><br><span class="line">         [-<span class="number">0.6431</span>,  <span class="number">0.0748</span>,  <span class="number">0.6969</span>],</span><br><span class="line">         [ <span class="number">0.9124</span>, -<span class="number">2.3616</span>,  <span class="number">1.1151</span>]]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>embedding = nn.Embedding(<span class="number">10</span>, <span class="number">3</span>, padding_idx=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.LongTensor([[<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>embedding(<span class="built_in">input</span>)</span><br><span class="line">tensor([[[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [ <span class="number">0.1535</span>, -<span class="number">2.0309</span>,  <span class="number">0.9315</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">         [-<span class="number">0.1655</span>,  <span class="number">0.9897</span>,  <span class="number">0.0635</span>]]])</span><br></pre></td></tr></table></figure><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表大小是1000</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br></pre></td></tr></table></figure><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4</span></span><br><span class="line">x = Variable(torch.LongTensor([[<span class="number">100</span>,<span class="number">2</span>,<span class="number">421</span>,<span class="number">508</span>],[<span class="number">491</span>,<span class="number">998</span>,<span class="number">1</span>,<span class="number">221</span>]]))</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line">embr = emb(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;embr:&quot;</span>, embr)</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">embr: Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">35.9321</span>   <span class="number">3.2582</span> -<span class="number">17.7301</span>  ...    <span class="number">3.4109</span>  <span class="number">13.8832</span>  <span class="number">39.0272</span></span><br><span class="line">   <span class="number">8.5410</span>  -<span class="number">3.5790</span> -<span class="number">12.0460</span>  ...   <span class="number">40.1880</span>  <span class="number">36.6009</span>  <span class="number">34.7141</span></span><br><span class="line"> -<span class="number">17.0650</span>  -<span class="number">1.8705</span> -<span class="number">20.1807</span>  ...  -<span class="number">12.5556</span> -<span class="number">34.0739</span>  <span class="number">35.6536</span></span><br><span class="line">  <span class="number">20.6105</span>   <span class="number">4.4314</span>  <span class="number">14.9912</span>  ...   -<span class="number">0.1342</span>  -<span class="number">9.9270</span>  <span class="number">28.6771</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line">  <span class="number">27.7016</span>  <span class="number">16.7183</span>  <span class="number">46.6900</span>  ...   <span class="number">17.9840</span>  <span class="number">17.2525</span>  -<span class="number">3.9709</span></span><br><span class="line">   <span class="number">3.0645</span>  -<span class="number">5.5105</span>  <span class="number">10.8802</span>  ...  -<span class="number">13.0069</span>  <span class="number">30.8834</span> -<span class="number">38.3209</span></span><br><span class="line">  <span class="number">33.1378</span> -<span class="number">32.1435</span>  -<span class="number">3.9369</span>  ...   <span class="number">15.6094</span> -<span class="number">29.7063</span>  <span class="number">40.1361</span></span><br><span class="line"> -<span class="number">31.5056</span>   <span class="number">3.3648</span>   <span class="number">1.4726</span>  ...    <span class="number">2.8047</span>  -<span class="number">9.6514</span> -<span class="number">23.4909</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><ul><li>输出效果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding = nn.Embedding(10, 3)</span></span><br><span class="line"><span class="comment"># input1 = torch.LongTensor([[1,2,4,5],[4,3,2,9]])</span></span><br><span class="line"><span class="comment"># embedding = nn.Embedding(10, 3)</span></span><br><span class="line"><span class="comment"># input1 = torch.LongTensor([[1 ,2, 4, 5], [4, 3, 2, 9]])</span></span><br><span class="line"><span class="comment"># print(embedding(input1))</span></span><br><span class="line"><span class="comment"># embedding = nn.Embedding(10, 3, padding_idx=0)</span></span><br><span class="line"><span class="comment"># input1 = torch.LongTensor([[0, 2, 0, 5]])</span></span><br><span class="line"><span class="comment"># print(embedding(input1))</span></span><br><span class="line"><span class="comment"># 构建Embedding类来实现文本嵌入层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="comment"># d_model: 词嵌入的维度</span></span><br><span class="line">        <span class="comment"># vocab: 词表的大小</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 定义Embedding层</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment"># 将参数传入到类中</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x：代表输入进模型的文本通过词汇映射后的数字张量</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化参数</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">x = Variable(torch.LongTensor([[<span class="number">100</span>, <span class="number">2</span>, <span class="number">421</span>, <span class="number">508</span>], [<span class="number">491</span>, <span class="number">998</span>, <span class="number">1</span>, <span class="number">221</span>]]))</span><br><span class="line"></span><br><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line">embr = emb(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;embr:&quot;</span>, embr)</span><br><span class="line"><span class="built_in">print</span>(embr.shape)</span><br></pre></td></tr></table></figure><p><code>P 6 位置编码器的作用</code></p><p>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失</p><ul><li>位置编码器的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度, </span></span><br><span class="line"><span class="string">           dropout: 置0比率, max_len: 每个句子的最大长度&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)<span class="comment"># 行 和 列</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span></span><br><span class="line">        <span class="comment"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span></span><br><span class="line">        <span class="comment"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span></span><br><span class="line">        <span class="comment"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span></span><br><span class="line">        <span class="comment"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span></span><br><span class="line">        <span class="comment"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span></span><br><span class="line">        <span class="comment"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span></span><br><span class="line">        <span class="comment"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span></span><br><span class="line">        <span class="comment"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span></span><br><span class="line">        <span class="comment"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)<span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span></span><br><span class="line">        <span class="comment"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span></span><br><span class="line">        <span class="comment"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span></span><br><span class="line">        <span class="comment"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数的参数是x, 表示文本序列的词嵌入表示&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span></span><br><span class="line">        <span class="comment"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span></span><br><span class="line">        <span class="comment"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 最后使用self.dropout对象进行&#x27;丢弃&#x27;操作, 并返回结果.</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><ul><li>nn.Dropout演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">Variable containing:</span><br><span class="line"> <span class="number">0.0000</span> -<span class="number">0.5856</span> -<span class="number">1.4094</span>  <span class="number">0.0000</span> -<span class="number">1.0290</span></span><br><span class="line"> <span class="number">2.0591</span> -<span class="number">1.3400</span> -<span class="number">1.7247</span> -<span class="number">0.9885</span>  <span class="number">0.1286</span></span><br><span class="line"> <span class="number">0.5099</span>  <span class="number">1.3715</span>  <span class="number">0.0000</span>  <span class="number">2.2079</span> -<span class="number">0.5497</span></span><br><span class="line">-<span class="number">0.0000</span> -<span class="number">0.7839</span> -<span class="number">1.2434</span> -<span class="number">0.1222</span>  <span class="number">1.2815</span></span><br><span class="line">[torch.FloatTensor of size 4x5]</span><br></pre></td></tr></table></figure><ul><li>torch.unsqueeze演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 置0比率为0.1</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 句子最大长度</span></span><br><span class="line">max_len=<span class="number">60</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入x是Embedding层的输出的张量, 形状是2 x 4 x 512</span></span><br><span class="line">x = embr</span><br><span class="line">Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">35.9321</span>   <span class="number">3.2582</span> -<span class="number">17.7301</span>  ...    <span class="number">3.4109</span>  <span class="number">13.8832</span>  <span class="number">39.0272</span></span><br><span class="line">   <span class="number">8.5410</span>  -<span class="number">3.5790</span> -<span class="number">12.0460</span>  ...   <span class="number">40.1880</span>  <span class="number">36.6009</span>  <span class="number">34.7141</span></span><br><span class="line"> -<span class="number">17.0650</span>  -<span class="number">1.8705</span> -<span class="number">20.1807</span>  ...  -<span class="number">12.5556</span> -<span class="number">34.0739</span>  <span class="number">35.6536</span></span><br><span class="line">  <span class="number">20.6105</span>   <span class="number">4.4314</span>  <span class="number">14.9912</span>  ...   -<span class="number">0.1342</span>  -<span class="number">9.9270</span>  <span class="number">28.6771</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line">  <span class="number">27.7016</span>  <span class="number">16.7183</span>  <span class="number">46.6900</span>  ...   <span class="number">17.9840</span>  <span class="number">17.2525</span>  -<span class="number">3.9709</span></span><br><span class="line">   <span class="number">3.0645</span>  -<span class="number">5.5105</span>  <span class="number">10.8802</span>  ...  -<span class="number">13.0069</span>  <span class="number">30.8834</span> -<span class="number">38.3209</span></span><br><span class="line">  <span class="number">33.1378</span> -<span class="number">32.1435</span>  -<span class="number">3.9369</span>  ...   <span class="number">15.6094</span> -<span class="number">29.7063</span>  <span class="number">40.1361</span></span><br><span class="line"> -<span class="number">31.5056</span>   <span class="number">3.3648</span>   <span class="number">1.4726</span>  ...    <span class="number">2.8047</span>  -<span class="number">9.6514</span> -<span class="number">23.4909</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">pe_result = pe(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;pe_result:&quot;</span>, pe_result)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pe_result: Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line"> -<span class="number">19.7050</span>   <span class="number">0.0000</span>   <span class="number">0.0000</span>  ...  -<span class="number">11.7557</span>  -<span class="number">0.0000</span>  <span class="number">23.4553</span></span><br><span class="line">  -<span class="number">1.4668</span> -<span class="number">62.2510</span>  -<span class="number">2.4012</span>  ...   <span class="number">66.5860</span> -<span class="number">24.4578</span> -<span class="number">37.7469</span></span><br><span class="line">   <span class="number">9.8642</span> -<span class="number">41.6497</span> -<span class="number">11.4968</span>  ...  -<span class="number">21.1293</span> -<span class="number">42.0945</span>  <span class="number">50.7943</span></span><br><span class="line">   <span class="number">0.0000</span>  <span class="number">34.1785</span> -<span class="number">33.0712</span>  ...   <span class="number">48.5520</span>   <span class="number">3.2540</span>  <span class="number">54.1348</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line">   <span class="number">7.7598</span> -<span class="number">21.0359</span>  <span class="number">15.0595</span>  ...  -<span class="number">35.6061</span>  -<span class="number">0.0000</span>   <span class="number">4.1772</span></span><br><span class="line"> -<span class="number">38.7230</span>   <span class="number">8.6578</span>  <span class="number">34.2935</span>  ...  -<span class="number">43.3556</span>  <span class="number">26.6052</span>   <span class="number">4.3084</span></span><br><span class="line">  <span class="number">24.6962</span>  <span class="number">37.3626</span> -<span class="number">26.9271</span>  ...   <span class="number">49.8989</span>   <span class="number">0.0000</span>  <span class="number">44.9158</span></span><br><span class="line"> -<span class="number">28.8435</span> -<span class="number">48.5963</span>  -<span class="number">0.9892</span>  ...  -<span class="number">52.5447</span>  -<span class="number">4.1475</span>  -<span class="number">3.0450</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><hr><ul><li>绘制词汇向量中特征的分布曲线:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一张15 x 5大小的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化PositionalEncoding类得到pe对象, 输入参数是20和0</span></span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, </span></span><br><span class="line"><span class="comment"># 且这个tensor里的数值都是0, 被处理后相当于位置编码张量</span></span><br><span class="line">y = pe(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值</span></span><br><span class="line"><span class="comment"># 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在画布上填写维度提示信息</span></span><br><span class="line">plt.legend([<span class="string">&quot;dim %d&quot;</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><p><img src="https://pic3.zhimg.com/80/v2-0c210867908a13e1f477e491ec5a2062_720w.webp" alt="img"></p><p>效果分析:</p><ul><li><p>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</p></li><li><p>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</p></li><li><p>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</p></li></ul><p><code>小节总结</code> p8</p><ul><li>学习了文本嵌入层的作用:<ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li></ul></li></ul><hr><ul><li>学习并实现了文本嵌入层的类: Embeddings<ul><li>初始化函数以d_model, 词嵌入维度, 和vocab, 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li><li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下d_model进行缩放, 控制数值大小.</li><li>它的输出是文本嵌入后的结果.</li></ul></li></ul><hr><ul><li>学习了位置编码器的作用:<ul><li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li></ul></li></ul><hr><ul><li>学习并实现了位置编码器的类: PositionalEncoding<ul><li>初始化函数以d_model, dropout, max_len为参数, 分别代表d_model: 词嵌入维度, dropout: 置0比率, max_len: 每个句子的最大长度.</li><li>forward函数中的输入参数为x, 是Embedding层的输出.</li><li>最终输出一个加入了位置编码信息的词嵌入张量.</li></ul></li></ul><hr><ul><li>实现了绘制词汇向量中特征的分布曲线:<ul><li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li><li>正弦波和余弦波的值域范围都是1到-1, 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li></ul></li></ul><h2 id="2-3-编码器部分实现-P10"><a href="#2-3-编码器部分实现-P10" class="headerlink" title="2.3 编码器部分实现 P10"></a>2.3 编码器部分实现 P10</h2><ul><li><p>学习目标</p><ul><li><p>了解编码器中各个组成部分的作用.</p></li><li><p>掌握编码器中各个组成部分的实现过程.</p></li></ul></li></ul><hr><ul><li>编码器部分:<ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp" alt="img"></p><ul><li>什么是掩码张量:<ul><li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有<code>1</code>和<code>0</code>的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是<code>让另外一个张量中的一些数值被遮掩</code>，也可以说被替换, 它的表现形式是一个张量.</li></ul></li></ul><hr><ul><li>掩码张量的作用:<ul><li>在transformer中, 掩码张量的主要作用在应用<code>attention</code>(将在下一小节讲解)时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li></ul></li></ul><hr><ul><li>生成掩码张量的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 首先定义掩码张量的形状</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span></span><br><span class="line">    <span class="comment"># 再使其中的数据类型变为无符号8位整形unit8 </span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span></span><br><span class="line">    <span class="comment"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span></span><br><span class="line">    <span class="comment"># 如果是0, subsequent_mask中的该位置由0变成1</span></span><br><span class="line">    <span class="comment"># 如果是1, subsequent_mask中的该位置由1变成0 </span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(<span class="number">1</span> - subsequent_mask)</span><br></pre></td></tr></table></figure><hr><ul><li>np.triu演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.triu([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]], k=-<span class="number">1</span>)</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.triu([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]], k=<span class="number">0</span>)</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">9</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.triu([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]], k=<span class="number">1</span>)</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">6</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><hr><ul><li>输入实例:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成的掩码张量的最后两维的大小</span></span><br><span class="line">size = <span class="number">5</span></span><br></pre></td></tr></table></figure><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sm = subsequent_mask(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;sm:&quot;</span>, sm)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最后两维形成一个下三角阵</span></span><br><span class="line">sm: (<span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span></span><br><span class="line">[torch.ByteTensor of size 1x5x5]</span><br></pre></td></tr></table></figure><hr><p><code>P 11 </code></p><ul><li>掩码张量的可视化:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><p><img src="https://pic1.zhimg.com/80/v2-7e76569222acd94a74de6d2ba5efa824_1440w.webp" alt="img"></p><p>效果分析:</p><ul><li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置;</li><li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li></ul><p><code>总结</code></p><ul><li>学习了什么是掩码张量:<ul><li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩, 也可以说被替换, 它的表现形式是一个张量.</li></ul></li></ul><hr><ul><li>学习了掩码张量的作用:<ul><li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attetion张量中的值计算有可能已知量未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li></ul></li></ul><hr><ul><li>学习并实现了生成向后遮掩的掩码张量函数: subsequent_mask<ul><li>它的输入是size, 代表掩码张量的大小.</li><li>它的输出是一个最后两维形成1方阵的下三角阵.</li><li>最后对生成的掩码张量进行了可视化分析, 更深一步理解了它的用途.</li></ul></li></ul><h3 id="2-3-2-注意力机制-P13"><a href="#2-3-2-注意力机制-P13" class="headerlink" title="2.3.2 注意力机制 P13"></a>2.3.2 注意力机制 P13</h3><ul><li>学习目标:<ul><li>了解什么是注意力计算规则和注意力机制.</li><li>掌握注意力计算规则的实现过程.</li></ul></li></ul><hr><ul><li>什么是注意力:<ul><li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力计算规则:<ul><li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li></ul></li></ul><hr><ul><li><p>我们这里使用的注意力的计算规则:<br>$$<br>\mathrm{Attention}(Q,K,V)&#x3D;\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V<br>$$</p></li><li><p>Q, K, V 的比喻解释:</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">假如我们有一个问题: 给出一段文本，使用一些关键词对它进行描述!</span><br><span class="line">为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key</span><br><span class="line">而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息</span><br><span class="line">这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息</span><br><span class="line">因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多</span><br><span class="line">并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，</span><br><span class="line">我们最终脑子里的value发生了变化</span><br><span class="line">根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.</span><br><span class="line"></span><br><span class="line">刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式</span><br><span class="line"><span class="comment"># 但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子</span></span><br><span class="line">使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制</span><br><span class="line">需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.</span><br></pre></td></tr></table></figure><hr><ul><li>什么是注意力机制:<ul><li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li></ul></li></ul><hr><ul><li>注意力机制在网络中实现的图形表示:</li></ul><p><img src="https://pic1.zhimg.com/80/v2-d7913da8a83d6ca9d67524560b263688_1440w.webp" alt="img"></p><p><code>P14</code> </p><ul><li>注意力计算规则的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;注意力机制的实现, 输入分别是query, key, value, mask: 掩码张量, </span></span><br><span class="line"><span class="string">       dropout是nn.Dropout层的实例化对象, 默认为None&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 首先取query的最后一维的大小, 一般情况下就等同于我们的词嵌入维度, 命名为d_k</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.</span></span><br><span class="line">    <span class="comment"># 得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, 如果掩码张量处为0</span></span><br><span class="line">        <span class="comment"># 则对应的scores张量用-1e9这个值来替换, 如下演示</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.</span></span><br><span class="line">    <span class="comment"># 这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 将p_attn传入dropout对象中进行&#x27;丢弃&#x27;处理</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><ul><li>tensor.masked_fill演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = Variable(torch.randn(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> </span><br><span class="line">Variable containing:</span><br><span class="line"> <span class="number">2.0344</span> -<span class="number">0.5450</span>  <span class="number">0.3365</span> -<span class="number">0.1888</span> -<span class="number">2.1803</span></span><br><span class="line"> <span class="number">1.5221</span> -<span class="number">0.3823</span>  <span class="number">0.8414</span>  <span class="number">0.7836</span> -<span class="number">0.8481</span></span><br><span class="line">-<span class="number">0.0345</span> -<span class="number">0.8643</span>  <span class="number">0.6476</span> -<span class="number">0.2713</span>  <span class="number">1.5645</span></span><br><span class="line"> <span class="number">0.8788</span> -<span class="number">2.2142</span>  <span class="number">0.4022</span>  <span class="number">0.1997</span>  <span class="number">0.1474</span></span><br><span class="line"> <span class="number">2.9109</span>  <span class="number">0.6006</span> -<span class="number">0.6745</span> -<span class="number">1.7262</span>  <span class="number">0.6977</span></span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = Variable(torch.zeros(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">Variable containing:</span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span>.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">Variable containing:</span><br><span class="line">-<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span></span><br><span class="line">-<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span></span><br><span class="line">-<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span></span><br><span class="line">-<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span></span><br><span class="line">-<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span> -<span class="number">1.0000e+09</span></span><br><span class="line">[torch.FloatTensor of size 5x5]</span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们令输入的query, key, value都相同, 位置编码的输出</span></span><br><span class="line">query = key = value = pe_result</span><br><span class="line">Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">46.5196</span>  <span class="number">16.2057</span> -<span class="number">41.5581</span>  ...  -<span class="number">16.0242</span> -<span class="number">17.8929</span> -<span class="number">43.0405</span></span><br><span class="line"> -<span class="number">32.6040</span>  <span class="number">16.1096</span> -<span class="number">29.5228</span>  ...    <span class="number">4.2721</span>  <span class="number">20.6034</span>  -<span class="number">1.2747</span></span><br><span class="line"> -<span class="number">18.6235</span>  <span class="number">14.5076</span>  -<span class="number">2.0105</span>  ...   <span class="number">15.6462</span> -<span class="number">24.6081</span> -<span class="number">30.3391</span></span><br><span class="line">   <span class="number">0.0000</span> -<span class="number">66.1486</span> -<span class="number">11.5123</span>  ...   <span class="number">20.1519</span>  -<span class="number">4.6823</span>   <span class="number">0.4916</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line"> -<span class="number">24.8681</span>   <span class="number">7.5495</span>  -<span class="number">5.0765</span>  ...   -<span class="number">7.5992</span> -<span class="number">26.6630</span>  <span class="number">40.9517</span></span><br><span class="line">  <span class="number">13.1581</span>  -<span class="number">3.1918</span> -<span class="number">30.9001</span>  ...   <span class="number">25.1187</span> -<span class="number">26.4621</span>   <span class="number">2.9542</span></span><br><span class="line"> -<span class="number">49.7690</span> -<span class="number">42.5019</span>   <span class="number">8.0198</span>  ...   -<span class="number">5.4809</span>  <span class="number">25.9403</span> -<span class="number">27.4931</span></span><br><span class="line"> -<span class="number">52.2775</span>  <span class="number">10.4006</span>   <span class="number">0.0000</span>  ...   -<span class="number">1.9985</span>   <span class="number">7.0106</span>  -<span class="number">0.5189</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">attn, p_attn = attention(query, key, value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将得到两个结果</span></span><br><span class="line"><span class="comment"># query的注意力表示:</span></span><br><span class="line">attn: Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line">   <span class="number">12.8269</span>    <span class="number">7.7403</span>   <span class="number">41.2225</span>  ...     <span class="number">1.4603</span>   <span class="number">27.8559</span>  -<span class="number">12.2600</span></span><br><span class="line">   <span class="number">12.4904</span>    <span class="number">0.0000</span>   <span class="number">24.1575</span>  ...     <span class="number">0.0000</span>    <span class="number">2.5838</span>   <span class="number">18.0647</span></span><br><span class="line">  -<span class="number">32.5959</span>   -<span class="number">4.6252</span>  -<span class="number">29.1050</span>  ...     <span class="number">0.0000</span>  -<span class="number">22.6409</span>  -<span class="number">11.8341</span></span><br><span class="line">    <span class="number">8.9921</span>  -<span class="number">33.0114</span>   -<span class="number">0.7393</span>  ...     <span class="number">4.7871</span>   -<span class="number">5.7735</span>    <span class="number">8.3374</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line">  -<span class="number">25.6705</span>   -<span class="number">4.0860</span>  -<span class="number">36.8226</span>  ...    <span class="number">37.2346</span>  -<span class="number">27.3576</span>    <span class="number">2.5497</span></span><br><span class="line">  -<span class="number">16.6674</span>   <span class="number">73.9788</span>  -<span class="number">33.3296</span>  ...    <span class="number">28.5028</span>   -<span class="number">5.5488</span>  -<span class="number">13.7564</span></span><br><span class="line">    <span class="number">0.0000</span>  -<span class="number">29.9039</span>   -<span class="number">3.0405</span>  ...     <span class="number">0.0000</span>   <span class="number">14.4408</span>   <span class="number">14.8579</span></span><br><span class="line">   <span class="number">30.7819</span>    <span class="number">0.0000</span>   <span class="number">21.3908</span>  ...   -<span class="number">29.0746</span>    <span class="number">0.0000</span>   -<span class="number">5.8475</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力张量:</span></span><br><span class="line">p_attn: Variable containing:</span><br><span class="line">(<span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span></span><br><span class="line"></span><br><span class="line">(<span class="number">1</span> ,.,.) = </span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 2x4x4]</span><br></pre></td></tr></table></figure><ul><li>带有mask的输入参数：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">query = key = value = pe_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 令mask为一个2x4x4的零张量</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">attn, p_attn = attention(query, key, value, mask=mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br></pre></td></tr></table></figure><hr><ul><li>带有mask的输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query的注意力表示:</span></span><br><span class="line">attn: Variable containing:</span><br><span class="line">( <span class="number">0</span> ,.,.) = </span><br><span class="line">   <span class="number">0.4284</span>  -<span class="number">7.4741</span>   <span class="number">8.8839</span>  ...    <span class="number">1.5618</span>   <span class="number">0.5063</span>   <span class="number">0.5770</span></span><br><span class="line">   <span class="number">0.4284</span>  -<span class="number">7.4741</span>   <span class="number">8.8839</span>  ...    <span class="number">1.5618</span>   <span class="number">0.5063</span>   <span class="number">0.5770</span></span><br><span class="line">   <span class="number">0.4284</span>  -<span class="number">7.4741</span>   <span class="number">8.8839</span>  ...    <span class="number">1.5618</span>   <span class="number">0.5063</span>   <span class="number">0.5770</span></span><br><span class="line">   <span class="number">0.4284</span>  -<span class="number">7.4741</span>   <span class="number">8.8839</span>  ...    <span class="number">1.5618</span>   <span class="number">0.5063</span>   <span class="number">0.5770</span></span><br><span class="line"></span><br><span class="line">( <span class="number">1</span> ,.,.) = </span><br><span class="line">  -<span class="number">2.8890</span>   <span class="number">9.9972</span> -<span class="number">12.9505</span>  ...    <span class="number">9.1657</span>  -<span class="number">4.6164</span>  -<span class="number">0.5491</span></span><br><span class="line">  -<span class="number">2.8890</span>   <span class="number">9.9972</span> -<span class="number">12.9505</span>  ...    <span class="number">9.1657</span>  -<span class="number">4.6164</span>  -<span class="number">0.5491</span></span><br><span class="line">  -<span class="number">2.8890</span>   <span class="number">9.9972</span> -<span class="number">12.9505</span>  ...    <span class="number">9.1657</span>  -<span class="number">4.6164</span>  -<span class="number">0.5491</span></span><br><span class="line">  -<span class="number">2.8890</span>   <span class="number">9.9972</span> -<span class="number">12.9505</span>  ...    <span class="number">9.1657</span>  -<span class="number">4.6164</span>  -<span class="number">0.5491</span></span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力张量:</span></span><br><span class="line">p_attn: Variable containing:</span><br><span class="line">(<span class="number">0</span> ,.,.) = </span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line"></span><br><span class="line">(<span class="number">1</span> ,.,.) = </span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span>  <span class="number">0.2500</span></span><br><span class="line">[torch.FloatTensor of size 2x4x4]</span><br></pre></td></tr></table></figure><p><code>总结</code>：</p><ul><li>学习了什么是注意力:<ul><li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力计算规则:<ul><li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li></ul></li></ul><hr><ul><li>学习了<code>Q, K, V</code>的比喻解释:<ul><li>Q是一段准备被概括的文本; K是给出的提示; V是大脑中的对提示K的延伸.</li><li><code>当Q=K=V时</code>, 称作自注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力机制:<ul><li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li></ul></li></ul><hr><ul><li>学习并实现了注意力计算规则的函数: attention<ul><li>它的输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li><li>它的输出有两个, query的注意力表示以及注意力张量.</li></ul></li></ul><h3 id="2-3-3-多头注意力机制-P17"><a href="#2-3-3-多头注意力机制-P17" class="headerlink" title="2.3.3 多头注意力机制 P17"></a>2.3.3 多头注意力机制 P17</h3><ul><li>学习目标:<ul><li>了解多头注意力机制的作用.</li><li>掌握多头注意力机制的实现过程.</li></ul></li></ul><hr><ul><li>什么是多头注意力机制:<ul><li>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即<code>三个变换张量对Q，K，V分别进行线性变换</code>，这些变换<code>不会改变原有张量的尺寸</code>，因此每个变换矩阵都是<code>方阵</code>，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</li></ul></li></ul><hr><ul><li><p>多头注意力机制结构图:</p><img src="https://article.biliimg.com/bfs/article/af87236e673d1a082c9941b51678674b155712160.png" style="zoom:80%;" /></li></ul><hr><ul><li>多头注意力机制的作用:<ul><li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li></ul></li></ul><hr><ul><li>多头注意力机制的代码实现:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于深度拷贝的copy工具包</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.</span></span><br><span class="line"><span class="comment"># 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span></span><br><span class="line">    <span class="comment"># 然后将其放在nn.ModuleList类型的列表中存放.</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们使用一个类来实现多头注意力机制的处理</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, head, embedding_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度， </span></span><br><span class="line"><span class="string">           dropout代表进行dropout操作时置0比率，默认是0.1.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span></span><br><span class="line">        <span class="comment"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span></span><br><span class="line">        <span class="keyword">assert</span> embedding_dim % head == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = embedding_dim // head</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 传入头数h</span></span><br><span class="line">        self.head = head</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，</span></span><br><span class="line">        <span class="comment"># 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.</span></span><br><span class="line">        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，</span></span><br><span class="line"><span class="string">           最后一个是注意力机制中可能需要的mask掩码张量，默认是None. &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果存在掩码张量mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 使用unsqueeze拓展维度</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之后就进入多头处理环节</span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，</span></span><br><span class="line">        <span class="comment"># 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span></span><br><span class="line">        <span class="comment"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span></span><br><span class="line">        <span class="comment"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span></span><br><span class="line">        <span class="comment"># 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，</span></span><br><span class="line">        <span class="comment"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">           [model(x).view(batch_size, -<span class="number">1</span>, self.head, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> model, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span></span><br><span class="line">        <span class="comment"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，</span></span><br><span class="line">        <span class="comment"># 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，</span></span><br><span class="line">        <span class="comment"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span></span><br><span class="line">        <span class="comment"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.head * self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure><ul><li>tensor.view演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x.view(<span class="number">16</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># Swaps 2nd and 3rd dimension</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>)  <span class="comment"># Does not change tensor layout in memory</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.equal(b, c)</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><ul><li>torch.transpose演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.9893</span>,  <span class="number">0.5809</span>],</span><br><span class="line">        [-<span class="number">0.1669</span>,  <span class="number">0.7299</span>,  <span class="number">0.4942</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.transpose(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.1669</span>],</span><br><span class="line">        [-<span class="number">0.9893</span>,  <span class="number">0.7299</span>],</span><br><span class="line">        [ <span class="number">0.5809</span>,  <span class="number">0.4942</span>]])</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 头数head</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词嵌入维度embedding_dim</span></span><br><span class="line">embedding_dim = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 置零比率dropout</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入的Q，K，V仍然相等</span></span><br><span class="line">query = value = key = pe_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入的掩码张量mask</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mha = MultiHeadedAttention(head, embedding_dim, dropout)</span><br><span class="line">mha_result = mha(query, key, value, mask)</span><br><span class="line"><span class="built_in">print</span>(mha_result)</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-<span class="number">0.3075</span>,  <span class="number">1.5687</span>, -<span class="number">2.5693</span>,  ..., -<span class="number">1.1098</span>,  <span class="number">0.0878</span>, -<span class="number">3.3609</span>],</span><br><span class="line">         [ <span class="number">3.8065</span>, -<span class="number">2.4538</span>, -<span class="number">0.3708</span>,  ..., -<span class="number">1.5205</span>, -<span class="number">1.1488</span>, -<span class="number">1.3984</span>],</span><br><span class="line">         [ <span class="number">2.4190</span>,  <span class="number">0.5376</span>, -<span class="number">2.8475</span>,  ...,  <span class="number">1.4218</span>, -<span class="number">0.4488</span>, -<span class="number">0.2984</span>],</span><br><span class="line">         [ <span class="number">2.9356</span>,  <span class="number">0.3620</span>, -<span class="number">3.8722</span>,  ..., -<span class="number">0.7996</span>,  <span class="number">0.1468</span>,  <span class="number">1.0345</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1423</span>,  <span class="number">0.6038</span>,  <span class="number">0.0954</span>,  ...,  <span class="number">2.2679</span>, -<span class="number">5.7749</span>,  <span class="number">1.4132</span>],</span><br><span class="line">         [ <span class="number">2.4066</span>, -<span class="number">0.2777</span>,  <span class="number">2.8102</span>,  ...,  <span class="number">0.1137</span>, -<span class="number">3.9517</span>, -<span class="number">2.9246</span>],</span><br><span class="line">         [ <span class="number">5.8201</span>,  <span class="number">1.1534</span>, -<span class="number">1.9191</span>,  ...,  <span class="number">0.1410</span>, -<span class="number">7.6110</span>,  <span class="number">1.0046</span>],</span><br><span class="line">         [ <span class="number">3.1209</span>,  <span class="number">1.0008</span>, -<span class="number">0.5317</span>,  ...,  <span class="number">2.8619</span>, -<span class="number">6.3204</span>, -<span class="number">1.3435</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>多头注意力机制总结</code>:P 20 – 04:00</p><ul><li>学习了什么是多头注意力机制:<ul><li>每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头.将每个头的获得的输入送到注意力机制中, 就形成了多头注意力机制.</li></ul></li></ul><hr><ul><li>学习了多头注意力机制的作用:<ul><li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li></ul></li></ul><hr><ul><li>学习并实现了多头注意力机制的类: MultiHeadedAttention<ul><li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li><li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li><li>clones函数的输出是装有N个克隆层的Module列表.</li><li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li><li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li><li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li></ul></li></ul><p><code>P19</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多头注意力机制的类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, head, embedding_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment"># head:代表几个头的参数</span></span><br><span class="line">        <span class="comment"># embedding_dim:代表词嵌入的维度</span></span><br><span class="line">        <span class="comment"># dropout:进行dropout操作时，置零的比率</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 要确认一个事实:多头的数量head需要整除嵌入词的维度embedding_dim</span></span><br><span class="line">        <span class="keyword">assert</span> embedding_dim % head == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得每个头获得的词向量的维度</span></span><br><span class="line">        self.d_k = embedding_dim // head</span><br><span class="line"></span><br><span class="line">        self.head = head</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得线形层，要获得4个，分别是Q,K,V以及最终的输出线形层</span></span><br><span class="line">        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化注意力张量</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化drouput对象</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># query, key, value是注意力机制的三个输入张量，mask代表掩码张量</span></span><br><span class="line">        <span class="comment"># 首先判断是否使用掩码张量</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 使用unsqueeze将掩码张量进行维度扩充，代表多头中的第n个头</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到batch_size</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 首先使用zip将网络层和输入数据连接在一起，模型的输出利用view和transpose进行维度和形状的改变</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [model(x).view(batch_size, -<span class="number">1</span>, self.head, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> model, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将每个头的输出传入到注意力层</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头的计算结果是4维张量，需要进行形状的转换</span></span><br><span class="line">        <span class="comment"># 前面已经将1，2两个维度进行过转置，在这里必须要重新转置回来，</span></span><br><span class="line">        <span class="comment"># 注意：经历了transpose()方法后，必须使用contiguous方法，否则无法使用view(）方法</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.head * self.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 最后将x输入线形层列表中的最后一个线形层中进行处理，得到最终的多头注意力结构输出</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="2-3-4-前馈全连接层"><a href="#2-3-4-前馈全连接层" class="headerlink" title="2.3.4 前馈全连接层"></a>2.3.4 前馈全连接层</h3><ul><li>学习目标:<ul><li>了解什么是前馈全连接层及其它的作用.</li><li>掌握前馈全连接层的实现过程.</li></ul></li></ul><hr><ul><li>什么是前馈全连接层:<ul><li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li></ul></li></ul><hr><ul><li>前馈全连接层的作用:<ul><li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li></ul></li></ul><hr><ul><li>前馈全连接层的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过类PositionwiseFeedForward来实现前馈全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment"># d_model：代表词嵌入的维度，同时也是两个线性层的输入维度和输出维度</span></span><br><span class="line">        <span class="comment"># d_ff：代表第一个线性层的输出维度，和第二个线性层的输入维度</span></span><br><span class="line">        <span class="comment"># dropout：经过Dropout层处理时，随机置零的比率</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数有三个输入参数分别是d_model, d_ff,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，</span></span><br><span class="line"><span class="string">           因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度.</span></span><br><span class="line"><span class="string">           最后一个是dropout置0比率.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2</span></span><br><span class="line">        <span class="comment"># 它们的参数分别是d_model, d_ff和d_ff, d_model</span></span><br><span class="line">        self.w1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="comment"># 然后使用nn的Dropout实例化了对象self.dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;输入参数为x，代表来自上一层的输出&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span></span><br><span class="line">        <span class="comment"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果</span></span><br><span class="line">        <span class="keyword">return</span> self.w2(self.dropout(F.relu(self.w1(x))))</span><br><span class="line"></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">x = mha_result</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">ff_result = ff(x)</span><br><span class="line"><span class="built_in">print</span>(ff_result)</span><br><span class="line"><span class="built_in">print</span>(ff_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>ReLU函数公式: ReLU(x)&#x3D;max(0, x)</li></ul><hr><ul><li>ReLU函数图像:</li></ul><img src="https://article.biliimg.com/bfs/article/7e3ca1eb7eb850ebfa19b2b4039ddd59155712160.png" style="zoom:80%;" /><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性变化的维度</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">dropout = <span class="number">0.2</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入参数x可以是多头注意力机制的输出</span></span><br><span class="line">x = mha_result</span><br><span class="line">tensor([[[-<span class="number">0.3075</span>,  <span class="number">1.5687</span>, -<span class="number">2.5693</span>,  ..., -<span class="number">1.1098</span>,  <span class="number">0.0878</span>, -<span class="number">3.3609</span>],</span><br><span class="line">         [ <span class="number">3.8065</span>, -<span class="number">2.4538</span>, -<span class="number">0.3708</span>,  ..., -<span class="number">1.5205</span>, -<span class="number">1.1488</span>, -<span class="number">1.3984</span>],</span><br><span class="line">         [ <span class="number">2.4190</span>,  <span class="number">0.5376</span>, -<span class="number">2.8475</span>,  ...,  <span class="number">1.4218</span>, -<span class="number">0.4488</span>, -<span class="number">0.2984</span>],</span><br><span class="line">         [ <span class="number">2.9356</span>,  <span class="number">0.3620</span>, -<span class="number">3.8722</span>,  ..., -<span class="number">0.7996</span>,  <span class="number">0.1468</span>,  <span class="number">1.0345</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.1423</span>,  <span class="number">0.6038</span>,  <span class="number">0.0954</span>,  ...,  <span class="number">2.2679</span>, -<span class="number">5.7749</span>,  <span class="number">1.4132</span>],</span><br><span class="line">         [ <span class="number">2.4066</span>, -<span class="number">0.2777</span>,  <span class="number">2.8102</span>,  ...,  <span class="number">0.1137</span>, -<span class="number">3.9517</span>, -<span class="number">2.9246</span>],</span><br><span class="line">         [ <span class="number">5.8201</span>,  <span class="number">1.1534</span>, -<span class="number">1.9191</span>,  ...,  <span class="number">0.1410</span>, -<span class="number">7.6110</span>,  <span class="number">1.0046</span>],</span><br><span class="line">         [ <span class="number">3.1209</span>,  <span class="number">1.0008</span>, -<span class="number">0.5317</span>,  ...,  <span class="number">2.8619</span>, -<span class="number">6.3204</span>, -<span class="number">1.3435</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">ff_result = ff(x)</span><br><span class="line"><span class="built_in">print</span>(ff_result)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-<span class="number">1.9488e+00</span>, -<span class="number">3.4060e-01</span>, -<span class="number">1.1216e+00</span>,  ...,  <span class="number">1.8203e-01</span>,</span><br><span class="line">          -<span class="number">2.6336e+00</span>,  <span class="number">2.0917e-03</span>],</span><br><span class="line">         [-<span class="number">2.5875e-02</span>,  <span class="number">1.1523e-01</span>, -<span class="number">9.5437e-01</span>,  ..., -<span class="number">2.6257e-01</span>,</span><br><span class="line">          -<span class="number">5.7620e-01</span>, -<span class="number">1.9225e-01</span>],</span><br><span class="line">         [-<span class="number">8.7508e-01</span>,  <span class="number">1.0092e+00</span>, -<span class="number">1.6515e+00</span>,  ...,  <span class="number">3.4446e-02</span>,</span><br><span class="line">          -<span class="number">1.5933e+00</span>, -<span class="number">3.1760e-01</span>],</span><br><span class="line">         [-<span class="number">2.7507e-01</span>,  <span class="number">4.7225e-01</span>, -<span class="number">2.0318e-01</span>,  ...,  <span class="number">1.0530e+00</span>,</span><br><span class="line">          -<span class="number">3.7910e-01</span>, -<span class="number">9.7730e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">2.2575e+00</span>, -<span class="number">2.0904e+00</span>,  <span class="number">2.9427e+00</span>,  ...,  <span class="number">9.6574e-01</span>,</span><br><span class="line">          -<span class="number">1.9754e+00</span>,  <span class="number">1.2797e+00</span>],</span><br><span class="line">         [-<span class="number">1.5114e+00</span>, -<span class="number">4.7963e-01</span>,  <span class="number">1.2881e+00</span>,  ..., -<span class="number">2.4882e-02</span>,</span><br><span class="line">          -<span class="number">1.5896e+00</span>, -<span class="number">1.0350e+00</span>],</span><br><span class="line">         [ <span class="number">1.7416e-01</span>, -<span class="number">4.0688e-01</span>,  <span class="number">1.9289e+00</span>,  ..., -<span class="number">4.9754e-01</span>,</span><br><span class="line">          -<span class="number">1.6320e+00</span>, -<span class="number">1.5217e+00</span>],</span><br><span class="line">         [-<span class="number">1.0874e-01</span>, -<span class="number">3.3842e-01</span>,  <span class="number">2.9379e-01</span>,  ..., -<span class="number">5.1276e-01</span>,</span><br><span class="line">          -<span class="number">1.6150e+00</span>, -<span class="number">1.1295e+00</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>前馈全连接层总结</code>:</p><ul><li>学习了什么是前馈全连接层:<ul><li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li></ul></li></ul><hr><ul><li>学习了前馈全连接层的作用:<ul><li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li></ul></li></ul><hr><ul><li>学习并实现了前馈全连接层的类: PositionwiseFeedForward<ul><li>它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li><li>它的输入参数x, 表示上层的输出.</li><li>它的输出是经过2层线性网络变换的特征表示.</li></ul></li></ul><h3 id="2-3-5-规范化层"><a href="#2-3-5-规范化层" class="headerlink" title="2.3.5 规范化层"></a>2.3.5 规范化层</h3><ul><li>学习目标:<ul><li>了解规范化层的作用.</li><li>掌握规范化层的实现过程.</li></ul></li></ul><hr><ul><li>规范化层的作用:<ul><li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li></ul></li></ul><hr><ul><li>规范化层的代码实现:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"> 通过LayerNorm实现规范化层的类</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment"># features: 词嵌入的维度</span></span><br><span class="line">        <span class="comment"># eps： 一个足够小的正数，用来在规范化计算公式的分母中，放置除零操作</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数有两个参数, 一个是features, 表示词嵌入的维度,</span></span><br><span class="line"><span class="string">           另一个是eps它是一个足够小的数, 在规范化公式的分母中出现,</span></span><br><span class="line"><span class="string">           防止分母为0.默认是1e-6.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span></span><br><span class="line">        <span class="comment"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数，</span></span><br><span class="line">        <span class="comment"># 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，</span></span><br><span class="line">        <span class="comment"># 使其即能满足规范化要求，又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span></span><br><span class="line">        self.a2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b2 = nn.Parameter(torch.zeros(features))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把eps传到类中</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x：上一层网络的输出</span></span><br><span class="line">        <span class="comment"># 首先对x进行最后一个维度上的求均值操作，同时保持输出维度和输入维度一直</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;输入参数x代表来自上一层的输出&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span></span><br><span class="line">        <span class="comment"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果，</span></span><br><span class="line">        <span class="comment"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可.</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)<span class="comment"># -1:在最后一个维度上</span></span><br><span class="line">        <span class="comment"># 按照规范化公式进行计算并返回</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a2 * (x - mean) / (std + self.eps) + self.b2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = d_model = <span class="number">512</span></span><br><span class="line">eps = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">x = ff_result</span><br><span class="line">ln = LayerNorm(features, eps)</span><br><span class="line">ln_result = ln(x)</span><br><span class="line"><span class="built_in">print</span>(ln_result)</span><br><span class="line"><span class="built_in">print</span>(ln_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">features = d_model = <span class="number">512</span></span><br><span class="line">eps = <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入x来自前馈全连接层的输出</span></span><br><span class="line">x = ff_result</span><br><span class="line">tensor([[[-<span class="number">1.9488e+00</span>, -<span class="number">3.4060e-01</span>, -<span class="number">1.1216e+00</span>,  ...,  <span class="number">1.8203e-01</span>,</span><br><span class="line">          -<span class="number">2.6336e+00</span>,  <span class="number">2.0917e-03</span>],</span><br><span class="line">         [-<span class="number">2.5875e-02</span>,  <span class="number">1.1523e-01</span>, -<span class="number">9.5437e-01</span>,  ..., -<span class="number">2.6257e-01</span>,</span><br><span class="line">          -<span class="number">5.7620e-01</span>, -<span class="number">1.9225e-01</span>],</span><br><span class="line">         [-<span class="number">8.7508e-01</span>,  <span class="number">1.0092e+00</span>, -<span class="number">1.6515e+00</span>,  ...,  <span class="number">3.4446e-02</span>,</span><br><span class="line">          -<span class="number">1.5933e+00</span>, -<span class="number">3.1760e-01</span>],</span><br><span class="line">         [-<span class="number">2.7507e-01</span>,  <span class="number">4.7225e-01</span>, -<span class="number">2.0318e-01</span>,  ...,  <span class="number">1.0530e+00</span>,</span><br><span class="line">          -<span class="number">3.7910e-01</span>, -<span class="number">9.7730e-01</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">2.2575e+00</span>, -<span class="number">2.0904e+00</span>,  <span class="number">2.9427e+00</span>,  ...,  <span class="number">9.6574e-01</span>,</span><br><span class="line">          -<span class="number">1.9754e+00</span>,  <span class="number">1.2797e+00</span>],</span><br><span class="line">         [-<span class="number">1.5114e+00</span>, -<span class="number">4.7963e-01</span>,  <span class="number">1.2881e+00</span>,  ..., -<span class="number">2.4882e-02</span>,</span><br><span class="line">          -<span class="number">1.5896e+00</span>, -<span class="number">1.0350e+00</span>],</span><br><span class="line">         [ <span class="number">1.7416e-01</span>, -<span class="number">4.0688e-01</span>,  <span class="number">1.9289e+00</span>,  ..., -<span class="number">4.9754e-01</span>,</span><br><span class="line">          -<span class="number">1.6320e+00</span>, -<span class="number">1.5217e+00</span>],</span><br><span class="line">         [-<span class="number">1.0874e-01</span>, -<span class="number">3.3842e-01</span>,  <span class="number">2.9379e-01</span>,  ..., -<span class="number">5.1276e-01</span>,</span><br><span class="line">          -<span class="number">1.6150e+00</span>, -<span class="number">1.1295e+00</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln = LayerNorm(feature, eps)</span><br><span class="line">ln_result = ln(x)</span><br><span class="line"><span class="built_in">print</span>(ln_result)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">2.2697</span>,  <span class="number">1.3911</span>, -<span class="number">0.4417</span>,  ...,  <span class="number">0.9937</span>,  <span class="number">0.6589</span>, -<span class="number">1.1902</span>],</span><br><span class="line">         [ <span class="number">1.5876</span>,  <span class="number">0.5182</span>,  <span class="number">0.6220</span>,  ...,  <span class="number">0.9836</span>,  <span class="number">0.0338</span>, -<span class="number">1.3393</span>],</span><br><span class="line">         [ <span class="number">1.8261</span>,  <span class="number">2.0161</span>,  <span class="number">0.2272</span>,  ...,  <span class="number">0.3004</span>,  <span class="number">0.5660</span>, -<span class="number">0.9044</span>],</span><br><span class="line">         [ <span class="number">1.5429</span>,  <span class="number">1.3221</span>, -<span class="number">0.2933</span>,  ...,  <span class="number">0.0406</span>,  <span class="number">1.0603</span>,  <span class="number">1.4666</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.2378</span>,  <span class="number">0.9952</span>,  <span class="number">1.2621</span>,  ..., -<span class="number">0.4334</span>, -<span class="number">1.1644</span>,  <span class="number">1.2082</span>],</span><br><span class="line">         [-<span class="number">1.0209</span>,  <span class="number">0.6435</span>,  <span class="number">0.4235</span>,  ..., -<span class="number">0.3448</span>, -<span class="number">1.0560</span>,  <span class="number">1.2347</span>],</span><br><span class="line">         [-<span class="number">0.8158</span>,  <span class="number">0.7118</span>,  <span class="number">0.4110</span>,  ...,  <span class="number">0.0990</span>, -<span class="number">1.4833</span>,  <span class="number">1.9434</span>],</span><br><span class="line">         [ <span class="number">0.9857</span>,  <span class="number">2.3924</span>,  <span class="number">0.3819</span>,  ...,  <span class="number">0.0157</span>, -<span class="number">1.6300</span>,  <span class="number">1.2251</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>规范化层总结</code>:</p><ul><li>学习了规范化层的作用:<ul><li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li></ul></li></ul><hr><ul><li>学习并实现了规范化层的类: LayerNorm<ul><li>它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.</li><li>它的输入参数x代表来自上一层的输出.</li><li>它的输出就是经过规范化的特征表示.</li></ul></li></ul><h3 id="2-3-6-子层连接结构"><a href="#2-3-6-子层连接结构" class="headerlink" title="2.3.6 子层连接结构"></a>2.3.6 子层连接结构</h3><ul><li>学习目标:<ul><li>了解什么是子层连接结构.</li><li>掌握子层连接结构的实现过程.</li></ul></li></ul><hr><ul><li>什么是子层连接结构:<ul><li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li></ul></li></ul><hr><ul><li><p>子层连接结构图:</p><p><img src="https://article.biliimg.com/bfs/article/122092d91f732d6ea7ef61d100d3f1fc155712160.png"></p></li></ul><hr><p><img src="https://article.biliimg.com/bfs/article/442c5ecb840fdeae191a0184e836f395155712160.png"></p><ul><li>子层连接结构的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用SublayerConnection来实现子层连接结构的类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;它输入参数有两个, size以及dropout， size一般是都是词嵌入维度的大小， </span></span><br><span class="line"><span class="string">           dropout本身是对模型结构中的节点数进行随机抑制的比率， </span></span><br><span class="line"><span class="string">           又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机置0的比率.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># 实例化了规范化对象self.norm</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        <span class="comment"># 又使用nn中预定义的droupout实例化一个self.dropout对象.</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，</span></span><br><span class="line"><span class="string">           将该子层连接中的子层函数作为第二个参数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span></span><br><span class="line">        <span class="comment"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span></span><br><span class="line">        <span class="comment"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment">###################################################</span></span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">x = pe_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">self_attn = MultiHeadedAttention(head, d_model)</span><br><span class="line"></span><br><span class="line">sublayer = <span class="keyword">lambda</span> x: self_attn(x, x, x, mask)</span><br><span class="line"></span><br><span class="line">sc = SublayerConnection(size, dropout)</span><br><span class="line">sc_result = sc(x, sublayer)</span><br><span class="line"><span class="built_in">print</span>(sc_result)</span><br><span class="line"><span class="built_in">print</span>(sc_result.shape)</span><br></pre></td></tr></table></figure><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 令x为位置编码器的输出</span></span><br><span class="line">x = pe_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设子层中装的是多头注意力层, 实例化这个类</span></span><br><span class="line">self_attn =  MultiHeadedAttention(head, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用lambda获得一个函数类型的子层</span></span><br><span class="line">sublayer = <span class="keyword">lambda</span> x: self_attn(x, x, x, mask)</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc = SublayerConnection(size, dropout)</span><br><span class="line">sc_result = sc(x, sublayer)</span><br><span class="line"><span class="built_in">print</span>(sc_result)</span><br><span class="line"><span class="built_in">print</span>(sc_result.shape)</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">14.8830</span>,  <span class="number">22.4106</span>, -<span class="number">31.4739</span>,  ...,  <span class="number">21.0882</span>, -<span class="number">10.0338</span>,  -<span class="number">0.2588</span>],</span><br><span class="line">         [-<span class="number">25.1435</span>,   <span class="number">2.9246</span>, -<span class="number">16.1235</span>,  ...,  <span class="number">10.5069</span>,  -<span class="number">7.1007</span>,  -<span class="number">3.7396</span>],</span><br><span class="line">         [  <span class="number">0.1374</span>,  <span class="number">32.6438</span>,  <span class="number">12.3680</span>,  ..., -<span class="number">12.0251</span>, -<span class="number">40.5829</span>,   <span class="number">2.2297</span>],</span><br><span class="line">         [-<span class="number">13.3123</span>,  <span class="number">55.4689</span>,   <span class="number">9.5420</span>,  ..., -<span class="number">12.6622</span>,  <span class="number">23.4496</span>,  <span class="number">21.1531</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">13.3533</span>,  <span class="number">17.5674</span>, -<span class="number">13.3354</span>,  ...,  <span class="number">29.1366</span>,  -<span class="number">6.4898</span>,  <span class="number">35.8614</span>],</span><br><span class="line">         [-<span class="number">35.2286</span>,  <span class="number">18.7378</span>, -<span class="number">31.4337</span>,  ...,  <span class="number">11.1726</span>,  <span class="number">20.6372</span>,  <span class="number">29.8689</span>],</span><br><span class="line">         [-<span class="number">30.7627</span>,   <span class="number">0.0000</span>, -<span class="number">57.0587</span>,  ...,  <span class="number">15.0724</span>, -<span class="number">10.7196</span>, -<span class="number">18.6290</span>],</span><br><span class="line">         [ -<span class="number">2.7757</span>, -<span class="number">19.6408</span>,   <span class="number">0.0000</span>,  ...,  <span class="number">12.7660</span>,  <span class="number">21.6843</span>, -<span class="number">35.4784</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>子层连接结构总结</code>:</p><ul><li>什么是子层连接结构:<ul><li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构）, 在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li></ul></li></ul><hr><ul><li>学习并实现了子层连接结构的类: SublayerConnection<ul><li>类的初始化函数输入参数是size, dropout, 分别代表<code>词嵌入大小</code>和<code>置零比率</code>.</li><li>它的实例化对象输入参数是x, sublayer, 分别代表上一层输出以及子层的函数表示.</li><li>它的输出就是通过子层连接结构处理的输出.</li></ul></li></ul><h3 id="2-3-7-编码器层"><a href="#2-3-7-编码器层" class="headerlink" title="2.3.7 编码器层"></a>2.3.7 编码器层</h3><ul><li><p>学习目标:</p><ul><li><p>了解编码器层的作用.</p></li><li><p>掌握编码器层的实现过程.</p></li></ul></li></ul><hr><ul><li>编码器层的作用:<ul><li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li></ul></li></ul><hr><ul><li>编码器层的构成图:</li></ul><img src="https://article.biliimg.com/bfs/article/f46db8ec8cc3f5cc82fe1a014104a212155712160.png" style="zoom: 67%;" /><ul><li>编码器层的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 使用EncoderLayer类实现编码器层 构建编码器层的类</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="comment"># size：代表词嵌入的维度</span></span><br><span class="line">        <span class="comment"># self_at tn：代表传入的多头自注意力子层的实例化对象</span></span><br><span class="line">        <span class="comment"># feed_forward：代表前馈全连接层实例化对象</span></span><br><span class="line">        <span class="comment"># dropout：进行dropout操作时的置零比率</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码器层的大小,</span></span><br><span class="line"><span class="string">           第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制,</span></span><br><span class="line"><span class="string">           第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象, 最后一个是置0比率dropout.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先将self_attn和feed_forward传入其中.</span></span><br><span class="line">        <span class="comment"># 将两个实例化对象和参数传入类中</span></span><br><span class="line"></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆操作</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 把size传入其中</span></span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># X：代表上一层的传入张量</span></span><br><span class="line">        <span class="comment"># mask：代表掩码张量</span></span><br><span class="line">        <span class="comment"># 首先让经过第一个子层连接结构，内部包含多头自注意力机制子层</span></span><br><span class="line">        <span class="comment"># 再让张量经过第二个子层连接结构，其中包含前馈全连接网络</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask.</span></span><br><span class="line">        <span class="comment"># 里面就是按照结构图左侧的流程. 首先通过第一个子层连接结构，其中包含多头自注意力子层，</span></span><br><span class="line">        <span class="comment"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">x = pe_result</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">self_attn = MultiHeadedAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">el = EncoderLayer(size, self_attn, ff, dropout)</span><br><span class="line">el_result = el(x, mask)</span><br><span class="line"><span class="built_in">print</span>(el_result)</span><br><span class="line"><span class="built_in">print</span>(el_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">33.6988</span>, -<span class="number">30.7224</span>,  <span class="number">20.9575</span>,  ...,   <span class="number">5.2968</span>, -<span class="number">48.5658</span>,  <span class="number">20.0734</span>],</span><br><span class="line">         [-<span class="number">18.1999</span>,  <span class="number">34.2358</span>,  <span class="number">40.3094</span>,  ...,  <span class="number">10.1102</span>,  <span class="number">58.3381</span>,  <span class="number">58.4962</span>],</span><br><span class="line">         [ <span class="number">32.1243</span>,  <span class="number">16.7921</span>,  -<span class="number">6.8024</span>,  ...,  <span class="number">23.0022</span>, -<span class="number">18.1463</span>, -<span class="number">17.1263</span>],</span><br><span class="line">         [ -<span class="number">9.3475</span>,  -<span class="number">3.3605</span>, -<span class="number">55.3494</span>,  ...,  <span class="number">43.6333</span>,  -<span class="number">0.1900</span>,   <span class="number">0.1625</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">32.8937</span>, -<span class="number">46.2808</span>,   <span class="number">8.5047</span>,  ...,  <span class="number">29.1837</span>,  <span class="number">22.5962</span>, -<span class="number">14.4349</span>],</span><br><span class="line">         [ <span class="number">21.3379</span>,  <span class="number">20.0657</span>, -<span class="number">31.7256</span>,  ..., -<span class="number">13.4079</span>, -<span class="number">44.0706</span>,  -<span class="number">9.9504</span>],</span><br><span class="line">         [ <span class="number">19.7478</span>,  -<span class="number">1.0848</span>,  <span class="number">11.8884</span>,  ...,  -<span class="number">9.5794</span>,   <span class="number">0.0675</span>,  -<span class="number">4.7123</span>],</span><br><span class="line">         [ -<span class="number">6.8023</span>, -<span class="number">16.1176</span>,  <span class="number">20.9476</span>,  ...,  -<span class="number">6.5469</span>,  <span class="number">34.8391</span>, -<span class="number">14.9798</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>编码器层总结</code>：</p><ul><li>学习了编码器层的作用:<ul><li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li></ul></li></ul><hr><ul><li>学习并实现了编码器层的类: EncoderLayer<ul><li>类的初始化函数共有4个, 别是size，其实就是我们词嵌入维度的大小. 第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率dropout.<ul><li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li><li>它的输出代表经过整个编码层的特征表示.</li></ul></li></ul></li></ul><h3 id="2-3-8-编码器"><a href="#2-3-8-编码器" class="headerlink" title="2.3.8 编码器"></a>2.3.8 编码器</h3><ul><li>学习目标:<ul><li>了解编码器的作用.</li><li>掌握编码器的实现过程.</li></ul></li></ul><hr><ul><li>编码器的作用:<ul><li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li></ul></li></ul><hr><ul><li>编码器的结构图:</li></ul><p><img src="https://pic1.zhimg.com/80/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp" alt="img"></p><ul><li>编码器的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Encoder类来实现编码器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数的两个参数分别代表编码器层和编码器层的个数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 首先使用clones函数克隆N个编码器层放在self.layers中</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        <span class="comment"># 再初始化一个规范化层, 它将用在编码器的最后面.</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数的输入和编码器层相同, x代表上一层的输出, mask代表掩码张量&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span></span><br><span class="line">        <span class="comment"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span></span><br><span class="line">        <span class="comment"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数</span></span><br><span class="line"><span class="comment"># 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.</span></span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">c = copy.deepcopy</span><br><span class="line">attn = MultiHeadedAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">layer = EncoderLayer(size, c(attn), c(ff), dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码器中编码器层的个数N</span></span><br><span class="line">N = <span class="number">8</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">en = Encoder(layer, N)</span><br><span class="line">en_result = en(x, mask)</span><br><span class="line"><span class="built_in">print</span>(en_result)</span><br><span class="line"><span class="built_in">print</span>(en_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-<span class="number">0.2081</span>, -<span class="number">0.3586</span>, -<span class="number">0.2353</span>,  ...,  <span class="number">2.5646</span>, -<span class="number">0.2851</span>,  <span class="number">0.0238</span>],</span><br><span class="line">         [ <span class="number">0.7957</span>, -<span class="number">0.5481</span>,  <span class="number">1.2443</span>,  ...,  <span class="number">0.7927</span>,  <span class="number">0.6404</span>, -<span class="number">0.0484</span>],</span><br><span class="line">         [-<span class="number">0.1212</span>,  <span class="number">0.4320</span>, -<span class="number">0.5644</span>,  ...,  <span class="number">1.3287</span>, -<span class="number">0.0935</span>, -<span class="number">0.6861</span>],</span><br><span class="line">         [-<span class="number">0.3937</span>, -<span class="number">0.6150</span>,  <span class="number">2.2394</span>,  ..., -<span class="number">1.5354</span>,  <span class="number">0.7981</span>,  <span class="number">1.7907</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">2.3005</span>,  <span class="number">0.3757</span>,  <span class="number">1.0360</span>,  ...,  <span class="number">1.4019</span>,  <span class="number">0.6493</span>, -<span class="number">0.1467</span>],</span><br><span class="line">         [ <span class="number">0.5653</span>,  <span class="number">0.1569</span>,  <span class="number">0.4075</span>,  ..., -<span class="number">0.3205</span>,  <span class="number">1.4774</span>, -<span class="number">0.5856</span>],</span><br><span class="line">         [-<span class="number">1.0555</span>,  <span class="number">0.0061</span>, -<span class="number">1.8165</span>,  ..., -<span class="number">0.4339</span>, -<span class="number">1.8780</span>,  <span class="number">0.2467</span>],</span><br><span class="line">         [-<span class="number">2.1617</span>, -<span class="number">1.5532</span>, -<span class="number">1.4330</span>,  ..., -<span class="number">0.9433</span>, -<span class="number">0.5304</span>, -<span class="number">1.7022</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><strong>编码器总结：</strong></p><ul><li>学习了编码器的作用:<ul><li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li></ul></li></ul><hr><ul><li>学习并实现了编码器的类: Encoder<ul><li>类的初始化函数参数有两个，分别是layer和N，代表编码器层和编码器层的个数.</li><li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li><li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li></ul></li></ul><h2 id="2-4-解码器部分实现"><a href="#2-4-解码器部分实现" class="headerlink" title="2.4 解码器部分实现"></a>2.4 解码器部分实现</h2><ul><li><p>学习目标</p><ul><li><p>了解解码器中各个组成部分的作用.</p></li><li><p>掌握解码器中各个组成部分的实现过程.</p></li></ul></li></ul><hr><ul><li><p>解码器部分:</p><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和<code>规范化层以及一个残差连接</code></li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="https://pic1.zhimg.com/80/v2-4e17a521400c92b41a5927e5c40aeef0_1440w.webp" alt="img"></p><p>说明:</p><ul><li>解码器层中的各个部分，如，多头注意力机制，规范化层，前馈全连接网络，子层连接结构都与编码器中的实现相同. 因此这里可以直接拿来构建解码器层.</li></ul><hr><h3 id="2-4-1-解码器层"><a href="#2-4-1-解码器层" class="headerlink" title="2.4.1 解码器层"></a>2.4.1 解码器层</h3><ul><li>学习目标:<ul><li>了解解码器的作用.</li><li>掌握解码器的实现过程.</li></ul></li></ul><hr><ul><li>解码器的作用:<ul><li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li></ul></li></ul><hr><ul><li>解码器层的代码实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用DecoderLayer的类实现解码器层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，</span></span><br><span class="line"><span class="string">            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V， </span></span><br><span class="line"><span class="string">            第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 在初始化函数中， 主要就是将这些输入传到类中</span></span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        <span class="comment"># 按照结构图使用clones函数克隆三个子层连接对象.</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, source_mask, target_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数中的参数有4个，分别是来自上一层的输入x，</span></span><br><span class="line"><span class="string">           来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将memory表示成m方便之后使用</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，</span></span><br><span class="line">        <span class="comment"># 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span></span><br><span class="line">        <span class="comment"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span></span><br><span class="line">        <span class="comment"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，</span></span><br><span class="line">        <span class="comment"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, target_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span></span><br><span class="line">        <span class="comment"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span></span><br><span class="line">        <span class="comment"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, source_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈全连接层也和之前相同 </span></span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.</span></span><br><span class="line">x = pe_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># memory是来自编码器的输出</span></span><br><span class="line">memory = en_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">source_mask = target_mask = mask</span><br></pre></td></tr></table></figure><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)</span><br><span class="line">dl_result = dl(x, memory, source_mask, target_mask)</span><br><span class="line"><span class="built_in">print</span>(dl_result)</span><br><span class="line"><span class="built_in">print</span>(dl_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.9604e+00</span>,  <span class="number">3.9288e+01</span>, -<span class="number">5.2422e+01</span>,  ...,  <span class="number">2.1041e-01</span>,</span><br><span class="line">          -<span class="number">5.5063e+01</span>,  <span class="number">1.5233e-01</span>],</span><br><span class="line">         [ <span class="number">1.0135e-01</span>, -<span class="number">3.7779e-01</span>,  <span class="number">6.5491e+01</span>,  ...,  <span class="number">2.8062e+01</span>,</span><br><span class="line">          -<span class="number">3.7780e+01</span>, -<span class="number">3.9577e+01</span>],</span><br><span class="line">         [ <span class="number">1.9526e+01</span>, -<span class="number">2.5741e+01</span>,  <span class="number">2.6926e-01</span>,  ..., -<span class="number">1.5316e+01</span>,</span><br><span class="line">           <span class="number">1.4543e+00</span>,  <span class="number">2.7714e+00</span>],</span><br><span class="line">         [-<span class="number">2.1528e+01</span>,  <span class="number">2.0141e+01</span>,  <span class="number">2.1999e+01</span>,  ...,  <span class="number">2.2099e+00</span>,</span><br><span class="line">          -<span class="number">1.7267e+01</span>, -<span class="number">1.6687e+01</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6.7259e+00</span>, -<span class="number">2.6918e+01</span>,  <span class="number">1.1807e+01</span>,  ..., -<span class="number">3.6453e+01</span>,</span><br><span class="line">          -<span class="number">2.9231e+01</span>,  <span class="number">1.1288e+01</span>],</span><br><span class="line">         [ <span class="number">7.7484e+01</span>, -<span class="number">5.0572e-01</span>, -<span class="number">1.3096e+01</span>,  ...,  <span class="number">3.6302e-01</span>,</span><br><span class="line">           <span class="number">1.9907e+01</span>, -<span class="number">1.2160e+00</span>],</span><br><span class="line">         [ <span class="number">2.6703e+01</span>,  <span class="number">4.4737e+01</span>, -<span class="number">3.1590e+01</span>,  ...,  <span class="number">4.1540e-03</span>,</span><br><span class="line">           <span class="number">5.2587e+00</span>,  <span class="number">5.2382e+00</span>],</span><br><span class="line">         [ <span class="number">4.7435e+01</span>, -<span class="number">3.7599e-01</span>,  <span class="number">5.0898e+01</span>,  ...,  <span class="number">5.6361e+00</span>,</span><br><span class="line">           <span class="number">3.5891e+01</span>,  <span class="number">1.5697e+01</span>]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><code>解码器层总结</code>：</p><ul><li>学习了解码器层的作用:<ul><li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li></ul></li></ul><hr><ul><li>学习并实现了解码器层的类: DecoderLayer<ul><li>类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q&#x3D;K&#x3D;V，第三个是src_attn，多头注意力对象，这里Q!&#x3D;K&#x3D;V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</li><li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li><li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li></ul></li></ul><h3 id="2-4-2-解码器"><a href="#2-4-2-解码器" class="headerlink" title="2.4.2 解码器"></a>2.4.2 解码器</h3><ul><li>学习目标:<ul><li>了解解码器的作用.</li><li>掌握解码器的实现过程.</li></ul></li></ul><hr><ul><li>解码器的作用:<ul><li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li></ul></li></ul><hr><ul><li>解码器的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用类Decoder来实现解码器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span></span><br><span class="line">        <span class="comment"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, source_mask, target_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，</span></span><br><span class="line"><span class="string">           source_mask, target_mask代表源数据和目标数据的掩码张量&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span></span><br><span class="line">        <span class="comment"># 得出最后的结果，再进行一次规范化返回即可. </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, source_mask, target_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别是解码器层layer和解码器层的个数N</span></span><br><span class="line">size = <span class="number">512</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">c = copy.deepcopy</span><br><span class="line">attn = MultiHeadedAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)</span><br><span class="line">N = <span class="number">8</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入参数与解码器层的输入参数相同</span></span><br><span class="line">x = pe_result</span><br><span class="line">memory = en_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">source_mask = target_mask = mask</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">de = Decoder(layer, N)</span><br><span class="line">de_result = de(x, memory, source_mask, target_mask)</span><br><span class="line"><span class="built_in">print</span>(de_result)</span><br><span class="line"><span class="built_in">print</span>(de_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">0.9898</span>, -<span class="number">0.3216</span>, -<span class="number">1.2439</span>,  ...,  <span class="number">0.7427</span>, -<span class="number">0.0717</span>, -<span class="number">0.0814</span>],</span><br><span class="line">         [-<span class="number">0.7432</span>,  <span class="number">0.6985</span>,  <span class="number">1.5551</span>,  ...,  <span class="number">0.5232</span>, -<span class="number">0.5685</span>,  <span class="number">1.3387</span>],</span><br><span class="line">         [ <span class="number">0.2149</span>,  <span class="number">0.5274</span>, -<span class="number">1.6414</span>,  ...,  <span class="number">0.7476</span>,  <span class="number">0.5082</span>, -<span class="number">3.0132</span>],</span><br><span class="line">         [ <span class="number">0.4408</span>,  <span class="number">0.9416</span>,  <span class="number">0.4522</span>,  ..., -<span class="number">0.1506</span>,  <span class="number">1.5591</span>, -<span class="number">0.6453</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.9027</span>,  <span class="number">0.5874</span>,  <span class="number">0.6981</span>,  ...,  <span class="number">2.2899</span>,  <span class="number">0.2933</span>, -<span class="number">0.7508</span>],</span><br><span class="line">         [ <span class="number">1.2246</span>, -<span class="number">1.0856</span>, -<span class="number">0.2497</span>,  ..., -<span class="number">1.2377</span>,  <span class="number">0.0847</span>, -<span class="number">0.0221</span>],</span><br><span class="line">         [ <span class="number">3.4012</span>, -<span class="number">0.4181</span>, -<span class="number">2.0968</span>,  ..., -<span class="number">1.5427</span>,  <span class="number">0.1090</span>, -<span class="number">0.3882</span>],</span><br><span class="line">         [-<span class="number">0.1050</span>, -<span class="number">0.5140</span>, -<span class="number">0.6494</span>,  ..., -<span class="number">0.4358</span>, -<span class="number">1.2173</span>,  <span class="number">0.4161</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><p><strong>解码器总结：</strong></p><ul><li>学习了解码器的作用:<ul><li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li></ul></li></ul><hr><ul><li>学习并实现了解码器的类: Decoder<ul><li>类的初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.</li><li>forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，src_mask, tgt_mask代表源数据和目标数据的掩码张量.</li><li>输出解码过程的最终特征表示.</li></ul></li></ul><h2 id="2-5-输出部分实现"><a href="#2-5-输出部分实现" class="headerlink" title="2.5 输出部分实现"></a>2.5 输出部分实现</h2><ul><li><p>学习目标</p><ul><li><p>了解线性层和softmax的作用.</p></li><li><p>掌握线性层和softmax的实现过程.</p></li></ul></li></ul><hr><ul><li>输出部分包含:<ul><li>线性层</li><li>softmax层</li></ul></li></ul><p><img src="https://pic3.zhimg.com/80/v2-4b2d21a7199c6f261c9caca82e213b96_1440w.webp" alt="img"></p><ul><li>线性层的作用<ul><li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li></ul></li></ul><hr><ul><li>softmax层的作用</li><li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li></ul><hr><ul><li>线性层和softmax层的代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span></span><br><span class="line"><span class="comment"># 因此把类的名字叫做Generator, 生成器类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        <span class="comment"># 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, </span></span><br><span class="line">        <span class="comment"># 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size</span></span><br><span class="line">        self.project = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向逻辑函数中输入是上一层的输出张量x&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span></span><br><span class="line">        <span class="comment"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span></span><br><span class="line">        <span class="comment"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span></span><br><span class="line">        <span class="comment"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span></span><br><span class="line">        <span class="comment"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.project(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><hr><ul><li>nn.Linear演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词表大小是1000</span></span><br><span class="line">vocab_size = <span class="number">1000</span></span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span></span><br><span class="line">x = de_result</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gen = Generator(d_model, vocab_size)</span><br><span class="line">gen_result = gen(x)</span><br><span class="line"><span class="built_in">print</span>(gen_result)</span><br><span class="line"><span class="built_in">print</span>(gen_result.shape)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-<span class="number">7.8098</span>, -<span class="number">7.5260</span>, -<span class="number">6.9244</span>,  ..., -<span class="number">7.6340</span>, -<span class="number">6.9026</span>, -<span class="number">7.5232</span>],</span><br><span class="line">         [-<span class="number">6.9093</span>, -<span class="number">7.3295</span>, -<span class="number">7.2972</span>,  ..., -<span class="number">6.6221</span>, -<span class="number">7.2268</span>, -<span class="number">7.0772</span>],</span><br><span class="line">         [-<span class="number">7.0263</span>, -<span class="number">7.2229</span>, -<span class="number">7.8533</span>,  ..., -<span class="number">6.7307</span>, -<span class="number">6.9294</span>, -<span class="number">7.3042</span>],</span><br><span class="line">         [-<span class="number">6.5045</span>, -<span class="number">6.0504</span>, -<span class="number">6.6241</span>,  ..., -<span class="number">5.9063</span>, -<span class="number">6.5361</span>, -<span class="number">7.1484</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">7.1651</span>, -<span class="number">6.0224</span>, -<span class="number">7.4931</span>,  ..., -<span class="number">7.9565</span>, -<span class="number">8.0460</span>, -<span class="number">6.6490</span>],</span><br><span class="line">         [-<span class="number">6.3779</span>, -<span class="number">7.6133</span>, -<span class="number">8.3572</span>,  ..., -<span class="number">6.6565</span>, -<span class="number">7.1867</span>, -<span class="number">6.5112</span>],</span><br><span class="line">         [-<span class="number">6.4914</span>, -<span class="number">6.9289</span>, -<span class="number">6.2634</span>,  ..., -<span class="number">6.2471</span>, -<span class="number">7.5348</span>, -<span class="number">6.8541</span>],</span><br><span class="line">         [-<span class="number">6.8651</span>, -<span class="number">7.0460</span>, -<span class="number">7.6239</span>,  ..., -<span class="number">7.1411</span>, -<span class="number">6.5496</span>, -<span class="number">7.3749</span>]]],</span><br><span class="line">       grad_fn=&lt;LogSoftmaxBackward&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">1000</span>])</span><br></pre></td></tr></table></figure><ul><li><p>小节总结</p><ul><li>学习了输出部分包含:<ul><li>线性层</li><li>softmax层</li></ul></li></ul><hr><ul><li>线性层的作用:<ul><li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li></ul></li></ul><hr><ul><li><p>softmax层的作用:</p><ul><li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li></ul></li></ul><hr><ul><li><p>学习并实现了线性层和softmax层的类: Generator</p><ul><li>初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.</li><li>forward函数接受上一层的输出.</li><li>最终获得经过线性层和softmax层处理的结果.</li></ul></li></ul></li></ul><h2 id="2-6-模型的构建"><a href="#2-6-模型的构建" class="headerlink" title="2.6 模型的构建"></a>2.6 模型的构建</h2><ul><li>学习目标<ul><li>掌握编码器-解码器结构的实现过程.</li><li>掌握Transformer模型的构建过程.</li></ul></li></ul><hr><ul><li>通过上面的小节, 我们已经完成了所有组成部分的实现, 接下来就来实现完整的编码器-解码器结构.</li></ul><hr><ul><li>Transformer总体架构图:</li></ul><img src="https://article.biliimg.com/bfs/article/e29cac3e97a731b009805f5fc7842ffa155712160.png" style="zoom:80%;" /><h3 id="编码器-解码器结构的代码实现"><a href="#编码器-解码器结构的代码实现" class="headerlink" title="编码器-解码器结构的代码实现"></a>编码器-解码器结构的代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用EncoderDecoder类来实现编码器-解码器结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, source_embed, target_embed, generator</span>):</span><br><span class="line">        <span class="comment"># encoder：代表编码器对象</span></span><br><span class="line">        <span class="comment"># decoder：代表解码器对象</span></span><br><span class="line">        <span class="comment"># source_embed：代表源数据的嵌入函数</span></span><br><span class="line">        <span class="comment"># target_embed：代表目标数据的嵌入函数</span></span><br><span class="line">        <span class="comment"># generator：代表输出部分类别生成器对象</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;初始化函数中有5个参数, 分别是编码器对象, 解码器对象, </span></span><br><span class="line"><span class="string">           源数据嵌入函数, 目标数据嵌入函数,  以及输出部分的类别生成器对象</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        <span class="comment"># 将参数传入到类中</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = source_embed</span><br><span class="line">        self.tgt_embed = target_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, source, target, source_mask, target_mask</span>):</span><br><span class="line">        <span class="comment"># source：代表源数据</span></span><br><span class="line">        <span class="comment"># target：代表目标数据</span></span><br><span class="line">        <span class="comment"># source_mask：代表源数据的掩码张量</span></span><br><span class="line">        <span class="comment"># target_mask：代表目标数据的掩码张量</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;在forward函数中，有四个参数, source代表源数据, target代表目标数据, </span></span><br><span class="line"><span class="string">           source_mask和target_mask代表对应的掩码张量&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span></span><br><span class="line">        <span class="comment"># 与source_mask，target，和target_mask一同传给解码函数.</span></span><br><span class="line">        <span class="keyword">return</span> self.generator(self.decode(self.encode(source, source_mask), source_mask,</span><br><span class="line">                            target, target_mask))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, source, source_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;编码函数, 以source和source_mask为参数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 使用src_embed对source做处理, 然后和source_mask一起传给self.encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(source), source_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, source_mask, target, target_mask</span>):</span><br><span class="line">        <span class="comment"># memory：代表经历编码器编码后的输出张量</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 使用tgt_embed对target做处理, 然后和source_mask, target_mask, memory一起传给self.decoder</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)</span><br></pre></td></tr></table></figure><hr><ul><li>实例化参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">encoder = en</span><br><span class="line">decoder = de</span><br><span class="line">source_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">target_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">generator = gen</span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设源数据与目标数据相同, 实际中并不相同</span></span><br><span class="line">source = target = Variable(torch.LongTensor([[<span class="number">100</span>, <span class="number">2</span>, <span class="number">421</span>, <span class="number">508</span>], [<span class="number">491</span>, <span class="number">998</span>, <span class="number">1</span>, <span class="number">221</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设src_mask与tgt_mask相同，实际中并不相同</span></span><br><span class="line">source_mask = target_mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)</span><br><span class="line">ed_result = ed(source, target, source_mask, target_mask)</span><br><span class="line"><span class="built_in">print</span>(ed_result)</span><br><span class="line"><span class="built_in">print</span>(ed_result.shape)</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">0.2102</span>, -<span class="number">0.0826</span>, -<span class="number">0.0550</span>,  ...,  <span class="number">1.5555</span>,  <span class="number">1.3025</span>, -<span class="number">0.6296</span>],</span><br><span class="line">         [ <span class="number">0.8270</span>, -<span class="number">0.5372</span>, -<span class="number">0.9559</span>,  ...,  <span class="number">0.3665</span>,  <span class="number">0.4338</span>, -<span class="number">0.7505</span>],</span><br><span class="line">         [ <span class="number">0.4956</span>, -<span class="number">0.5133</span>, -<span class="number">0.9323</span>,  ...,  <span class="number">1.0773</span>,  <span class="number">1.1913</span>, -<span class="number">0.6240</span>],</span><br><span class="line">         [ <span class="number">0.5770</span>, -<span class="number">0.6258</span>, -<span class="number">0.4833</span>,  ...,  <span class="number">0.1171</span>,  <span class="number">1.0069</span>, -<span class="number">1.9030</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.4355</span>, -<span class="number">1.7115</span>, -<span class="number">1.5685</span>,  ..., -<span class="number">0.6941</span>, -<span class="number">0.1878</span>, -<span class="number">0.1137</span>],</span><br><span class="line">         [-<span class="number">0.8867</span>, -<span class="number">1.2207</span>, -<span class="number">1.4151</span>,  ..., -<span class="number">0.9618</span>,  <span class="number">0.1722</span>, -<span class="number">0.9562</span>],</span><br><span class="line">         [-<span class="number">0.0946</span>, -<span class="number">0.9012</span>, -<span class="number">1.6388</span>,  ..., -<span class="number">0.2604</span>, -<span class="number">0.3357</span>, -<span class="number">0.6436</span>],</span><br><span class="line">         [-<span class="number">1.1204</span>, -<span class="number">1.4481</span>, -<span class="number">1.5888</span>,  ..., -<span class="number">0.8816</span>, -<span class="number">0.6497</span>,  <span class="number">0.0606</span>]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure><hr><ul><li>接着将基于以上结构构建用于训练的模型.</li></ul><hr><ul><li>Tansformer模型构建过程的代码分析</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">source_vocab, target_vocab, N=<span class="number">6</span>, </span></span><br><span class="line"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, head=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="comment">#source_voc ab：代表源数据的词汇总数</span></span><br><span class="line">    <span class="comment">#target_voc ab：代表目标数据的词汇总数</span></span><br><span class="line">    <span class="comment">#N：代表编码器和解码器堆叠的层数</span></span><br><span class="line">    <span class="comment">#d_model：代表词嵌入的维度</span></span><br><span class="line">    <span class="comment">#d_ff：代表前馈全连接层中变换矩阵的维度</span></span><br><span class="line">    <span class="comment">#head：多头注意力机制中的头数</span></span><br><span class="line">    <span class="comment">#dropout：指置零的比率</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;该函数用来构建模型, 有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，</span></span><br><span class="line"><span class="string">       编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，</span></span><br><span class="line"><span class="string">       多头注意力结构中的多头数，以及置零比率dropout.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span></span><br><span class="line">    <span class="comment"># 来保证他们彼此之间相互独立，不受干扰.</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化了多头注意力类，得到对象attn</span></span><br><span class="line">    attn = MultiHeadedAttention(head, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后实例化前馈全连接类，得到对象ff </span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化位置编码类，得到对象position</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span></span><br><span class="line">    <span class="comment"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span></span><br><span class="line">    <span class="comment"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span></span><br><span class="line">    <span class="comment"># 在编码器层中有attention子层以及前馈全连接子层，</span></span><br><span class="line">    <span class="comment"># 在解码器层中有两个attention子层以及前馈全连接层.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#实例化模型model，利用的是Encoder Decoder类</span></span><br><span class="line"><span class="comment">#编码器的结构里面有2个子层，attention层和前馈全连接层</span></span><br><span class="line"><span class="comment">#解码器的结构中有3个子层，两个attention层和前馈全连接层</span></span><br><span class="line"></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),</span><br><span class="line">        Generator(d_model, target_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span></span><br><span class="line">    <span class="comment"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span></span><br><span class="line">    <span class="comment">#初始化整个模型中的参数，判断参数的维度大于1，将矩阵初始化成一个服从均匀分布的矩阵</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li>nn.init.xavier_uniform演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果服从均匀分布U(-a, a)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = torch.empty(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w</span><br><span class="line">tensor([[-<span class="number">0.7742</span>,  <span class="number">0.5413</span>,  <span class="number">0.5478</span>, -<span class="number">0.4806</span>, -<span class="number">0.2555</span>],</span><br><span class="line">        [-<span class="number">0.8358</span>,  <span class="number">0.4673</span>,  <span class="number">0.3012</span>,  <span class="number">0.3882</span>, -<span class="number">0.6375</span>],</span><br><span class="line">        [ <span class="number">0.4622</span>, -<span class="number">0.0794</span>,  <span class="number">0.1851</span>,  <span class="number">0.8462</span>, -<span class="number">0.3591</span>]])</span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">source_vocab = <span class="number">11</span></span><br><span class="line">target_vocab = <span class="number">11</span> </span><br><span class="line">N = <span class="number">6</span></span><br><span class="line"><span class="comment"># 其他参数都使用默认值 </span></span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    res = make_model(source_vocab, target_vocab, N)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据Transformer结构图构建的最终模型结构</span></span><br><span class="line">EncoderDecoder(</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w_1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">          (w_2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">1</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">1</span>): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w_1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">          (w_2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">1</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm(</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (decoder): Decoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w_1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">          (w_2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">1</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">2</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">1</span>): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadedAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">1</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">            (<span class="number">3</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w_1): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">2048</span>)</span><br><span class="line">          (w_2): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">512</span>)</span><br><span class="line">          (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (<span class="number">0</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">1</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">          (<span class="number">2</span>): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm(</span><br><span class="line">            )</span><br><span class="line">            (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm(</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (src_embed): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Embeddings(</span><br><span class="line">      (lut): Embedding(<span class="number">11</span>, <span class="number">512</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (tgt_embed): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Embeddings(</span><br><span class="line">      (lut): Embedding(<span class="number">11</span>, <span class="number">512</span>)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (generator): Generator(</span><br><span class="line">    (proj): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">11</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>小节总结</strong></p><ul><li>学习并实现了编码器-解码器结构的类: EncoderDecoder<ul><li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li><li>类中共实现三个函数, forward, encode, decode</li><li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li><li>encode是编码函数, 以source和source_mask为参数.</li><li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li></ul></li></ul><hr><ul><li>学习并实现了模型构建函数: make_model<ul><li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li></ul></li></ul><h2 id="2-7-模型基本测试运行-P44"><a href="#2-7-模型基本测试运行-P44" class="headerlink" title="2.7 模型基本测试运行 P44"></a>2.7 模型基本测试运行 P44</h2><ul><li><p>学习目标</p><ul><li><p>了解Transformer模型基本测试的copy任务.</p></li><li><p>掌握实现copy任务的四步曲.</p></li></ul></li></ul><hr><ul><li><p>copy任务介绍:</p><ul><li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li><li>任务意义: copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li></ul></li></ul><hr><ul><li>使用copy任务进行模型基本测试的四步曲<ul><li>第一步: 构建数据集生成器</li><li>第二步: 获得Transformer模型及其优化器和损失函数</li><li>第三步: 运行模型进行训练和评估</li><li>第四步: 使用模型进行贪婪解码</li></ul></li></ul><hr><ul><li>第一步: 构建数据集生成器</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入工具包Batch, 它能够对原始样本数据生成对应批次的掩码张量</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> Batch  </span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> get_std_opt</span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> LabelSmoothing</span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> SimpleLossCompute</span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> run_epoch</span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> greedy_decode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_generator</span>(<span class="params">V, batch, num_batch</span>):</span><br><span class="line">    <span class="comment"># V：随机生成数据的最大值+1</span></span><br><span class="line"><span class="comment"># batch_size：每次输送给模型的样本数量，经历这些样本训练后进行一次参数的更新</span></span><br><span class="line"><span class="comment"># num_batch：一共输送模型多少轮数据</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;该函数用于随机生成copy任务的数据, 它的三个输入参数是V: 随机生成数字的最大值+1, </span></span><br><span class="line"><span class="string">       batch: 每次输送给模型更新一次参数的数据量, num_batch: 一共输送num_batch次完成一轮</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用for循环遍历nbatches</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batch):</span><br><span class="line">        <span class="comment"># 使用num py中的random.rand in t（）来随机生成[1，V)</span></span><br><span class="line"><span class="comment"># 分布的形状(batch，10）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在循环中使用np的random.randint方法随机生成[1, V)的整数, </span></span><br><span class="line">        <span class="comment"># 分布在(batch, 10)形状的矩阵中, 然后再把numpy形式转换称torch中的tensor.</span></span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列, </span></span><br><span class="line">        <span class="comment"># 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度</span></span><br><span class="line">        <span class="comment"># 因此requires_grad设置为False</span></span><br><span class="line">        source = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        target = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回</span></span><br><span class="line">        <span class="keyword">yield</span> Batch(source, target)</span><br></pre></td></tr></table></figure><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将生成0-10的整数</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次喂给模型20个数据进行参数更新</span></span><br><span class="line">batch = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 连续喂30次完成全部数据的遍历, 也就是1轮</span></span><br><span class="line">num_batch = <span class="number">30</span></span><br></pre></td></tr></table></figure><hr><ul><li>调用:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    res = data_generator(V, batch, num_batch)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 会得到一个数据生成器(生成器对象)</span></span><br><span class="line">&lt;generator <span class="built_in">object</span> data_gen at <span class="number">0x10c053e08</span>&gt;</span><br></pre></td></tr></table></figure><hr><ul><li>第二步: 获得Transformer模型及其优化器和损失函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入优化器工具包get_std_opt, 该工具用于获得标准的针对Transformer模型的优化器 </span></span><br><span class="line"><span class="comment"># 该标准优化器基于Adam优化器, 使其对序列到序列的任务更有效.</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> get_std_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入标签平滑工具包, 该工具用于标签平滑, 标签平滑的作用就是小幅度的改变原有标签值的值域</span></span><br><span class="line"><span class="comment"># 因为在理论上即使是人工的标注数据也可能并非完全正确, 会受到一些外界因素的影响而产生一些微小的偏差</span></span><br><span class="line"><span class="comment"># 因此使用标签平滑来弥补这种偏差, 减少模型对某一条规律的绝对认知, 以防止过拟合. 通过下面示例了解更多.</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> LabelSmoothing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入损失计算工具包, 该工具能够使用标签平滑后的结果进行损失的计算, </span></span><br><span class="line"><span class="comment"># 损失的计算方法可以认为是交叉熵损失函数.</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> SimpleLossCompute</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用make_model获得model</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用get_std_opt获得模型优化器</span></span><br><span class="line">model_optimizer = get_std_opt(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion, model_optimizer)</span><br></pre></td></tr></table></figure><hr><ul><li>标签平滑示例:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> LabelSmoothing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用LabelSmoothing实例化一个crit对象.</span></span><br><span class="line"><span class="comment"># 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小</span></span><br><span class="line"><span class="comment"># 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字</span></span><br><span class="line"><span class="comment"># 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度</span></span><br><span class="line"><span class="comment"># 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].</span></span><br><span class="line">crit = LabelSmoothing(size=<span class="number">5</span>, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定一个任意的模型最后输出预测结果和真实结果</span></span><br><span class="line">predict = Variable(torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标签的表示值是0，1，2</span></span><br><span class="line">target = Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将predict, target传入到对象中</span></span><br><span class="line">crit(predict, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制标签平滑图像</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure><hr><ul><li>标签平滑图像:</li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202311052258109.png" alt="image-20231105223915947"></p><ul><li><p>标签平滑图像分析:</p><ul><li>我们目光集中在<code>黄色小方块</code>上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从0.5到2.5.</li><li>它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从-0.5到1.5, 总的值域空间由原来的[0, 2]变成了[-0.5, 2.5].</li></ul></li></ul><hr><ul><li>第三步: 运行模型进行训练和评估</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模型单轮训练工具包run_epoch, 该工具将对模型使用给定的损失函数计算方法进行单轮参数更新.</span></span><br><span class="line"><span class="comment"># 并打印每轮参数更新的损失结果.</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> run_epoch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">model, loss, epochs=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;模型训练函数, 共有三个参数, model代表将要进行训练的模型</span></span><br><span class="line"><span class="string">       loss代表使用的损失计算方法, epochs代表模型训练的轮数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历轮数</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 模型使用训练模式, 所有参数将被更新</span></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># 训练时, batch_size是20</span></span><br><span class="line">        run_epoch(data_generator(V, <span class="number">8</span>, <span class="number">20</span>), model, loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型使用评估模式, 参数将不会变化 </span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># 评估时, batch_size是5</span></span><br><span class="line">        run_epoch(data_generator(V, <span class="number">8</span>, <span class="number">5</span>), model, loss)</span><br></pre></td></tr></table></figure><hr><ul><li>输入参数:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行10轮训练</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model和loss都是来自上一步的结果</span></span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.315704</span> Tokens per Sec: <span class="number">309.740843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">2.602743</span> Tokens per Sec: <span class="number">393.885743</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">2.563469</span> Tokens per Sec: <span class="number">347.746994</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">2.065951</span> Tokens per Sec: <span class="number">422.632783</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">2.218468</span> Tokens per Sec: <span class="number">346.982987</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.771149</span> Tokens per Sec: <span class="number">396.451901</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.979203</span> Tokens per Sec: <span class="number">350.384045</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.648887</span> Tokens per Sec: <span class="number">361.534817</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.824539</span> Tokens per Sec: <span class="number">349.660287</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.550169</span> Tokens per Sec: <span class="number">319.302558</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.676636</span> Tokens per Sec: <span class="number">369.678638</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.394759</span> Tokens per Sec: <span class="number">364.660371</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.473153</span> Tokens per Sec: <span class="number">324.016068</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.142609</span> Tokens per Sec: <span class="number">422.345444</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.410883</span> Tokens per Sec: <span class="number">365.395922</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.828656</span> Tokens per Sec: <span class="number">401.538655</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.254409</span> Tokens per Sec: <span class="number">346.133228</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.745532</span> Tokens per Sec: <span class="number">402.395937</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.952969</span> Tokens per Sec: <span class="number">324.858870</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.373509</span> Tokens per Sec: <span class="number">358.814760</span></span><br></pre></td></tr></table></figure><hr><ul><li>第四步: 使用模型进行贪婪解码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入贪婪解码工具包greedy_decode, 该工具将对最终结进行贪婪解码</span></span><br><span class="line"><span class="comment"># 贪婪解码的方式是每次预测都选择概率最大的结果作为输出, </span></span><br><span class="line"><span class="comment"># 它不一定能获得全局最优性, 但却拥有最高的执行效率.</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer_utils <span class="keyword">import</span> greedy_decode </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">model, loss, epochs=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 首先进入训练模式，所有的参数将会被更新</span></span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">        run_epoch(data_generator(V, <span class="number">8</span>, <span class="number">20</span>), model, loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练结束后， 进入评估模式，所有的参数固定不变</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        run_epoch(data_generator(V, <span class="number">8</span>, <span class="number">5</span>), model, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 跳出for循环后，掉膘模型训练结束，进入评估模式</span></span><br><span class="line">    <span class="comment"># 模型进入测试模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 假定的输入张量</span></span><br><span class="line">    <span class="comment"># 初始化一个输入张量</span></span><br><span class="line">    source = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义源数据掩码张量, 因为元素都是1, 在我们这里1代表不遮掩</span></span><br><span class="line">    <span class="comment"># 因此相当于对源数据没有任何遮掩.</span></span><br><span class="line">    <span class="comment"># 初始化一个输入张量的掩码张量，全1代表没有任何的遮掩</span></span><br><span class="line">    source_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后将model, src, src_mask, 解码的最大长度限制max_len, 默认为10</span></span><br><span class="line">    <span class="comment"># 以及起始标志数字, 默认为1, 我们这里使用的也是1</span></span><br><span class="line">    <span class="comment">#设定解码的最大长度max_len等于10，起始数字的标志默认等于1</span></span><br><span class="line"></span><br><span class="line">    result = greedy_decode(model, source, source_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run(model, loss) </span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    <span class="number">1</span>     <span class="number">3</span>     <span class="number">2</span>     <span class="number">5</span>     <span class="number">4</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size 1x10]</span><br></pre></td></tr></table></figure><hr><p><strong>小节总结</strong></p><ul><li><p>学习了copy任务的相关知识:</p><ul><li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li><li>任务意义: <code>copy任务</code>在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li></ul></li></ul><hr><ul><li><p>学习了使用copy任务进行模型基本测试的四步曲:</p><ul><li>第一步: 构建数据集生成器</li><li>第二步: 获得Transformer模型及其优化器和损失函数</li><li>第三步: 运行模型进行训练和评估</li><li>第四步: 使用模型进行贪婪解码</li></ul></li></ul><hr><ul><li><p>学习并实现了构建数据集生成器函数: data_gen</p><ul><li>它有三个输入参数, 分别是V: 随机生成数字的最大值+1, batch: 每次输送给模型更新一次参数的数据量, nbatches: 一共输送nbatches次完成一轮.</li><li>该函数最终得到一个生成器对象.</li></ul></li></ul><hr><ul><li><p>学习了获得Transformer模型及其优化器和损失函数:</p><ul><li>通过导入优化器工具包get_std_opt, 获得标准优化器.</li><li>通过导入标签平滑工具包LabelSmoothing, 进行标签平滑.</li><li>通过导入损失计算工具包SimpleLossCompute, 计算损失.</li></ul></li></ul><hr><ul><li><p>学习并实现了运行模型进行训练和评估函数: run</p><ul><li>在函数中导入模型单轮训练工具包run_epoch, 对模型进行单轮训练.</li><li>函数共有三个参数, model代表将要进行训练的模型, slc代表使用的损失计算方法, epochs代表模型训练的轮数.</li><li>函数最终打印了模型训练和评估两个过程的损失.</li></ul></li></ul><hr><ul><li><p>学习并实现了使用模型进行贪婪解码:</p><ul><li>通过导入贪婪解码工具包greedy_decode, 根据输入得到最后输出, 完成了copy任务.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记四：ResNet（残差网络）</title>
      <link href="/2024/04/7bc6.html"/>
      <url>/2024/04/7bc6.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer学习笔记四：ResNet（残差网络）"><a href="#Transformer学习笔记四：ResNet（残差网络）" class="headerlink" title="Transformer学习笔记四：ResNet（残差网络）"></a>Transformer学习笔记四：ResNet（残差网络）</h1><p>关于Transformer的笔记，预计出如下几篇：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/454482273">Positional Encoding （位置编码），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/455399791">Self-attention（自注意力机制），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/456863215">Batch Norm &amp; Layer Norm（批量标准化&#x2F;层标准化），点击跳转</a></li><li>ResNet（残差网络）</li><li><a href="https://zhuanlan.zhihu.com/p/460678461">Subword Tokenization（子词分词法），点击跳转</a></li><li>组装：Transformer</li></ol><p>封面介绍一下Brainstorm小诸葛：<br>【派别】：汽车人<br>【职责】：生化机械工程师<br>【优点也是缺点】：想法太多太快，导致脑电路过热或者短路。<br>【经历】在赛博坦星球时，作为一个理性的理想主义者而抑郁不得志。最终在星云找到了自己的世外桃源，潜心做开发研究工作，主要成就是人机互换领域的突破。<br>“再强大的暴君也无法抑制思想的自由。”</p><p>在Transformer中，数据过Attention层和FFN层后，都会经过一个<strong>Add &amp; Norm</strong>处理。其中，Add为<strong>residule block（残差模块）</strong>，数据在这里进行<strong>residule connection（残差连接）</strong>。残差连接的思想最经典的代表就是2015年被提出的<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>，这个用于解决深层网络训练问题的模型最早被用于图像任务处理上，现在已经成为一种普适性的深度学习方法。这篇笔记将对此进行解析，笔记内容包括：</p><p>一、背景</p><ul><li>1.1 梯度消失&#x2F;爆炸</li><li>1.2 网络退化(Degradation)</li></ul><p>二、思路</p><ul><li>2.1 为什么需要更深的网络</li><li>2.2 理想中的深网络表现</li></ul><p>三、实践和实验效果</p><ul><li>3.1 构造恒等映射：残差学习（residule learning）</li><li>3.2 实验过程及结果</li></ul><p>四、Transformer中的残差连接</p><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在进行<strong>深层</strong>网络学习的过程中，有两个避不开的问题：</p><h3 id="1-1-梯度消失-爆炸"><a href="#1-1-梯度消失-爆炸" class="headerlink" title="1.1 梯度消失&#x2F;爆炸"></a>1.1 梯度消失&#x2F;爆炸</h3><p><img src="https://s2.loli.net/2024/04/26/4v51hD32sxkmjZH.webp" alt="img"></p><p>数据在神经网络中的传播过程</p><p>如图所示的三层神经网络，每一层的线性层和非线性层可以表示为：</p><p>$$Z^{[L]}&#x3D;W^{[L]}*A^{[L-1]}+b^{[L]}$$ （线性变化层）</p><p>$$A^{[L]}&#x3D;g^{[L]}(Z^{[L]})$$ （非线性变化&#x2F;激活函数层）</p><p>假设现在要计算第一层 $W^{[1]}$的梯度，那么根据链式法则，有：</p><p>$$\begin{gathered}<br>\frac{\partial LOSS}{\partial W^{[1]}} &#x3D;\frac{\partial LOSS}{\partial A^{[3]}}\frac{\partial A^{[3]}}{\partial Z^{[3]}}\frac{\partial Z^{[3]}}{\partial A^{[2]}}\frac{\partial A^{[2]}}{\partial Z^{[2]}}\frac{\partial Z^{[2]}}{\partial A^{[1]}}\frac{\partial A^{[1]}}{\partial Z^{[1]}}\frac{\partial Z^{[1]}}{\partial W^{[1]}} \<br>&#x3D;\frac{\partial LOSS}{\partial A^{[3]}}g^{[3]}{}^{\prime}W^{[3]}g^{[2]}{}^{\prime}W^{[2]}g^{[1]}{}^{\prime}\frac{\partial Z^{[1]}}{\partial W^{[1]}}<br>\end{gathered}$$</p><p>如果在神经网络中，多层都满足$g^{[L]^{\prime}}W^{[L]}$&gt;1 ，则越往下的网络层的梯度越大，这就造成了<strong>梯度爆炸</strong>的问题。反之，若多层都满足 $g^{[L]^{\prime}}W^{[L]}$&lt;1 ，则越往下的网络层梯度越小，引起<strong>梯度消失</strong>的问题。而在深度学习网络中，为了让模型学到更多非线性的特征，在激活层往往使用例如<strong>sigmoid</strong>这样的激活函数。对sigmoid来说，<strong>其导数的取值范围在</strong> (0, 1&#x2F;4] ，在层数堆叠的情况下，更容易出现梯度消失的问题。</p><p>面对梯度消失&#x2F;爆炸的情况，可以通过Normalization等方式解决，使得模型最终能够收敛。</p><h3 id="1-2-网络退化-Degradation"><a href="#1-2-网络退化-Degradation" class="headerlink" title="1.2 网络退化(Degradation)"></a>1.2 网络退化(Degradation)</h3><p>因为梯度消失&#x2F;爆炸所导致的深层网络模型不收敛的问题，已经得到了解决。那么现在新的问题出现了：<strong>在模型能够收敛的情况下，网络越深，模型的准确率越低，同时，模型的准确率先达到饱和，此后迅速下降</strong>。这个情况我们称之为<strong>网络退化（Degradation）。</strong>如下图，56层网络在测试集（右）上的错误率比20层网络要更高，这个现象也不是因为overfitting所引起的，因为在训练集上，深层网络的表现依然更差。</p><p><img src="https://s2.loli.net/2024/04/26/Dnp2fTdXargeYs5.webp" alt="因为网络过深而产生的退化(degradation）问题"></p><p>因为网络过深而产生的退化(degradation）问题</p><p>因此，<code>ResNet就作为一种解决网络退化问题的有效办法出现了</code>，借助ResNet，我们能够有效训练出更深的网络模型（可以超过1000层），使得深网络的表现不差于浅网络。</p><h2 id="二、思路"><a href="#二、思路" class="headerlink" title="二、思路"></a>二、思路</h2><h3 id="2-1-为什么需要更深的网络"><a href="#2-1-为什么需要更深的网络" class="headerlink" title="2.1 为什么需要更深的网络"></a>2.1 为什么需要更深的网络</h3><p>神经网络帮我们避免了繁重的特征工程过程。借助神经网络中的非线形操作，可以帮助我们更好地拟合模型的特征。为了增加模型的表达能力，一种直觉的想法是，增加网络的深度，一来使得网络的每一层都尽量学到不同的模式，二来更好地利用网络的非线性拟合能力。</p><h3 id="2-2-理想中的深网络表现"><a href="#2-2-理想中的深网络表现" class="headerlink" title="2.2 理想中的深网络表现"></a>2.2 理想中的深网络表现</h3><p>理想中的深网络，其表现不应该差于浅网络。举一个简单的例子，下图左边是2层的浅网络，右边是4层的深网络，我们只要令深网络的最后两层的输入输出相等，那么两个网络就是等效的，这种操作被称为<strong>恒等映射（Identity Mapping)。</strong></p><p><img src="https://s2.loli.net/2024/04/26/vpqYUOxghrBJudX.webp" alt="恒等映射"></p><p>当然，这样完全相等的映射是一种极端情况，更为理想的情况是，在网络的深层，让网络尽量逼近这样的极端情况，使得网络在学到新东西的同时，其输出又能逼近输入，这样就能保证深网络的效果不会比浅网络更差。</p><p><code>总结：在网络的深层，需要学习一种恒等映射（Identity Mapping）。</code> </p><h2 id="三、实践和实验效果"><a href="#三、实践和实验效果" class="headerlink" title="三、实践和实验效果"></a>三、实践和实验效果</h2><h3 id="3-1-构造恒等映射：残差学习（residule-learning）"><a href="#3-1-构造恒等映射：残差学习（residule-learning）" class="headerlink" title="3.1 构造恒等映射：残差学习（residule learning）"></a>3.1 构造恒等映射：残差学习（residule learning）</h3><p>最暴力的构造恒等映射的方法，就是在相应网络部分的尾端增加一层学习层 $𝑊^{[𝐼𝑀]}$ ，来满足输出和输入逼近。但是本来深网络要学的参数就很庞大了，再构造新的参数层，又增加了模型的复杂度。</p><p><img src="https://s2.loli.net/2024/04/26/USuzaCy5NBT8kHm.webp" alt="额外添加参数来构造恒等映射，缺点是增加了模型的复杂度"></p><p>额外添加参数来构造恒等映射，缺点是增加了模型的复杂度</p><p>能不能在不添加参数层的情况下，实现恒等映射的功能？考虑下图：</p><p><img src="https://s2.loli.net/2024/04/26/9u1OoGAMTKn7Ras.webp" alt="img"></p><p>蓝色星星f^{*}$是“真正”拟合我们数据集的函数，而 $$F_1$,…,$F_6$ 表示分别表示不同层数的神经网络（层数为1的，层数为2的…）。在左边的构造方式中，函数是非嵌套的，可以发现6层神经网络可能比单层神经网络距离最优解更远。而在右边的嵌套式构造中，则保证了更多层的神经网络至少能取到更浅的神经网络的最优解。受到这一思想的启发，我们在深层网络中引入残差模块，具体运作方式如下：</p><p><img src="https://s2.loli.net/2024/04/26/wVy57gkD8MIzlO1.webp" alt="ResNet核心构造"></p><p>如图所示，这个残差模块包含了神经网络中的两层，其中， 𝑋 表示输入， 𝐹(𝑋) 表示过这两层之后的结果， 𝐻(𝑋) 表示恒等映射，则在这样的构造方式下，恒等映射可以写成：</p><p>$$H(X)&#x3D;F(X)+X(1)$$</p><p> F(X) 就被称之为<strong>残差函数（residule function）。</strong>在网络深层的时候，在优化目标的约束下，模型通过学习使得 𝐹(𝑋) 逼近0**（residule learning)**，让深层函数在学到东西的情况下，又不会发生网络退化的问题。</p><p>通过这样的构造方式，让 𝐹(𝑋) 嵌套在了 𝐻(𝑋) 中，这样跃层构造的方式也被称为**残差连接(residule connection)&#x2F; 跳跃连接(skip connection)&#x2F;短路(shortcuts)**。模型并不是严格的跨越2层，可以根据需要跨越3，4层进行连接。同时，等式（1）是在假设输入输出同维，即 𝐹(𝑋) 和 𝑋 同维的情况，不同维时，只需要在 𝑋 前面增加一个转换矩阵$ 𝑊_𝑠 $即可。</p><h3 id="3-2-实验过程及结果"><a href="#3-2-实验过程及结果" class="headerlink" title="3.2 实验过程及结果"></a>3.2 实验过程及结果</h3><p>在ResNet的论文中，做了很丰富的实验，这里仅贴出它在ImageNet 2012数据集上的试验结果。<br>这个实验的网络以VGG网络为参考，构造了<strong>34-layer plain DNN</strong>和<strong>34-layer residule DNN。</strong>前者是一个标准的深层网络，后者是增加残差处理的深层网络。基本构造如下：（图很小吧，都怪网络太深，没关系，不看也可以）</p><p><img src="https://s2.loli.net/2024/04/26/BygSZQhRiJEKVHO.webp" alt="ResNet原始论文中的实验网络架构"></p><p>以下是实验结果。左图是18层和34层的plain DNN，右是18层和34层的residule DNN。粗线表示训练集上的错误率，细线表示验证集上的错误率。可以发现在残差网络中，深网络的错误率都已经被压到了浅网络之下，同时也比plain DNN的错误率更低。</p><p><img src="https://s2.loli.net/2024/04/26/ca2jhGWFtZkmL1n.webp" alt="img"></p><p><img src="https://s2.loli.net/2024/04/26/9EaeXRnqoytjbCu.webp" alt="img"></p><h2 id="四、Transformer中的残差连接"><a href="#四、Transformer中的残差连接" class="headerlink" title="四、Transformer中的残差连接"></a>四、Transformer中的残差连接</h2><p>在transformer的encoder和decoder中，各用到了6层的attention模块，每一个attention模块又和一个FeedForward层（简称FFN）相接。对每一层的attention和FFN，都采用了一次残差连接，即把每一个位置的输入数据和输出数据相加，使得Transformer能够有效训练更深的网络。在残差连接过后，再采取Layer Nomalization的方式。具体的操作过程见下图，箭头表示画不下了，从左边转到右边去画：</p><p><img src="https://s2.loli.net/2024/04/26/qntfecaTPjwMSD5.webp" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记三：为什么Transformer要用LayerNorm/Batch Normalization &amp; Layer Normalization （批量&amp;层标准化)</title>
      <link href="/2024/04/Tranformer3.html"/>
      <url>/2024/04/Tranformer3.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer学习笔记三：为什么Transformer要用LayerNorm-Batch-Normalization-Layer-Normalization-（批量-层标准化"><a href="#Transformer学习笔记三：为什么Transformer要用LayerNorm-Batch-Normalization-Layer-Normalization-（批量-层标准化" class="headerlink" title="Transformer学习笔记三：为什么Transformer要用LayerNorm&#x2F;Batch Normalization &amp; Layer Normalization （批量&amp;层标准化)"></a>Transformer学习笔记三：为什么Transformer要用LayerNorm&#x2F;Batch Normalization &amp; Layer Normalization （批量&amp;层标准化)</h1><p><strong>20230225更新：最新在更新ChatGPT系列，感兴趣的朋友可以移步：<a href="https://zhuanlan.zhihu.com/p/605516116">猛猿：ChatGPT技术解析系列之：训练框架InstructGPT</a></strong></p><p>关于Transformer的笔记，预计出如下几篇：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/454482273">Positional Encoding （位置编码），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/455399791">Self-attention（自注意力机制），点击跳转</a></li><li>Batch Norm &amp; Layer Norm（批量标准化&#x2F;层标准化）</li><li><a href="https://zhuanlan.zhihu.com/p/459065530">ResNet（残差网络），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/460678461">Subword Tokenization（子词分词法），点击跳转</a></li><li>组装：Transformer</li></ol><p>这一篇写Transformer里标准化的方法。在Transformer中，数据过Attention层和FFN层后，都会经过一个Add &amp; Norm处理。其中，Add为residule block（残差模块），数据在这里进行residule connection（残差连接）。而Norm即为Normalization（标准化）模块。<strong>Transformer中采用的是Layer Normalization（层标准化）方式</strong>。常用的标准化方法有Batch Normalization，Layer Normalization，Group Normalization，Instance Normalization等，这篇笔记将在论文研究的基础上，着重聚焦于前两者。笔记内容包括：</p><p>一、Batch Normalization</p><ul><li><p>1.1 提出背景</p></li><li><ul><li>1.1.1 ICS所带来的问题</li><li>1.1.2 解决ICS的常规方法</li></ul></li><li><p>1.2 BN的实践</p></li><li><ul><li>1.2.1 思路</li><li>1.2.1 训练过程中的BN</li><li>1.2.2 测试过程中的BN</li></ul></li><li><p>1.3 BN的优势总结</p></li><li><p>1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？</p></li></ul><p>二、Layer Normalization</p><ul><li>2.1 背景 (为何NLP多用LN，图像多用BN)</li><li>2.2 思路</li><li>2.3 训练过程和测试过程中的LN</li></ul><p>三、Transformer LN改进方法：Pre-LN</p><ul><li>3.1 思路和实践方法</li><li>3.2 实验效果</li></ul><h2 id="一、Batch-Normalization"><a href="#一、Batch-Normalization" class="headerlink" title="一、Batch Normalization"></a>一、Batch Normalization</h2><p>本节1.2-1.3的部分，在借鉴<a href="https://zhuanlan.zhihu.com/p/34879333">天雨粟：Batch Normalization原理与实战</a>解说的基础上，增加了自己对论文和实操的解读，并附上图解。上面这篇文章写得非常清晰，推荐给大家阅读～</p><h3 id="1-1-提出背景"><a href="#1-1-提出背景" class="headerlink" title="1.1 提出背景"></a>1.1 提出背景</h3><p>Batch Normalization（以下简称BN）的方法最早由<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1502.03167.pdf">Ioffe&amp;Szegedy</a>在2015年提出，主要用于解决在深度学习中产生的<strong>ICS</strong>（Internal Covariate Shift）的问题。若模型输入层数据分布发生变化，则模型在这波变化数据上的表现将有所波动，输入层分布的变化称为Covariate Shift，解决它的办法就是常说的Domain Adaptation。同理，在深度学习中，第L+1层的输入，也可能随着第L层参数的变动，而引起分布的变动。这样每一层在训练时，都要去适应这样的分布变化，使得训练变得困难。这种层间输入分布变动的情况，就是Internal Covariate Shift。而BN提出的初衷就是为了解决这一问题。</p><p>$$Z^{[L]}&#x3D;W^{[L]}*A^{[L-1]}+b^{[L]}$$（线性变化层）</p><p>$$A^{[L]}&#x3D;g^{[L]}(Z^{[L]})$$（非线性变化&#x2F;激活函数层）</p><p>（ICS：随着梯度下降的进行， $W[L]$和$b[L]$都会被更新，则$Z^{[L]}$的分布改变，进而影响$A^{[L]}$分布，也就是第L+1层的输出）</p><h3 id="1-1-1-ICS所带来的问题"><a href="#1-1-1-ICS所带来的问题" class="headerlink" title="1.1.1 ICS所带来的问题"></a>1.1.1 ICS所带来的问题</h3><p><strong>（1）在过激活层的时候，容易陷入激活层的梯度饱和区，降低模型收敛速度。</strong><br>这一现象发生在我们对模型使用饱和激活函数(saturated activation function)，例如sigmoid，tanh时。如下图：</p><p><img src="https://s2.loli.net/2024/04/24/A8cnJsv35hSbE2m.webp" alt="img"></p><p>几种常用的激活函数</p><p>可以发现当绝对值越大时，数据落入图中两端的梯度饱和区（saturated regime），造成梯度消失，进而降低模型收敛速度。当数据分布变动非常大时，这样的情况是经常发生的。当然，解决这一问题的办法可以采用非饱和的激活函数，例如ReLu。<br><strong>（2）需要采用更低的学习速率，这样同样也降低了模型收敛速度。</strong><br>如前所说，由于输入变动大，上层网络需要不断调整去适应下层网络，因此这个时候的学习速率不宜设得过大，因为梯度下降的每一步都不是“确信”的。</p><p>可以发现当绝对值越大时，数据落入图中两端的梯度饱和区（saturated regime），造成梯度消失，进而降低模型收敛速度。当数据分布变动非常大时，这样的情况是经常发生的。当然，解决这一问题的办法可以采用非饱和的激活函数，例如ReLu。<br><strong>（2）需要采用更低的学习速率，这样同样也降低了模型收敛速度。</strong><br>如前所说，由于输入变动大，上层网络需要不断调整去适应下层网络，因此这个时候的学习速率不宜设得过大，因为梯度下降的每一步都不是“确信”的。</p><ul><li>使得输入的特征具有相同的均值和方差。例如采用PCA，就让所有特征的分布均值为0，方差为1</li><li>去除特征之间的相关性。</li></ul><p>然而在每一层使用白化，给模型增加了运算量。而小心地调整学习速率或其他参数，又陷入到了超参调整策略的复杂中。因此，BN作为一种更优雅的解决办法被提出了。</p><h3 id="1-2-BN的实践"><a href="#1-2-BN的实践" class="headerlink" title="1.2 BN的实践"></a>1.2 BN的实践</h3><h3 id="1-2-1-思路"><a href="#1-2-1-思路" class="headerlink" title="1.2.1 思路"></a>1.2.1 思路</h3><ul><li>对每一个batch进行操作，使得对于这一个batch中所有的输入数据，它们的每一个特征都是均值为0，方差为1的分布</li><li>单纯把所有的输入限制为(0,1)分布也是不合理的，这样会降低数据的表达能力（第L层辛苦学到的东西，这里都暴力变成（0,1）分布了）。因此需要再加一个线性变换操作，让数据恢复其表达能力。这个线性变化中的两个参数 𝛾,𝛽 是需要模型去学习的。</li></ul><p>整个BN的过程可以见下图：</p><p><img src="https://s2.loli.net/2024/04/24/uQPhajEny1tJK8c.webp" alt="img"></p><p>Batch Normalization的过程</p><p>上图所示的是2D数据下的BN，而在NLP或图像任务中，我们通常遇到3D或4D的数据，例如：</p><ul><li>图像中的数据维度：（N, C, H, W)。其中N表示数据量（图数），C表示channel数，H表示高度，W表示宽度。</li><li>NLP中的数据为度：（B, S, E）。其中B表示批量大小，S表示序列长度，F表示序列里每个token的embedding向量维度。</li></ul><p>如下图，它们在执行BN时，在图中每一个蓝色的平面上求取$\mu,\sigma $，同时让模型自己学习 $\gamma,\beta $。其中”H,W”表示的是”H<em>W”，即每一个channel里pixel的数量。为了表达统一，这张图用作NLP任务说明时，可将(N, C, H</em>W)分别理解成(B, S, E)。</p><p><img src="https://s2.loli.net/2024/04/24/Q8dCNIVYuD6Pji9.webp" alt="img"></p><p>BN的范围：对蓝色部分求均值和方差</p><h3 id="1-2-1-训练过程中的BN"><a href="#1-2-1-训练过程中的BN" class="headerlink" title="1.2.1 训练过程中的BN"></a>1.2.1 训练过程中的BN</h3><p>配合上面的图例，我们来具体写一下训练中BN的计算方式。<br>假设一个batch中有m个样本，则在神经网络的某一层中，我们记第i个样本在改层第j个神经元中，经过线性变换后的输出为$Z_j^i$，则BN的过程可以写成（图中的每一个红色方框）：</p><p>$$\begin{aligned}<br>&amp;\mu_{j} &#x3D;\frac1m\sum_{i&#x3D;1}^mZ_j^i  \<br>&amp;\sigma_j^2 &#x3D;\frac1m\sum_{i&#x3D;1}^m(Z_j^i-\mu_j)^2  \<br>&amp;\tilde{Z}_{j} &#x3D;\gamma_j\frac{Z_j-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}+\beta_j<br>\end{aligned}$$</p><p>其中$\epsilon $是为了防止方差为0时产生的计算错误，而$\gamma_{j},\beta_{j}$则是模型学习出来的参数，目的是为了尽量还原数据的表达能力。</p><h3 id="1-2-2-测试过程中的BN"><a href="#1-2-2-测试过程中的BN" class="headerlink" title="1.2.2 测试过程中的BN"></a>1.2.2 测试过程中的BN</h3><p>在训练过程里，我们等一个batch的数据都装满之后，再把数据装入模型，做BN。但是在测试过程中，我们却更希望模型能来一条数据就做一次预测，而不是等到装满一个batch再做预测。也就是说，我们希望测试过程共的数据不依赖batch，每一条数据都有一个唯一预测结果。这就产生了训练和测试中的gap：测试里的 𝜇,𝜎 要怎么算呢？一般来说有两种方法。</p><p><strong>（1）用训练集中的均值和方差做测试集中均值和方差的无偏估计</strong></p><p>保留训练模型中，<strong>每一组</strong>batch的<strong>每一个</strong>特征在<strong>每一层</strong>的$\mu_{batch},\sigma_{batch}^2$，这样我们就可以得到测试数据均值和方差的无偏估计：</p><p>$$\begin{aligned}<br>\mu_{test}&amp; &#x3D;\mathbb{E}(\mu_{batch})  \<br>\sigma_{test}^2&amp; &#x3D;\frac m{m-1}\mathbb{E}(\sigma_{batch}^2)  \<br>BN(X_{test})&amp; &#x3D;\gamma\frac{X_{test}-\mu_{test}}{\sqrt{\sigma_{test}^2+\epsilon}}+\beta<br>\end{aligned}$$</p><p>其中m表示的是批量大小。<br>这种做法有一个明显的缺点：需要消耗较大的存储空间，保存训练过程中所有的均值和方差结果（每一组，每一个，每一层）。</p><p><strong>（2）Momentum：移动平均法(Moving Average)</strong></p><p>稍微改变一下训练过程中计算均值和方差的办法，设$μ_t$是当前步骤求出的均值，$\bar{\mu}$是之前的训练步骤累积起来求得的均值（也称running mean），则：<br>$$\bar{\mu}\leftarrow p\bar{\mu}+(1-p)\mu^t$$<br>其中，p是momentum的超参，表示模型在多大程度上依赖于过去的均值和当前的均值。$\bar{\mu}$则是新一轮的ruuning mean，也就是当前步骤里最终使用的mean。同理，对于方差，我们也有：<br>$$\bar{\sigma^2}\leftarrow p\bar{\sigma^2}+(1-p){\sigma^2}^t$$</p><p>采用这种方法的好处是：</p><ul><li>节省了存储空间，不需要保存所有的均值和方差结果，只需要保存running mean和running variance即可</li><li>方便在训练模型的阶段追踪模型的表现。一般来讲，在模型训练的中途，我们会塞入validation dataset，对模型训练的效果进行追踪。采用移动平均法，不需要等模型训练过程结束再去做无偏估计，我们直接用running mean和running variance就可以在validation上评估模型。</li></ul><h3 id="1-3-BN的优势总结"><a href="#1-3-BN的优势总结" class="headerlink" title="1.3 BN的优势总结"></a>1.3 BN的优势总结</h3><ul><li>通过解决ICS的问题，使得每一层神经网络的输入分布稳定，在这个基础上可以使用较大的学习率，加速了模型的训练速度</li><li>起到一定的正则作用，进而减少了dropout的使用。当我们通过BN规整数据的分布以后，就可以尽量避免一些极端值造成的overfitting的问题</li><li>使得数据不落入饱和性激活函数（如sigmoid，tanh等）饱和区间，避免梯度消失的问题</li></ul><h3 id="1-4-大反转：著名深度学习方法BN成功的秘密竟不在ICS？"><a href="#1-4-大反转：著名深度学习方法BN成功的秘密竟不在ICS？" class="headerlink" title="1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？"></a><strong>1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？</strong></h3><p>以解决ICS为目的而提出的BN，在各个比较实验中都取得了更优的结果。但是来自MIT的<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1805.11604.pdf">Santurkar et al. 2019</a>却指出：</p><ul><li>就算发生了ICS问题，模型的表现也没有更差</li><li>BN对解决ICS问题的能力是有限的</li><li><strong>BN奏效的根本原因在于它让optimization landscape更平滑</strong></li></ul><p>而在这之后的很多论文里也都对这一点进行了不同理论和实验的论证。<br>（每篇论文的Intro部分开头总有一句话类似于：“BN的奏效至今还是个玄学”。。。）</p><p>图中是VGG网络在标准，BN，noisy-BN下的实验结果。其中noisy-BN表示对神经网络的每一层输入，都随机添加来自分布(non-zero mean, non-unit variance)的噪音数据，并且在不同的timestep上，这个分布的mean和variance都在改变。noisy-BN保证了在神经网络的每一层下，输入分布都有严重的ICS问题。但是从试验结果来看，noisy-BN的准确率比标准下的准确率还要更高一些，这说明ICS问题并不是模型效果差的一个绝对原因。</p><p><img src="https://s2.loli.net/2024/04/24/nAahP7UBDSs1NMO.webp" alt="img"></p><p>而当用VGG网络训练CIFAR-10数据时，也可以发现，在更深层的网络（例如Layer11）中，在采用BN的情况下，数据分布也没有想象中的“规整”：</p><p><img src="https://s2.loli.net/2024/04/24/63hyAvEFglzaC5k.webp" alt="img"></p><p>最后，在VGG网络上，对于不同的训练step，计算其在不同batch上loss和gradient的方差（a和b中的阴影部分），同时测量 $\beta-smoothness$（简答理解为l2-norm表示的在一个梯度下降过程中的最大斜率差）。可以发现BN相较于标准情况都来得更加平滑。</p><p><img src="https://s2.loli.net/2024/04/24/ipNIKHJhFbcWAP8.webp" alt="img"></p><h2 id="二、Layer-Normalization"><a href="#二、Layer-Normalization" class="headerlink" title="二、Layer Normalization"></a>二、Layer Normalization</h2><h3 id="2-1-背景"><a href="#2-1-背景" class="headerlink" title="2.1 背景"></a>2.1 背景</h3><p>BN提出后，被广泛作用在CNN任务上来处理图像，并取得了很好的效果。针对文本任务，<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1607.06450.pdf">Ba et al. 2016</a> 提出在RNN上使用Layer Normalization（以下简称LN）的方法，用于解决BN无法很好地处理文本数据长度不一的问题。例如采用RNN模型+BN，我们需要对不同数据的同一个位置的token向量计算 $\mu,\sigma^{2}$，在句子长短不一的情况下，容易出现：</p><ul><li>测试集中出现比训练集更长的数据，由于BN在训练中累积 $\mu_{batch},\sigma_{batch}^2$，在测试中使用累计的经验统计量的原因，导致测试集中多出来的数据没有相应的统计量以供使用。 （在实际应用中，通常会对语言类的数据设置一个max_len，多裁少pad，这时没有上面所说的这个问题。但这里我们讨论的是理论上的情况，即理论上，诸如Transformer这样的模型，是支持任意长度的输入数据的）</li><li>长短不一的情况下，文本中的某些位置没有足够的batch_size的数据，使得计算出来的$\mu,\sigma^2$产生偏差。例如<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2003.07845.pdf">Shen et al. (2020)</a>就指出，在数据集Cifar-10（模型RestNet20)和IWLST14（模型Transformer）的训练过程中，计算当前epoch所有batch的统计量$\mu_B,\sigma_B^2$和当前累计（running）统计量$\mu,\sigma^2$的平均Euclidean distance，可以发现文本数据较图像数据的分布差异更大：</li></ul><p><img src="https://s2.loli.net/2024/04/24/j8FEphl1fYOokn7.webp" alt="img"></p><p>这是一个结合实验的解释，而引起这个现象的原因，可能不止是“长短不一”这一个，也可能和数据本身在某一维度分布上的差异性有关。目前相关知识水平有限，只能理解到这里，未来如果有更确切的想法，会在这里补充。</p><h3 id="2-2-思路"><a href="#2-2-思路" class="headerlink" title="2.2 思路"></a>2.2 思路</h3><p>整体做法类似于BN，不同的是LN不是在特征间进行标准化操作（横向操作），而是在整条数据间进行标准化操作（纵向操作）。</p><p><img src="https://s2.loli.net/2024/04/24/fq2BXKlR56GQgwH.webp" alt="img"></p><p>Layer Normalization计算过程</p><p>在图像问题中，LN是指对一整张图片进行标准化处理，即在一张图片所有channel的pixel范围内计算均值和方差。而在NLP的问题中，LN是指在一个句子的一个token的范围内进行标准化。</p><p><img src="https://s2.loli.net/2024/04/24/sBVejnadXg5tQAS.webp" alt="img"></p><p>图像数据中的LN计算范围：在蓝色范围内计算统计值</p><p><img src="https://s2.loli.net/2024/04/24/3k8i2LdrAIXmElt.webp" alt="img"></p><p>NLP任务中的计算范围</p><h3 id="2-3-训练过程和测试过程中的LN"><a href="#2-3-训练过程和测试过程中的LN" class="headerlink" title="2.3 训练过程和测试过程中的LN"></a>2.3 训练过程和测试过程中的LN</h3><p>LN使得各条数据间在进行标准化的时候相互独立，因此LN在训练和测试过程中是一致的。LN不需要保留训练过程中的$\mu,\sigma^2$，每当来一条数据时，对这条数据的指定范围内单独计算所需统计量即可。</p><h2 id="三、Transformer-LN改进方法：Pre-LN"><a href="#三、Transformer-LN改进方法：Pre-LN" class="headerlink" title="三、Transformer LN改进方法：Pre-LN"></a>三、Transformer LN改进方法：Pre-LN</h2><p>原始transformer中，采用的是Post-LN，即LN在residule block（图中addtion）之后。<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2002.04745.pdf">Xiong et al. (2020)</a>中提出了一种更优Pre-LN的架构，即LN在residule block之前，它能和Post-LN达到相同甚至更好的训练结果，同时规避了在训练Post-LN中产生的种种问题。两种架构的具体形式可以见下图。</p><p><img src="https://s2.loli.net/2024/04/24/3p6MjLfcgzedKxy.webp" alt="img"></p><p>这篇论文通过理论分析和实验的方式，证明了Pre-LN相比的Post-LN的优势，主要表现在：</p><ul><li>在learning rate schedular上，Pre-LN不需要采用warm-up策略，而Post-LN必须要使用warm-up策略才可以在数据集上取得较好的Loss和BLEU结果。</li><li>在收敛速度上，由于Pre-LN不采用warm-up，其一开始的learning rate较Post-LN更高，因此它的收敛速度更快。</li><li>在超参调整上，warm-up策略带来了两个需要调整的参数：$𝑙𝑟_{𝑚𝑎𝑥}$（最大学习率）和 $T_{warmup}$ (warmup过程的总步数）。这两个参数的调整将会影响到模型最终的效果。而由于transformer模型的训练代价是昂贵的，因此多引入超参，也给模型训练带来了一定难度。</li></ul><blockquote><p><strong>Quick Tips：</strong>warm-up learning rate，即指在训练初期的一定步数内，缓慢将学习率从0升至 $𝑙𝑟_{𝑚𝑎𝑥}$ ，超过此步数范围则采用decay learning rate的策略。在大batch数据集的训练中，warm-up learning rate具有较好的表现。</p></blockquote><p><img src="https://s2.loli.net/2024/04/24/gH7tavlpjzbRZuG.webp" alt="img"></p><p>$lr(t)&#x3D;\frac t{T_{warmup}}lr_{max},t\leq T_{warmup}$</p><p>其中，t表示当前训练步数，$T_{warmup}$ 表示warm-up过程总步数，$𝑙𝑟_{𝑚𝑎𝑥}$表示learning rate最高点。</p><p>总结看来，Pre-LN带来的好处，基本都是因为不需要做warm-up引起的。而引起这一差异的根本原因是：</p><ul><li>Post-LN在输出层的gradient norm较大，且越往下层走，gradient norm呈现下降趋势。这种情况下，在训练初期若采用一个较大的学习率，容易引起模型的震荡。</li><li>Pre-LN在输出层的gradient norm较小，且其不随层数递增或递减而变动，保持稳定。</li><li>无论使用何种Optimzer，不采用warm-up的Post-LN的效果都不如采用warm-up的情况，也不如Pre-LN。</li></ul><p>以三点原因在论文中有详细的理论和实验证明，为了节省篇幅，这里仅贴上实验结果。实验场景为机器翻译，分别在IWSLT(German to English)和WMT(English to German)这两个数据集上进行。w&#x2F;o warm-up表示without warm-up，w&#x2F;warm-up表示with warm-up， $$lr_{max}$$在Post-LN中表示warm-up步骤的最高learning rate，在Pre-LN中表示初始化的learning rate。评价指标分别为Loss和BLEU。这个实验结果证明了上述所说的Pre-LN的优势。更细节的实验和内容可以参见论文。</p><p><img src="https://s2.loli.net/2024/04/24/Ijdl1mGHxe3if98.webp" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记二：Self-Attention（自注意力机制）</title>
      <link href="/2024/04/4417.html"/>
      <url>/2024/04/4417.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer学习笔记二：Self-Attention（自注意力机制）"><a href="#Transformer学习笔记二：Self-Attention（自注意力机制）" class="headerlink" title="Transformer学习笔记二：Self-Attention（自注意力机制）"></a>Transformer学习笔记二：Self-Attention（自注意力机制）</h1><h2 id="一、笔记架构"><a href="#一、笔记架构" class="headerlink" title="一、笔记架构"></a>一、笔记架构</h2><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241116401.png" alt="image-20240424111606353"></p><p>Transformer中的三处Attention</p><p>关于Transformer的系列笔记，预计出如下几篇：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/454482273">Positional Encoding （位置编码），点击跳转</a></li><li>Self-attention（自注意力机制）</li><li><a href="https://zhuanlan.zhihu.com/p/456863215">Batch Norm &amp; Layer Norm（批量标准化&#x2F;层标准化），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/459065530">ResNet（残差网络），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/460678461">Subword Tokenization（子词分词法），点击跳转</a></li><li>组装：Transformer</li></ol><p>笔记持续更新中～希望和各位小伙伴们一起学习～</p><p>在Transformer中，一共涉及到三个Attention零件。这篇笔记将基于这三个零件，对attention机制进行探讨，主要内容包括：</p><p>（1）Attention机制的基本框架<br>（2）Attention Score的计算方法<br>- Dot product<br>- Additive product<br>- Scaled dot product (Transformer论文使用的方法，这里将探讨乘上因子$1&#x2F;\sqrt{d}_k$的意义）<br>（3）Masked Attention<br>（4）Multihead Attention实现方式及可视化（多头的意义）<br>（5）Attention代码实践</p><h2 id="二、Attention构造"><a href="#二、Attention构造" class="headerlink" title="二、Attention构造"></a>二、Attention构造</h2><h3 id="2-1-Attention的基本运作方式"><a href="#2-1-Attention的基本运作方式" class="headerlink" title="2.1 Attention的基本运作方式"></a>2.1 Attention的基本运作方式</h3><p>首先，来看RNN这样一个用于处理序列数据的经典模型。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241117259.webp" alt="img"></p><p>图1: 传统RNN</p><p>在RNN当中，tokens是一个一个被喂给模型的。比如在a3的位置，模型要等a1和a2的信息都处理完成后，才可以生成a3。这样的作用机制，使得RNN存在以下几个问题：<br><strong>(1) Sequential operations的复杂度随着序列长度的增加而增加。</strong><br>这是指模型下一步计算的等待时间，在RNN中为O(N)。该复杂度越大，模型并行计算的能力越差，反之则反。<br><strong>(2) Maximum Path length的复杂度随着序列长度的增加而增加。</strong><br>这是指信息从一个数据点传送到另一个数据点所需要的距离，在RNN中同样为O(N)，距离越大，则在传送的过程中越容易出现信息缺失的情况，即数据点对于远距离处的信息，是很难“看见”的。</p><p>那么，在处理序列化数据的时候，是否有办法，在提升模型的并行运算能力的同时，对于序列中的每个token，也能让它不损失信息地看见序列里的其他tokens呢？</p><p>Attention就作为一种很好的改进办法出现了。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241118414.webp" alt="img"></p><p>图2: Self-attention</p><p>如图，蓝色方框为一个attention模型。在每个位置，例如在a2处产生b2时，attention将会同时看过a1到a4的每个token。此外，每个token生成其对应的输出的过程是同时进行的，计算不需要等待。下面来看attention内部具体的运算过程。</p><h3 id="2-2-Attention的计算过程图解"><a href="#2-2-Attention的计算过程图解" class="headerlink" title="2.2 Attention的计算过程图解"></a>2.2 Attention的计算过程图解</h3><p><strong>2.2.1 Self-attention</strong></p><p><strong>（1）计算框架</strong></p><p>Self-attention的意思是，我们给Attention的输入都来自同一个序列，其计算方式如下：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241118843.webp" alt="img"></p><p>图3: self-attention计算框架 （图片来自李宏毅老师PPT）</p><p>这张图所表示的大致运算过程是：<br>对于每个token，先产生三个向量query，key，value：</p><ul><li>query向量类比于<strong>询问</strong>。某个token问：“其余的token都和我有多大程度的相关呀？”</li><li>key向量类比于<strong>索引</strong>。某个token说：“我把每个询问内容的回答都压缩了下装在我的key里”</li><li>value向量类比于<strong>回答</strong>。某个token说：“我把我自身涵盖的信息又抽取了一层装在我的value里”</li></ul><p>以图中的token a2为例：</p><ul><li>它产生一个query，每个query都去和别的token的key做“<strong>某种方式</strong>”的计算，得到的结果我们称为attention score（即为图中的$$\alpha $$）。则一共得到四个attention score。（attention score又可以被称为attention weight）。</li><li>将这四个score分别乘上每个token的value，我们会得到四个抽取信息完毕的向量。</li><li>将这四个向量相加，就是最终a2过attention模型后所产生的结果b2。</li></ul><p><strong>（2）产生query，key和value</strong></p><p>下图描述了产生query(q)，key(k)和value(v)的过程：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241120795.webp" alt="img"></p><p>图4: 产生query， key和value</p><p>假设batch_size&#x3D;1，输入序列X的形状为(seq_len &#x3D; 4, d_model &#x3D; 6)，则对于这串序列，我们产生三个参数矩阵： $W^Q,W^K,W^V$。通过上述的矩阵乘法，我们可以得到最终的结果Q，K，V。</p><p>一般来说， $W^Q$$和$$W^K$都同样使用k_dim， $𝑊^𝑉 $使用v_dim。k_dim和v_dim不一定要相等，但在transformer的原始论文中，采用的策略是，设num_heads为self-attention的头数，则:<br>𝑘_𝑑𝑖𝑚&#x3D;𝑣_𝑑𝑖𝑚&#x3D;𝑑_𝑚𝑜𝑑𝑒𝑙&#x2F;&#x2F;𝑛𝑢𝑚_ℎ𝑒𝑎𝑑𝑠</p><p>上图所绘是num_heads &#x3D; 1的情况。关于num_heads的概念，在本文的后面会详细解释。</p><p><strong>（3）计算attention score</strong></p><p>总结一下，到目前为止，对于某条输入序列X，我们有：</p><p>​                         $Q&#x3D;XW^Q$  $K&#x3D;XW^K$ $V&#x3D;XW^V$ </p><p>现在，我们做两件事：</p><ul><li>利用Q和K，计算出attention score矩阵，这个矩阵由图3中的$\alpha$组成。</li><li>利用V和attention score矩阵，计算出Attention层最终的输出结果矩阵，这个矩阵由图3中的b组成。</li></ul><p>记最终的输出结果为$Attention(Q,K,V)$，则有：</p><p>$$Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p><p>这个$d_k$就是k_dim，而$softmax(\frac{QK^T}{\sqrt{d_k}})$就是Attention Score矩阵，我们来详细看下这个矩阵的计算过程。</p><p>如图5，计算attention score的主流方式有两种，在transformer的论文中，采用的是dot-product（因为不需要额外再去训练一个W矩阵，运算量更小），因此我们来重点关注一下dot-product。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241127133.webp" alt="img">图5: 计算attention score的两种方式</p><p>更确切地说，论文中所采用的是scaled dot-product，因为乘上了因子$1&#x2F;\sqrt{d_k}$。在softmax之后，attention score矩阵的每一行表示一个token，每一列表示该token和对应位置token的$\alpha$值，因为进行了softmax，每一行的 𝛼$\alpha$值相加等于1。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241128985.webp" alt="img"></p><p>图6: scaled-dot-product</p><p><strong>（勘误：紫色方框中的下标应该是 $\alpha_{11}$,$\alpha_{12}$,$\alpha_{13}$,$\alpha_{14}$, )</strong></p><p><strong>之所以进行scaling，是为了使得在softmax的过程中，梯度下降得更加稳定，避免因为梯度过小而造成模型参数更新的停滞</strong>。下面我们通过数学证明，来解释这个结论。为了表达方便（也为了和论文的标识保持一致），我们把k_dim写成 $d_k$，同理v_dim写成$d_v$ ，S表示softmax函数，假设在做softmax之前，紫色矩阵里的每一个值为$\alpha_{ij}^*$，则有：</p><p>$\alpha_{ij}&#x3D;S(\alpha_{ij}^*)&#x3D;\frac{e^{\alpha_{ij}^*}}{\sum_{j&#x3D;1}^{d_k}e^{\alpha_{ij}^*}}$</p><p>聚焦到紫色矩阵的某一行，对于其中某个$j^{\prime}$，我们有：</p><p>$$\begin{aligned}&amp;\frac{\partial S(\alpha_{ij^{\prime}}^*)}{\partial\alpha_{ij^{\prime}}^*}&#x3D;S(\alpha_{ij^{\prime}}^*)(1-S(\alpha_{ij^{\prime}}^*))\&amp;\frac{\partial S(\alpha_{ij^{\prime}}^*)}{\partial\alpha_{ij}^*}&#x3D;-S(\alpha_{ij^{\prime}}^*)S(\alpha_{ij}^*),&amp;j\neq j^{\prime}\end{aligned}$$</p><p>从上面可以看出：</p><ul><li>当$\alpha_{ij^{\prime}}^*$相对于同一行其他的$\alpha_{ij}^*$ 更大的时候，$S(\alpha_{ij^{\prime}}^*)$趋近于1，$S(\alpha_{ij}^*)$趋近于0，此时以上的两个结果都趋近于0。</li><li>当$\alpha_{ij^{\prime}}^*$相对于同一行其他的$\alpha_{ij}^*$ 更小的时候，$S(\alpha_{ij^{\prime}}^*)$趋近于0，$S(\alpha_{ij}^*)$趋近于1，此时以上的两个结果都趋近于0。</li></ul><p><strong>总结起来，即当</strong>$\alpha_{ij^{\prime}}^*$<strong>相对于其他结果过大或者过小时，都会造成softmax函数的偏导趋近于0（梯度过低）。</strong>在这种情况下，整个模型在backprop的过程中，经过softmax之后，就无法继续传播到softmax之前的函数上，造成模型参数无法更新，影响了模型的训练效率。</p><p>那么$\alpha_{ij}^*$是怎么计算来的呢？通过前面的讲解可以知道：<br>$$\alpha_{ij}^*&#x3D;qk^T&#x3D;\sum_{j&#x3D;1}^{d_k}q_{ij}k_{ji}$$<br>假设向量q和k中的每一个元素都是相互独立，均值为0，方差为1的随机变量，那么易知$\alpha_{ij}^*$的均值也为0，方差为$d_k$。$d_k$较大，意味着不同$\alpha_{ij}^*$间值的差距也很大，这就导致了上面所说的梯度消失的问题。</p><h3 id="2-3-Masked-Attention"><a href="#2-3-Masked-Attention" class="headerlink" title="2.3 Masked Attention"></a>2.3 Masked Attention</h3><p>有时候，我们并不想在做attention的时候，让一个token看到整个序列，我们只想让它看见它左边的序列，而要把右边的序列遮蔽（Mask）起来。例如在transformer的decoder层中，我们就用到了masked attention，这样的操作可以理解为模型为了防止decoder在解码encoder层输出时“作弊”，提前看到了剩下的答案，因此需要强迫模型根据输入序列左边的结果进行attention。</p><p>Masked的实现机制其实很简单，如图：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241136935.webp" alt="img"></p><p>图7: Masked Attention</p><p>首先，我们按照前文所说，正常算attention score，然后我们用一个MASK矩阵去处理它（这里的+号并不是表示相加，只是表示提供了位置覆盖的信息）。在MASK矩阵标1的地方，也就是需要遮蔽的地方，我们把原来的值替换为一个很小的值（比如-1e09），而在MASK矩阵标0的地方，我们保留原始的值。这样，在进softmax的时候，那些被替换的值由于太小，就可以自动忽略不计，从而起到遮蔽的效果。</p><p>举例来说明MASK矩阵的含义，每一行表示对应位置的token。例如在第一行第一个位置是0，其余位置是1，这表示第一个token在attention时，只看到它自己，它右边的tokens是看不到的。以此类推。</p><h3 id="2-4-Multihead-Attention"><a href="#2-4-Multihead-Attention" class="headerlink" title="2.4 Multihead Attention"></a>2.4 Multihead Attention</h3><p>在图像中，我们知道有不同的channel，每一个channel可以用来识别一种模式。如果我们对一张图采用attention，比如把这张图的像素格子拉平成一列，那么我们可以对每个像素格子训练不同的head，每个head就类比于一个channel，用于识别不同的模式。</p><p>而在NLP中，这种模式识别同样重要。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等。</p><p>图8展示了multihead attention的运作方式。设头的数量为num_heads，那么本质上，就是训练num_heads个 $W^Q$，$ W^K$,$W^V$个矩阵，用于生成num_heads个 𝑄,𝐾,𝑉 结果。每个结果的计算方式和单头的attention的计算方式一致。最终将生成的b连接起来生成最后的结果。图9详细展示了8个head的矩阵化的运算过程，由于拆分成了多头，则此时有</p><p>$$k_dim&#x3D;v_dim&#x3D;d_model&#x2F;&#x2F;num_heads$$也就是说$W^Q$，$ W^K$,$W^V$的维度变为 ($d_model,d_model&#x2F;&#x2F;num_heads$)。按照这个规则拆分后，多头的运算量和原来单头的运算量一样。同时在图9中，在输出部分出现了一个$W^O$矩阵，这个矩阵用于将拼接起来的多头输出转换为最终总输出</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241142279.webp" alt="img"></p><p>图8: Multihead Attention</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241142973.webp" alt="img"></p><p>图9: 8头Attention矩阵化计算过程</p><p>将每个head上的attention score分数打出，可以具象化地感受每个head的关注点，以入句子”The animal didn’t cross the streest because it was too tired”为例，可视化代码可<a href="https://link.zhihu.com/?target=https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb%23scrollTo=OJKU36QAfqOC">点此</a>（存在Google colab上，需要翻墙）。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241144601.webp" alt="img"></p><p>图10: 单头attention可视化</p><p>如图10，颜色越深表示attention score越大，我们构造并连接五层的attention模块，可以发现it和animal，street关系密切。现在我们把8个头全部加上去，参见图11。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241144562.webp" alt="img"></p><p>图11: 8头attention</p><h2 id="三、Attention代码实践"><a href="#三、Attention代码实践" class="headerlink" title="三、Attention代码实践"></a>三、Attention代码实践</h2><p>这里提供一个Mutihead Attention的python实现方法，它可以快速帮助我们了解一个attention层的计算过程，同时可以很方便地打出中间步骤。Tensorflow和Pytorch的源码里有更为工业化的实现方式，包加速运算、引入bias，自定义维度等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Any</span>, <span class="type">Union</span>, <span class="type">Callable</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                num_heads: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                d_model: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                dropout: <span class="built_in">float</span>=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model must be divisible by num_heads&quot;</span></span><br><span class="line">        <span class="comment"># Assume v_dim always equals k_dim</span></span><br><span class="line">        self.k_dim = d_model // num_heads</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.proj_weights = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) <span class="comment"># W^Q, W^K, W^V, W^O</span></span><br><span class="line">        self.attention_score = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                query:Tensor, </span></span><br><span class="line"><span class="params">                key: Tensor, </span></span><br><span class="line"><span class="params">                value: Tensor, </span></span><br><span class="line"><span class="params">                mask:<span class="type">Optional</span>[Tensor]=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            key: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            value: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            mask: shape (batch_size, seq_len, seq_len). Since we assume all data use a same mask, so</span></span><br><span class="line"><span class="string">                  here the shape also equals to (1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            out: shape (batch_size, seq_len, d_model). The output of a multihead attention layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Apply W^Q, W^K, W^V to generate new query, key, value</span></span><br><span class="line">        query, key, value \</span><br><span class="line">            = [proj_weight(x).view(batch_size, -<span class="number">1</span>, self.num_heads, self.k_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">for</span> proj_weight, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.proj_weights, [query, key, value])] <span class="comment"># -1 equals to seq_len</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Calculate attention score and the out</span></span><br><span class="line">        out, self.attention_score = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; output</span></span><br><span class="line">        out = out.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(batch_size, -<span class="number">1</span>, self.num_heads * self.k_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4) Apply W^O to get the final output</span></span><br><span class="line">        out = self.proj_weights[-<span class="number">1</span>](out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">        <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query: Tensor, </span></span><br><span class="line"><span class="params">              key: Tensor, </span></span><br><span class="line"><span class="params">              value: Tensor, </span></span><br><span class="line"><span class="params">              mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">              dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Define how to calculate attention score</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query: shape (batch_size, num_heads, seq_len, k_dim)</span></span><br><span class="line"><span class="string">        key: shape(batch_size, num_heads, seq_len, k_dim)</span></span><br><span class="line"><span class="string">        value: shape(batch_size, num_heads, seq_len, v_dim)</span></span><br><span class="line"><span class="string">        mask: shape (batch_size, num_heads, seq_len, seq_len). Since our assumption, here the shape is</span></span><br><span class="line"><span class="string">              (1, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        out: shape (batch_size, v_dim). Output of an attention head.</span></span><br><span class="line"><span class="string">        attention_score: shape (seq_len, seq_len).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    k_dim = query.size(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shape (seq_len ,seq_len)，row: token，col: that token&#x27;s attention score</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(k_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e10</span>)</span><br><span class="line"></span><br><span class="line">    attention_score = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attention_score = dropout(attention_score)</span><br><span class="line">        </span><br><span class="line">    out = torch.matmul(attention_score, value)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out, attention_score <span class="comment"># shape: (seq_len, v_dim), (seq_len, seq_lem)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    d_model = <span class="number">8</span></span><br><span class="line">    seq_len = <span class="number">3</span></span><br><span class="line">    batch_size = <span class="number">6</span></span><br><span class="line">    num_heads = <span class="number">2</span></span><br><span class="line">    <span class="comment"># mask = None</span></span><br><span class="line">    mask = torch.tril(torch.ones((seq_len, seq_len)), diagonal = <span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">input</span> = torch.rand(batch_size, seq_len, d_model)</span><br><span class="line">    multi_attn = MultiHeadedAttention(num_heads = num_heads, d_model = d_model, dropout = <span class="number">0.1</span>)</span><br><span class="line">    out = multi_attn(query = <span class="built_in">input</span>, key = <span class="built_in">input</span>, value = <span class="built_in">input</span>, mask = mask)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404241151445.webp" alt="动图"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记一：Positional Encoding（位置编码）</title>
      <link href="/2024/04/e5ad.html"/>
      <url>/2024/04/e5ad.html</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer学习笔记一：Positional-Encoding（位置编码）"><a href="#Transformer学习笔记一：Positional-Encoding（位置编码）" class="headerlink" title="Transformer学习笔记一：Positional Encoding（位置编码）"></a>Transformer学习笔记一：Positional Encoding（位置编码）</h1><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231200450.png"></p><p><strong>小小引流一下，最近在更新ChatGPT系列，感兴趣的朋友可以移步<a href="https://zhuanlan.zhihu.com/p/605516116">猛猿：ChatGPT技术解析系列之：训练框架InstructGPT</a></strong></p><p>自从2017年Transformer模型被提出以来，它已经从论文最初的机器翻译领域，转向图像，语音，视频等等方面的应用（实现作者们在论文结论里的大同之梦）。原论文的篇幅很紧密，不看代码的话，缺乏了很多细节描述。我的学历经历大概是两周啃paper+代码 &#x3D;&gt; 两周挖细节&#x3D;&gt;未来这个模型还有很多值得端详。在Transformer系列的笔记里，我把模型拆成了各个零件进行学习，最后把这些零件组装成Transformer，涵盖内容如下：</p><ol><li>Positional Encoding （位置编码）</li><li><a href="https://zhuanlan.zhihu.com/p/455399791">Self-attention（自注意力机制），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/456863215">Batch Norm &amp; Layer Norm（批量标准化&#x2F;层标准化）,点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/459065530">ResNet（残差网络），点击跳转</a></li><li><a href="https://zhuanlan.zhihu.com/p/460678461">Subword Tokenization（子词分词法），点击跳转</a></li><li>组装：Transformer</li></ol><p>这是Transformer系列的第一篇。这个笔记系列（即上方超链接）持续更新，欢迎大家一起来学习～</p><p>本篇目录结构如下：</p><p>一、什么是位置编码</p><p>二、构造位置编码的方法 &#x2F;演变历程</p><ul><li>2.1 用整型值标记位置</li><li>2.2 用[0,1]范围标记位置</li><li>2.3 用二进制向量标记位置</li><li>2.4 用周期函数（sin）来表示位置</li><li>2.5 用sin和cos交替来表示位置</li></ul><p>三、Transformer中位置编码方法：Sinusoidal functions</p><ul><li>3.1 Transformer 位置编码定义</li><li>3.2 Transformer位置编码可视化</li><li>3.3 Transformer位置编码的重要性质</li></ul><p>四、参考</p><h2 id="一、什么是位置编码"><a href="#一、什么是位置编码" class="headerlink" title="一、什么是位置编码"></a>一、什么是位置编码</h2><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231202824.png" alt="image-20240423120208778"></p><p>在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足：</p><p>$$input&#x3D;input_embedding+positional_encoding$$</p><p>这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model &#x3D; 512）</p><p>那么，我们为什么需要position encoding呢？在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231203755.png" alt="image-20240423120311690"></p><p>在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：<br>（1）绝对位置信息。a1是第一个token，a2是第二个token……<br>（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位……<br>（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置….<br>但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。</p><h2 id="二、构造位置编码的方法-演变历程"><a href="#二、构造位置编码的方法-演变历程" class="headerlink" title="二、构造位置编码的方法 &#x2F;演变历程"></a>二、构造位置编码的方法 &#x2F;演变历程</h2><h3 id="2-1-用整型值标记位置"><a href="#2-1-用整型值标记位置" class="headerlink" title="2.1 用整型值标记位置"></a>2.1 用整型值标记位置</h3><p>一种自然而然的想法是，给第一个token标记1，给第二个token标记2…，以此类推。<br>这种方法产生了以下几个主要问题：<br>（1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。<br>（2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。</p><h3 id="2-2-用-0-1-范围标记位置"><a href="#2-2-用-0-1-范围标记位置" class="headerlink" title="2.2 用[0,1]范围标记位置"></a>2.2 用[0,1]范围标记位置</h3><p>为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。<br>但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。</p><p>因此，我们需要这样一种位置表示方式，满足于：<br>（1）它能用来表示一个token在序列中的绝对位置<br>（2）在序列长度不同的情况下，不同序列中token的相对位置&#x2F;距离也要保持一致<br>（3）可以用来表示模型在训练过程中从来没有看到过的句子长度。</p><h3 id="2-3-用二进制向量标记位置"><a href="#2-3-用二进制向量标记位置" class="headerlink" title="2.3 用二进制向量标记位置"></a>2.3 用二进制向量标记位置</h3><p>考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model &#x3D; 3，那么我们的位置向量可以表示成：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231204272.jpeg" alt="img"></p><p>这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。</p><p>但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model &#x3D; 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231205009.webp" alt="img"></p><p>如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线），那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。</p><h3 id="2-4-用周期函数（sin）来表示位置"><a href="#2-4-用周期函数（sin）来表示位置" class="headerlink" title="2.4 用周期函数（sin）来表示位置"></a>2.4 用周期函数（sin）来表示位置</h3><p>回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量可以表示为：</p><p>$$PE_t&#x3D;[sin(\frac{1}{2^0}t),sin(\frac{1}{2^1}t)\ldots,sin(\frac{1}{2^{i-1}}t),\ldots,sin(\frac{1}{2^{d_{model}-1}}t)]$$</p><p>结合下图，来理解一下这样设计的含义。图中每一行表示一个$PE_{t}$，每一列表示 $PE_{t}$中的第i个元素。旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中t的作用）。通过频率$\frac1{2^{i-1}}$来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 这也类似于二进制编码，每一位上都是0和1的交互，越往低位走（越往左边走），交互的频率越慢。</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231207562.webp" alt="img"></p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231207393.webp" alt="越往左边走，交互频率越慢"></p><p>越往左边走，交互频率越慢</p><p>由于sin是周期函数，因此从纵向来看，如果函数的频率偏大，引起波长偏短，则<strong>不同t下的位置向量可能出现重合</strong>的情况。比如在下图中(d_model &#x3D; 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置响亮点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231208351.webp" alt="img"></p><p>为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了 $\frac{1}{10000^{i&#x2F;(d_{model}-1)}}$ 这个频率（这里i其实不是表示第i个位置，但是大致意思差不多，下面会细说）</p><p>总结一下，到这里我们把位置向量表示为：</p><p>$$PE_t&#x3D;[sin(w_0t),sin(w_1t)\ldots,sin(w_{i-1}t),\ldots,sin(w_{d_{model}-1}t)]$$</p><p>其中， $w_i&#x3D;\frac{1}{10000^{i&#x2F;(d_{model}-1)}}$</p><h3 id="2-5-用sin和cos交替来表示位置"><a href="#2-5-用sin和cos交替来表示位置" class="headerlink" title="2.5 用sin和cos交替来表示位置"></a>2.5 用sin和cos交替来表示位置</h3><p>目前为止，我们的位置向量实现了如下功能：<br>（1）每个token的向量唯一（每个sin函数的频率足够小）<br>（2）位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质）</p><p>那现在我们对位置向量再提出一个要求，<strong>不同的位置向量是可以通过线性转换得到的</strong>。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置，即我们想要：</p><p>$$PE_{t+\triangle t}&#x3D;T_{\triangle t}*PE_t$$</p><p>这里，T表示一个线性变换矩阵。观察这个目标式子，联想到在向量空间中一种常用的线形变换——旋转。在这里，我们将t想象为一个角度，那么$\triangle t$就是其旋转的角度，则上面的式子可以进一步写成：</p><p>$$\begin{pmatrix}\sin(t+\triangle t)\\cos((t+\triangle t)\end{pmatrix}&#x3D;\begin{pmatrix}\cos\triangle t&amp;\sin\triangle t\-\sin\triangle t&amp;\cos\triangle t\end{pmatrix}\begin{pmatrix}\sin t\\cos t\end{pmatrix}$$</p><p>有了这个构想，我们就可以把原来元素全都是sin函数的 $PE_{t}$做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有：</p><p>$$PE_{t}&#x3D;[sin(w_0t),cos(w_0t),sin(w_1t),cos(w_1t),\ldots,sin(w_{\frac{d_{model}}{2}-1}t),\cos(w_{\frac{d_{model}}{2}-1}t)]$$</p><p>在这样的表示下，我们可以很容易用一个线性变换，把$PE_{t}$转变为$PE_{t+\triangle t}$ :</p><p>$$PE_{t+\triangle t}&#x3D;T_{\triangle t}*PE_{t}&#x3D;\begin{pmatrix}\begin{bmatrix}cos(w_0\bigtriangleup t) &amp; sin(w_0\bigtriangleup t)\ -sin(w_0\bigtriangleup t) &amp; cos(w_0\bigtriangleup t)\end{bmatrix} &amp; \cdots &amp; 0\ \cdots &amp; \cdots &amp; \cdots\ 0 &amp; \cdots &amp; \begin{bmatrix}cos(w_{\frac{d_{madd}}2-1}\bigtriangleup t) &amp; sin(w_{\frac{d_{madd}}2-1}\bigtriangleup t)\ -sin(w_{\frac{d_{madd}}2-1}\bigtriangleup t) &amp; cos(w_{\frac{d_{madd}}2-1}\bigtriangleup t)\end{bmatrix}\end{pmatrix}\begin{pmatrix}sin(w_0t)\ cos(w_0t)\ \cdots\ sin(w_{\frac{d_{madd}}2-1}t)\ cos(w_{\frac{d_{madd}}2-1}t)\end{pmatrix}&#x3D;\begin{pmatrix}sin(w_0(t+\triangle t))\ cos(w_0(t+\triangle t))\ \cdots\ sin(w_{\frac{d_{madd}}2-1}(t+\triangle t))\ cos(w_{\frac{d_{madd}}2-1}(t+\triangle t))\end{pmatrix}$$</p><h2 id="三、Transformer中位置编码方法：Sinusoidal-functions"><a href="#三、Transformer中位置编码方法：Sinusoidal-functions" class="headerlink" title="三、Transformer中位置编码方法：Sinusoidal functions"></a>三、Transformer中位置编码方法：Sinusoidal functions</h2><h3 id="3-1-Transformer-位置编码定义"><a href="#3-1-Transformer-位置编码定义" class="headerlink" title="3.1 Transformer 位置编码定义"></a>3.1 Transformer 位置编码定义</h3><p>有了上面的演变过程后，现在我们就可以正式来看transformer中的位置编码方法了。</p><p>定义：<br>- t是这个token在序列中的实际位置（例如第一个token为1，第二个token为2…）<br>- $PE_t\in\mathbb{R}^d$是这个token的位置向量，$PE_t^{(i)}$表示这个位置向量里的第i个元素<br>- $d_{model}$是这个token的维度（在论文中，是512)</p><p>则$PE_t^{(i)}$可以表示为：</p><p>$$\left.PE_t^{(i)}&#x3D;\left{\begin{array}{lr}\sin(w_kt),&amp;ifi&#x3D;2k\\cos(w_kt),&amp;ifi&#x3D;2k+1\end{array}\right.\right.$$</p><p>这里：$w_k&#x3D;\frac{1}{10000^{2k&#x2F;d_{model}}}$，$i&#x3D;0,1,2,3,\ldots,\frac{d_{model}}2-1$</p><p>看得有点懵不要紧，这个意思和2.5中的意思是一模一样的，把512维的向量两两一组，每组都是一个sin和一个cos，这两个函数共享同一个频率$w_{i}$，一共有256组，由于我们从0开始编号，所以最后一组编号是255。sin&#x2F;cos函数的波长（由$w_{i}$决定）则从 2𝜋 增长到 2𝜋∗10000。</p><h3 id="3-2-Transformer位置编码可视化"><a href="#3-2-Transformer位置编码可视化" class="headerlink" title="3.2 Transformer位置编码可视化"></a>3.2 Transformer位置编码可视化</h3><p>下图是一串序列长度为50，位置编码维度为128的位置编码可视化结果：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231215754.webp" alt="img"></p><p>可以发现，由于sin&#x2F;cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是蓝色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。</p><h3 id="3-3-Transformer位置编码的重要性质"><a href="#3-3-Transformer位置编码的重要性质" class="headerlink" title="3.3 Transformer位置编码的重要性质"></a>3.3 Transformer位置编码的重要性质</h3><p>让我们再深入探究一下位置编码的性质。</p><p><strong>(1) 性质一：两个位置编码的点积(dot product)仅取决于偏移量</strong> △𝑡 <strong>，也即两个位置编码的点积可以反应出两个位置编码间的距离。</strong></p><p>证明：</p><p>$$\begin{aligned}<br>PE_{t}^{T}*PE_{t+\triangle t}&amp; &#x3D;\sum_{i&#x3D;0}^{\frac{d_{model}}2-1}[sin(w_it)sin(w_i(t+\triangle t)+cos(w_it)cos(w_i(t+\triangle t)]  \<br>&amp;&#x3D;\sum_{i&#x3D;0}^{\frac{d_{model}}2-1}cos(w_i(t-(t+\triangle t))) \<br>&amp;&#x3D;\sum_{i&#x3D;0}^{\frac{d_{model}}{2}-1}cos(w_i\bigtriangleup t)<br>\end{aligned}$$</p><p>**(2) 性质二：位置编码的点积是无向的，即 **$PE_{t}^{T}*PE_{t+\triangle t}&#x3D;PE_{t}^{T}*PE_{t-\triangle t}$</p><p>证明：<br>由于cos函数的对称性，基于性质1，这一点即可证明。<br>我们可以分别训练不同维度的位置向量，然后以某个位置向量$PE_{t}$为基准，去计算其左右和它相距$\triangle t$的位置向量的点积，可以得到如下结果：</p><p><img src="https://pic3.zhimg.com/80/v2-3c9fd774843c50cfceca7e47ffd18d3a_1440w.webp" alt="img"></p><p>这里横轴的k指的就是$\triangle t$，可以发现，距离是对成分布的，且总体来说，$\triangle t$越大或者越小的时候，内积也越小，可以反馈距离的远近。也就是说，虽然位置向量的点积可以用于表示**距离(distance-aware)<strong>，但是它却不能用来表示位置的</strong>方向性(lack-of-directionality)**。</p><p>当位置编码随着input被喂进attention层时，采用的映射方其实是：</p><p>$PE_t^TW_Q^TW_KPE_{t+k}$</p><p>这里$W_Q^T$和$W_{K}$表示self-attention中的query和key参数矩阵，他们可以被简写成 𝑊 表示attention score的矩阵，到这里看不懂也没事，在self-attention的笔记里会说明的）。我们可以随机初始化两组 ，$W_{1}$，$W_{2}$，然后将$PE_t^TW_1PE_{t+k}$， $PE_t^TW_2PE_{t+k}$和$PE_t^TPE_{t+k}$这三个内积进行比较，得到的结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/202404231220261.webp" alt="img"></p><p>绿色和黄色即是$W_{1}$，$W_{2}$的结果。可以发现，进入attention层之后，内积的**距离意识(distance-aware)**的模式也遭到了破坏。更详细的细节，可以参见复旦大学这一篇用transformer做NER的<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1911.04474.pdf">论文</a>中。</p><p>在Transformer的论文中，比较了用positional encoding和learnable position embedding(让模型自己学位置参数）两种方法，得到的结论是两种方法对模型最终的衡量指标差别不大。不过在后面的BERT中，已经改成用learnable position embedding的方法了，也许是因为positional encoding在进attention层后一些优异性质消失的原因（猜想）。Positional encoding有一些想象+实验+论证的意味，而编码的方式也不只这一种，比如把sin和cos换个位置，依然可以用来编码。关于positional encoding，我也还在持续探索中。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>1 3 Transformer</title>
      <link href="/2024/04/463e.html"/>
      <url>/2024/04/463e.html</url>
      
        <content type="html"><![CDATA[<h1 id="第一章-Transformer背景介绍"><a href="#第一章-Transformer背景介绍" class="headerlink" title="第一章 Transformer背景介绍"></a>第一章 Transformer背景介绍</h1><h2 id="1-1-Transformer的诞生"><a href="#1-1-Transformer的诞生" class="headerlink" title="1.1 Transformer的诞生"></a>1.1 Transformer的诞生</h2><hr><p>2018年10月，Google发出一篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》, BERT模型横空出世, 并横扫NLP领域11项任务的最佳成绩!</p><hr><p>论文地址: <a href="http://link.zhihu.com/?target=https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p><hr><p>而在BERT中发挥重要作用的结构就是Transformer, 之后又相继出现XLNET，roBERT等模型击败了BERT，但是他们的核心没有变，仍然是：Transformer.</p><hr><h2 id="1-2-Transformer的优势"><a href="#1-2-Transformer的优势" class="headerlink" title="1.2 Transformer的优势"></a>1.2 Transformer的优势</h2><hr><p>相比之前占领市场的LSTM和GRU模型，Transformer有两个显著的优势:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1, Transformer能够利用分布式GPU进行并行训练，提升模型训练效率.    </span><br><span class="line">2, 在分析预测更长的文本时, 捕捉间隔较长的语义关联效果更好.   </span><br></pre></td></tr></table></figure><hr><p>下面是一张在测评比较图:</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/v2-b07ae6d625c826984a7e357ee1c3ae79_1440w.webp" alt="img"></p><hr><h2 id="1-3-Transformer的市场"><a href="#1-3-Transformer的市场" class="headerlink" title="1.3 Transformer的市场"></a>1.3 Transformer的市场</h2><hr><p>在著名的SOTA机器翻译榜单上, 几乎所有排名靠前的模型都使用Transformer,</p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/v2-493d94f73e744d3eaefbd18e394a6db0_1440w.webp" alt="img"></p><p>其基本上可以看作是工业界的风向标, 市场空间自然不必多说！</p><h1 id="第二章：-Transformer架构解析"><a href="#第二章：-Transformer架构解析" class="headerlink" title="第二章： Transformer架构解析"></a>第二章： Transformer架构解析</h1><h2 id="2-1-认识Transformer架构"><a href="#2-1-认识Transformer架构" class="headerlink" title="2.1 认识Transformer架构"></a>2.1 认识Transformer架构</h2><ul><li><p>学习目标</p><ul><li><p>了解Transformer模型的作用.</p></li><li><p>了解Transformer总体架构图中各个组成部分的名称.</p></li></ul></li><li><p>Transformer模型的作用</p><ul><li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li></ul></li></ul><hr><ul><li><p>声明:</p><ul><li>在接下来的架构分析中, 我们将假设使用Transformer模型架构处理从一种语言文本到另一种语言文本的翻译工作, 因此很多命名方式遵循NLP中的规则. 比如: Embeddding层将称作文本嵌入层, Embedding层产生的张量称为词嵌入张量, 它的最后一维将称作词向量等.</li></ul></li><li><p>Transformer总体架构图</p></li></ul><p><img src="https://pic1.zhimg.com/80/v2-da7519ddb2c983cfc671d884a7646010_1440w.webp" alt="img"></p><ul><li>Transformer总体架构可分为四个部分:<ul><li>输入部分</li><li>输出部分</li><li>编码器部分</li><li>解码器部分</li></ul></li></ul><hr><ul><li>输入部分包含:<ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul></li></ul><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108192804130.png" alt="image-20231108192804130" style="zoom:80%;" /><p>输出部分包含:</p><ul><li>线性层</li><li>softmax层</li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/v2-4b2d21a7199c6f261c9caca82e213b96_1440w.webp" alt="img"></p><p>编码器部分:</p><ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/v2-306c89830ea9efe5953ae4d8b76492a4_1440w.webp" alt="img"></p><p>解码器部分:</p><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/20231108192527.png" alt="img"></p><p><code>小节总结</code></p><ul><li>学习了Transformer模型的作用:<ul><li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li></ul></li></ul><hr><ul><li>Transformer总体架构可分为四个部分:<ul><li>输入部分</li><li>输出部分</li><li>编码器部分</li><li>解码器部分</li></ul></li></ul><hr><ul><li>输入部分包含:<ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul></li></ul><hr><ul><li>输出部分包含:<ul><li>线性层</li><li>softmax处理器</li></ul></li></ul><hr><ul><li><p>编码器部分:</p></li><li><ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><hr><ul><li><p>解码器部分:</p></li><li><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><h1 id="第三章：Transformer经典案例"><a href="#第三章：Transformer经典案例" class="headerlink" title="第三章：Transformer经典案例"></a>第三章：Transformer经典案例</h1><h2 id="3-1-使用Transformer构建语言模型"><a href="#3-1-使用Transformer构建语言模型" class="headerlink" title="3.1 使用Transformer构建语言模型"></a>3.1 使用Transformer构建语言模型</h2><p><strong>学习目标</strong></p><ul><li>了解有关语言模型的知识.</li><li>掌握使用Transformer构建语言模型的实现过程.</li></ul><hr><ul><li><p>什么是语言模型:</p></li><li><ul><li>以一个符合语言规律的序列为输入，模型将利用序列间关系等特征，输出一个在所有词汇上的概率分布.这样的模型称为语言模型.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语言模型的训练语料一般来自于文章，对应的源文本和目标文本形如:</span></span><br><span class="line">src1 = <span class="string">&quot;I can do&quot;</span> tgt1 = <span class="string">&quot;can do it&quot;</span></span><br><span class="line">src2 = <span class="string">&quot;can do it&quot;</span>, tgt2 = <span class="string">&quot;do it &lt;eos&gt;&quot;</span></span><br></pre></td></tr></table></figure><hr><ul><li><p>语言模型能解决哪些问题:</p></li><li><ul><li>1, 根据语言模型的定义，可以在它的基础上完成机器翻译，文本生成等任务，因为我们通过最后输出的概率分布来预测下一个词汇是什么.</li><li>2, 语言模型可以判断输入的序列是否为一句完整的话，因为我们可以根据输出的概率分布查看最大概率是否落在句子结束符上，来判断完整性.</li><li>3, 语言模型本身的训练目标是预测下一个词，因为它的特征提取部分会抽象很多语言序列之间的关系，这些关系可能同样对其他语言类任务有效果.因此可以作为预训练模型进行迁移学习.</li></ul></li></ul><hr><h3 id="整个案例的实现可分为以下五个步骤"><a href="#整个案例的实现可分为以下五个步骤" class="headerlink" title="整个案例的实现可分为以下五个步骤"></a>整个案例的实现可分为以下五个步骤</h3><ul><li>第一步: 导入必备的工具包</li><li>第二步: 导入wikiText-2数据集并作基本处理</li><li>第三步: 构建用于模型输入的批次化数据</li><li>第四步: 构建训练和评估函数</li><li>第五步: 进行训练和评估(包括验证以及测试)</li></ul><hr><h4 id="第一步-导入必备的工具包"><a href="#第一步-导入必备的工具包" class="headerlink" title="第一步: 导入必备的工具包"></a>第一步: 导入必备的工具包</h4><p>环境配置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py37_torch131 python=<span class="number">3.7</span></span><br><span class="line"></span><br><span class="line">conda activate py37_torch131</span><br><span class="line">conda install pytorch=<span class="number">1.3</span><span class="number">.1</span> torchvision cudatoolkit=<span class="number">10.0</span></span><br><span class="line">pip install jupyter tqdm opencv-python matplotlib pandas -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>pytorch版本必须使用1.3.1, python版本使用3.6.x</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==<span class="number">1.3</span><span class="number">.1</span></span><br><span class="line">conda create -n post1 python=<span class="number">3.6</span></span><br><span class="line">conda activate post1</span><br></pre></td></tr></table></figure><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数学计算工具包math</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch以及torch.nn, torch.nn.functional</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch中经典文本数据集有关的工具包</span></span><br><span class="line"><span class="comment"># 具体详情参考下方torchtext介绍</span></span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"></span><br><span class="line"><span class="comment"># torchtext中的数据处理工具, get_tokenizer用于英文分词</span></span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 已经构建完成的TransformerModel</span></span><br><span class="line"><span class="keyword">from</span> pyitcast.transformer <span class="keyword">import</span> TransformerModel</span><br></pre></td></tr></table></figure><hr><ul><li><p>torchtext介绍:</p><ul><li>它是torch工具中处理NLP问题的常用数据处理包.</li></ul></li></ul><hr><ul><li><p>torchtext的重要功能:</p><ul><li>对文本数据进行处理, 比如文本语料加载, 文本迭代器构建等.</li><li>包含很多经典文本语料的预加载方法. 其中包括的语料有：用于情感分析的SST和IMDB, 用于问题分类的TREC, 用于及其翻译的 WMT14， IWSLT，以及用于语言模型任务wikiText-2, WikiText103, PennTreebank.</li></ul></li></ul><hr><ul><li>我们这里使用wikiText-2来训练语言模型, 下面有关该数据集的相关详情:</li></ul><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/v2-c3b86dbc06ce07be24c262652041c9ba_720w.webp" alt="img"></p><p><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108201728406.png" alt="image-20231108201728406"></p><ul><li>wikiText-2数据集的体量中等, 训练集共有600篇短文, 共208万左右的词汇, 33278个不重复词汇, OoV（有多少正常英文词汇不在该数据集中的占比）为2.6%，数据集中的短文都是维基百科中对一些概念的介绍和描述.</li></ul><hr><p><strong>第二步: 导入wikiText-2数据集并作基本处理</strong> P48</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建语料域, 语料域是存放语料的数据结构, </span></span><br><span class="line"><span class="comment"># 它的四个参数代表给存放语料（或称作文本）施加的作用. </span></span><br><span class="line"><span class="comment"># 分别为 tokenize,使用get_tokenizer(&quot;basic_english&quot;)获得一个分割器对象,</span></span><br><span class="line"><span class="comment"># 分割方式按照文本为基础英文进行分割. </span></span><br><span class="line"><span class="comment"># init_token为给文本施加的起始符 &lt;sos&gt;给文本施加的终止符&lt;eos&gt;, </span></span><br><span class="line"><span class="comment"># 最后一个lower为True, 存放的文本字母全部小写.</span></span><br><span class="line">TEXT = torchtext.data.Field(tokenize=get_tokenizer(<span class="string">&quot;basic_english&quot;</span>),</span><br><span class="line">                            init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>,</span><br><span class="line">                            eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>,</span><br><span class="line">                            lower=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终获得一个Field对象.</span></span><br><span class="line"><span class="comment"># &lt;torchtext.data.field.Field object at 0x7fc42a02e7f0&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后使用torchtext的数据集方法导入WikiText2数据, </span></span><br><span class="line"><span class="comment"># 并切分为对应训练文本, 验证文本，测试文本, 并对这些文本施加刚刚创建的语料域.</span></span><br><span class="line">train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以通过examples[0].text取出文本对象进行查看.</span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; test_txt.examples[0].text[:10]</span></span><br><span class="line"><span class="comment"># [&#x27;&lt;eos&gt;&#x27;, &#x27;=&#x27;, &#x27;robert&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;=&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;robert&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;is&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将训练集文本数据构建一个vocab对象, </span></span><br><span class="line"><span class="comment"># 这样可以使用vocab对象的stoi方法统计文本共包含的不重复词汇总数.</span></span><br><span class="line">TEXT.build_vocab(train_txt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后选择设备cuda或者cpu</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><hr><h4 id="第三步-构建用于模型输入的批次化数据-P50"><a href="#第三步-构建用于模型输入的批次化数据-P50" class="headerlink" title="第三步: 构建用于模型输入的批次化数据 P50"></a>第三步: 构建用于模型输入的批次化数据 P50</h4><ul><li>批次化过程的第一个函数batchify代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batchify</span>(<span class="params">data, bsz</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;batchify函数用于将文本数据映射成连续数字, 并转换成指定的样式, 指定的样式可参考下图.</span></span><br><span class="line"><span class="string">       它有两个输入参数, data就是我们之前得到的文本数据(train_txt, val_txt, test_txt),</span></span><br><span class="line"><span class="string">       bsz是就是batch_size, 每次模型更新参数的数据量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用TEXT的numericalize方法将单词映射成对应的连续数字.</span></span><br><span class="line">    data = TEXT.numericalize([data.examples[<span class="number">0</span>].text])</span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; data</span></span><br><span class="line">    <span class="comment"># tensor([[   3],</span></span><br><span class="line">    <span class="comment">#    [  12],</span></span><br><span class="line">    <span class="comment">#    [3852],</span></span><br><span class="line">    <span class="comment">#    ...,</span></span><br><span class="line">    <span class="comment">#    [   6],</span></span><br><span class="line">    <span class="comment">#    [   3],</span></span><br><span class="line">    <span class="comment">#    [   3]])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着用数据词汇总数除以bsz,</span></span><br><span class="line">    <span class="comment"># 取整数得到一个nbatch代表需要多少次batch后能够遍历完所有数据</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之后使用narrow方法对不规整的剩余数据进行删除,</span></span><br><span class="line">    <span class="comment"># 第一个参数是代表横轴删除还是纵轴删除, 0为横轴，1为纵轴</span></span><br><span class="line">    <span class="comment"># 第二个和第三个参数代表保留开始轴到结束轴的数值.类似于切片</span></span><br><span class="line">    <span class="comment"># 可参考下方演示示例进行更深理解.</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; data</span></span><br><span class="line">    <span class="comment"># tensor([[   3],</span></span><br><span class="line">    <span class="comment">#    [  12],</span></span><br><span class="line">    <span class="comment">#    [3852],</span></span><br><span class="line">    <span class="comment">#    ...,</span></span><br><span class="line">    <span class="comment">#    [  78],</span></span><br><span class="line">    <span class="comment">#    [ 299],</span></span><br><span class="line">    <span class="comment">#    [  36]])</span></span><br><span class="line">    <span class="comment"># 后面不能形成bsz个的一组数据被删除</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着我们使用view方法对data进行矩阵变换, 使其成为如下样式:</span></span><br><span class="line">    <span class="comment"># tensor([[    3,    25,  1849,  ...,     5,    65,    30],</span></span><br><span class="line">    <span class="comment">#    [   12,    66,    13,  ...,    35,  2438,  4064],</span></span><br><span class="line">    <span class="comment">#    [ 3852, 13667,  2962,  ...,   902,    33,    20],</span></span><br><span class="line">    <span class="comment">#    ...,</span></span><br><span class="line">    <span class="comment">#    [  154,     7,    10,  ...,     5,  1076,    78],</span></span><br><span class="line">    <span class="comment">#    [   25,     4,  4135,  ...,     4,    56,   299],</span></span><br><span class="line">    <span class="comment">#    [    6,    57,   385,  ...,  3168,   737,    36]])</span></span><br><span class="line">    <span class="comment"># 因为会做转置操作, 因此这个矩阵的形状是[None, bsz],</span></span><br><span class="line">    <span class="comment"># 如果输入是训练数据的话，形状为[104335, 20], 可以通过打印data.shape获得.</span></span><br><span class="line">    <span class="comment"># 也就是data的列数是等于bsz的值的.</span></span><br><span class="line">    data = data.view(bsz, -<span class="number">1</span>).t().contiguous()</span><br><span class="line">    <span class="comment"># 最后将数据分配在指定的设备上.</span></span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br></pre></td></tr></table></figure><hr><ul><li>batchify的样式转化图:</li></ul><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108133731374.png" alt="image-20231108133731374" style="zoom:80%;" /><ul><li>大写字母A，B，C … 代表句子中的每个单词.</li></ul><hr><ul><li>torch.narrow演示:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.narrow(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.narrow(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">tensor([[ <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>]])</span><br></pre></td></tr></table></figure><hr><ul><li>接下来我们将使用batchify来处理训练数据，验证数据以及测试数据 <code>P51</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练数据的batch size</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证和测试数据（统称为评估数据）的batch size</span></span><br><span class="line">eval_batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得train_data, val_data, test_data</span></span><br><span class="line">train_data = batchify(train_txt, batch_size)</span><br><span class="line">val_data = batchify(val_txt, eval_batch_size)</span><br><span class="line">test_data = batchify(test_txt, eval_batch_size)</span><br></pre></td></tr></table></figure><hr><ul><li>上面的分割批次并没有进行源数据与目标数据的处理, 接下来我们将根据语言模型训练的语料规定来构建源数据与目标数据.</li></ul><p>语言模型训练的语料规定:</p><ul><li>如果源数据为句子ABCD, ABCD代表句子中的词汇或符号, 则它的目标数据为BCDE, BCDE分别代表ABCD的下一个词汇.</li></ul><img src="https://cdn.jsdelivr.net/gh/beiyoudaxue/pic2023@main/img/image-20231108134624521.png" alt="image-20231108134624521" style="zoom:80%;" /><ul><li>如图所示，我们这里的句子序列是竖着的, 而且我们发现如果用一个批次处理完所有数据, 以训练数据为例, 每个句子长度高达104335, 这明显是不科学的, 因此我们在这里要限定每个批次中的句子<code>长度允许的最大值</code>bptt.</li></ul><hr><ul><li>批次化过程的第二个函数get_batch代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 令子长度允许的最大值bptt为35</span></span><br><span class="line">bptt = <span class="number">35</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">source, i</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于获得每个批次合理大小的源数据和目标数据.</span></span><br><span class="line"><span class="string">       参数source是通过batchify得到的train_data/val_data/test_data.</span></span><br><span class="line"><span class="string">       i是具体的批次次数.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 首先我们确定句子长度, 它将是在bptt和len(source) - 1 - i中最小值</span></span><br><span class="line">    <span class="comment"># 实质上, 前面的批次中都会是bptt的值, 只不过最后一个批次中, 句子长度</span></span><br><span class="line">    <span class="comment"># 可能不够bptt的35个, 因此会变为len(source) - 1 - i的值.</span></span><br><span class="line">    seq_len = <span class="built_in">min</span>(bptt, <span class="built_in">len</span>(source) - <span class="number">1</span> - i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 语言模型训练的源数据的第i批数据将是batchify的结果的切片[i:i+seq_len]</span></span><br><span class="line">    data = source[i:i+seq_len]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据语言模型训练的语料规定, 它的目标数据是源数据向后移动一位</span></span><br><span class="line">    <span class="comment"># 因为最后目标数据的切片会越界, 因此使用view(-1)来保证形状正常.</span></span><br><span class="line">    target = source[i+<span class="number">1</span>:i+<span class="number">1</span>+seq_len].view(-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data, target</span><br></pre></td></tr></table></figure><ul><li>输入实例:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以测试集数据为例</span></span><br><span class="line">source = test_data</span><br><span class="line">i = <span class="number">1</span></span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">data = tensor([[   <span class="number">12</span>,  <span class="number">1053</span>,   <span class="number">355</span>,   <span class="number">134</span>,    <span class="number">37</span>,     <span class="number">7</span>,     <span class="number">4</span>,     <span class="number">0</span>,   <span class="number">835</span>,  <span class="number">9834</span>],</span><br><span class="line">        [  <span class="number">635</span>,     <span class="number">8</span>,     <span class="number">5</span>,     <span class="number">5</span>,   <span class="number">421</span>,     <span class="number">4</span>,    <span class="number">88</span>,     <span class="number">8</span>,   <span class="number">573</span>,  <span class="number">2511</span>],</span><br><span class="line">        [    <span class="number">0</span>,    <span class="number">58</span>,     <span class="number">8</span>,     <span class="number">8</span>,     <span class="number">6</span>,   <span class="number">692</span>,   <span class="number">544</span>,     <span class="number">0</span>,   <span class="number">212</span>,     <span class="number">5</span>],</span><br><span class="line">        [   <span class="number">12</span>,     <span class="number">0</span>,   <span class="number">105</span>,    <span class="number">26</span>,     <span class="number">3</span>,     <span class="number">5</span>,     <span class="number">6</span>,     <span class="number">0</span>,     <span class="number">4</span>,    <span class="number">56</span>],</span><br><span class="line">        [    <span class="number">3</span>, <span class="number">16074</span>, <span class="number">21254</span>,   <span class="number">320</span>,     <span class="number">3</span>,   <span class="number">262</span>,    <span class="number">16</span>,     <span class="number">6</span>,  <span class="number">1087</span>,    <span class="number">89</span>],</span><br><span class="line">        [    <span class="number">3</span>,   <span class="number">751</span>,  <span class="number">3866</span>,    <span class="number">10</span>,    <span class="number">12</span>,    <span class="number">31</span>,   <span class="number">246</span>,   <span class="number">238</span>,    <span class="number">79</span>,    <span class="number">49</span>],</span><br><span class="line">        [  <span class="number">635</span>,   <span class="number">943</span>,    <span class="number">78</span>,    <span class="number">36</span>,    <span class="number">12</span>,   <span class="number">475</span>,    <span class="number">66</span>,    <span class="number">10</span>,     <span class="number">4</span>,   <span class="number">924</span>],</span><br><span class="line">        [    <span class="number">0</span>,  <span class="number">2358</span>,    <span class="number">52</span>,     <span class="number">4</span>,    <span class="number">12</span>,     <span class="number">4</span>,     <span class="number">5</span>,     <span class="number">0</span>, <span class="number">19831</span>,    <span class="number">21</span>],</span><br><span class="line">        [   <span class="number">26</span>,    <span class="number">38</span>,    <span class="number">54</span>,    <span class="number">40</span>,  <span class="number">1589</span>,  <span class="number">3729</span>,  <span class="number">1014</span>,     <span class="number">5</span>,     <span class="number">8</span>,     <span class="number">4</span>],</span><br><span class="line">        [   <span class="number">33</span>, <span class="number">17597</span>,    <span class="number">33</span>,  <span class="number">1661</span>,    <span class="number">15</span>,     <span class="number">7</span>,     <span class="number">5</span>,     <span class="number">0</span>,     <span class="number">4</span>,   <span class="number">170</span>],</span><br><span class="line">        [  <span class="number">335</span>,   <span class="number">268</span>,   <span class="number">117</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">4</span>,  <span class="number">3144</span>,  <span class="number">1557</span>,     <span class="number">0</span>,   <span class="number">160</span>],</span><br><span class="line">        [  <span class="number">106</span>,     <span class="number">4</span>,  <span class="number">4706</span>,  <span class="number">2245</span>,    <span class="number">12</span>,  <span class="number">1074</span>,    <span class="number">13</span>,  <span class="number">2105</span>,     <span class="number">5</span>,    <span class="number">29</span>],</span><br><span class="line">        [    <span class="number">5</span>, <span class="number">16074</span>,    <span class="number">10</span>,  <span class="number">1087</span>,    <span class="number">12</span>,   <span class="number">137</span>,   <span class="number">251</span>, <span class="number">13238</span>,     <span class="number">8</span>,     <span class="number">4</span>],</span><br><span class="line">        [  <span class="number">394</span>,   <span class="number">746</span>,     <span class="number">4</span>,     <span class="number">9</span>,    <span class="number">12</span>,  <span class="number">6032</span>,     <span class="number">4</span>,  <span class="number">2190</span>,   <span class="number">303</span>, <span class="number">12651</span>],</span><br><span class="line">        [    <span class="number">8</span>,   <span class="number">616</span>,  <span class="number">2107</span>,     <span class="number">4</span>,     <span class="number">3</span>,     <span class="number">4</span>,   <span class="number">425</span>,     <span class="number">0</span>,    <span class="number">10</span>,   <span class="number">510</span>],</span><br><span class="line">        [ <span class="number">1339</span>,   <span class="number">112</span>,    <span class="number">23</span>,   <span class="number">335</span>,     <span class="number">3</span>, <span class="number">22251</span>,  <span class="number">1162</span>,     <span class="number">9</span>,    <span class="number">11</span>,     <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">1212</span>,   <span class="number">468</span>,     <span class="number">6</span>,   <span class="number">820</span>,     <span class="number">9</span>,     <span class="number">7</span>,  <span class="number">1231</span>,  <span class="number">4202</span>,  <span class="number">2866</span>,   <span class="number">382</span>],</span><br><span class="line">        [    <span class="number">6</span>,    <span class="number">24</span>,   <span class="number">104</span>,     <span class="number">6</span>,     <span class="number">4</span>,     <span class="number">4</span>,     <span class="number">7</span>,    <span class="number">10</span>,     <span class="number">9</span>,   <span class="number">588</span>],</span><br><span class="line">        [   <span class="number">31</span>,   <span class="number">190</span>,     <span class="number">0</span>,     <span class="number">0</span>,   <span class="number">230</span>,   <span class="number">267</span>,     <span class="number">4</span>,   <span class="number">273</span>,   <span class="number">278</span>,     <span class="number">6</span>],</span><br><span class="line">        [   <span class="number">34</span>,    <span class="number">25</span>,    <span class="number">47</span>,    <span class="number">26</span>,  <span class="number">1864</span>,     <span class="number">6</span>,   <span class="number">694</span>,     <span class="number">0</span>,  <span class="number">2112</span>,     <span class="number">3</span>],</span><br><span class="line">        [   <span class="number">11</span>,     <span class="number">6</span>,    <span class="number">52</span>,   <span class="number">798</span>,     <span class="number">8</span>,    <span class="number">69</span>,    <span class="number">20</span>,    <span class="number">31</span>,    <span class="number">63</span>,     <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">1800</span>,    <span class="number">25</span>,  <span class="number">2141</span>,  <span class="number">2442</span>,   <span class="number">117</span>,    <span class="number">31</span>,   <span class="number">196</span>,  <span class="number">7290</span>,     <span class="number">4</span>,   <span class="number">298</span>],</span><br><span class="line">        [   <span class="number">15</span>,   <span class="number">171</span>,    <span class="number">15</span>,    <span class="number">17</span>,  <span class="number">1712</span>,    <span class="number">13</span>,   <span class="number">217</span>,    <span class="number">59</span>,   <span class="number">736</span>,     <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">4210</span>,   <span class="number">191</span>,   <span class="number">142</span>,    <span class="number">14</span>,  <span class="number">5251</span>,   <span class="number">939</span>,    <span class="number">59</span>,    <span class="number">38</span>, <span class="number">10055</span>, <span class="number">25132</span>],</span><br><span class="line">        [  <span class="number">302</span>,    <span class="number">23</span>, <span class="number">11718</span>,    <span class="number">11</span>,    <span class="number">11</span>,   <span class="number">599</span>,   <span class="number">382</span>,   <span class="number">317</span>,     <span class="number">8</span>,    <span class="number">13</span>],</span><br><span class="line">        [   <span class="number">16</span>,  <span class="number">1564</span>,     <span class="number">9</span>,  <span class="number">4808</span>,     <span class="number">6</span>,     <span class="number">0</span>,     <span class="number">6</span>,     <span class="number">6</span>,     <span class="number">4</span>,     <span class="number">4</span>],</span><br><span class="line">        [    <span class="number">4</span>,     <span class="number">7</span>,    <span class="number">39</span>,     <span class="number">7</span>,  <span class="number">3934</span>,     <span class="number">5</span>,     <span class="number">9</span>,     <span class="number">3</span>,  <span class="number">8047</span>,   <span class="number">557</span>],</span><br><span class="line">        [  <span class="number">394</span>,     <span class="number">0</span>, <span class="number">10715</span>,  <span class="number">3580</span>,  <span class="number">8682</span>,    <span class="number">31</span>,   <span class="number">242</span>,     <span class="number">0</span>, <span class="number">10055</span>,   <span class="number">170</span>],</span><br><span class="line">        [   <span class="number">96</span>,     <span class="number">6</span>,   <span class="number">144</span>,  <span class="number">3403</span>,     <span class="number">4</span>,    <span class="number">13</span>,  <span class="number">1014</span>,    <span class="number">14</span>,     <span class="number">6</span>,  <span class="number">2395</span>],</span><br><span class="line">        [    <span class="number">4</span>,     <span class="number">3</span>, <span class="number">13729</span>,    <span class="number">14</span>,    <span class="number">40</span>,     <span class="number">0</span>,     <span class="number">5</span>,    <span class="number">18</span>,   <span class="number">676</span>,  <span class="number">3267</span>],</span><br><span class="line">        [ <span class="number">1031</span>,     <span class="number">3</span>,     <span class="number">0</span>,   <span class="number">628</span>,  <span class="number">1589</span>,    <span class="number">22</span>, <span class="number">10916</span>, <span class="number">10969</span>,     <span class="number">5</span>, <span class="number">22548</span>],</span><br><span class="line">        [    <span class="number">9</span>,    <span class="number">12</span>,     <span class="number">6</span>,    <span class="number">84</span>,    <span class="number">15</span>,    <span class="number">49</span>,  <span class="number">3144</span>,     <span class="number">7</span>,   <span class="number">102</span>,    <span class="number">15</span>],</span><br><span class="line">        [  <span class="number">916</span>,    <span class="number">12</span>,     <span class="number">4</span>,   <span class="number">203</span>,     <span class="number">0</span>,   <span class="number">273</span>,   <span class="number">303</span>,   <span class="number">333</span>,  <span class="number">4318</span>,     <span class="number">0</span>],</span><br><span class="line">        [    <span class="number">6</span>,    <span class="number">12</span>,     <span class="number">0</span>,  <span class="number">4842</span>,     <span class="number">5</span>,    <span class="number">17</span>,     <span class="number">4</span>,    <span class="number">47</span>,  <span class="number">4138</span>,  <span class="number">2072</span>],</span><br><span class="line">        [   <span class="number">38</span>,   <span class="number">237</span>,     <span class="number">5</span>,    <span class="number">50</span>,    <span class="number">35</span>,    <span class="number">27</span>, <span class="number">18530</span>,   <span class="number">244</span>,    <span class="number">20</span>,     <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">target =  tensor([  <span class="number">635</span>,     <span class="number">8</span>,     <span class="number">5</span>,     <span class="number">5</span>,   <span class="number">421</span>,     <span class="number">4</span>,    <span class="number">88</span>,     <span class="number">8</span>,   <span class="number">573</span>,  <span class="number">2511</span>,</span><br><span class="line">            <span class="number">0</span>,    <span class="number">58</span>,     <span class="number">8</span>,     <span class="number">8</span>,     <span class="number">6</span>,   <span class="number">692</span>,   <span class="number">544</span>,     <span class="number">0</span>,   <span class="number">212</span>,     <span class="number">5</span>,</span><br><span class="line">           <span class="number">12</span>,     <span class="number">0</span>,   <span class="number">105</span>,    <span class="number">26</span>,     <span class="number">3</span>,     <span class="number">5</span>,     <span class="number">6</span>,     <span class="number">0</span>,     <span class="number">4</span>,    <span class="number">56</span>,</span><br><span class="line">            <span class="number">3</span>, <span class="number">16074</span>, <span class="number">21254</span>,   <span class="number">320</span>,     <span class="number">3</span>,   <span class="number">262</span>,    <span class="number">16</span>,     <span class="number">6</span>,  <span class="number">1087</span>,    <span class="number">89</span>,</span><br><span class="line">            <span class="number">3</span>,   <span class="number">751</span>,  <span class="number">3866</span>,    <span class="number">10</span>,    <span class="number">12</span>,    <span class="number">31</span>,   <span class="number">246</span>,   <span class="number">238</span>,    <span class="number">79</span>,    <span class="number">49</span>,</span><br><span class="line">          <span class="number">635</span>,   <span class="number">943</span>,    <span class="number">78</span>,    <span class="number">36</span>,    <span class="number">12</span>,   <span class="number">475</span>,    <span class="number">66</span>,    <span class="number">10</span>,     <span class="number">4</span>,   <span class="number">924</span>,</span><br><span class="line">            <span class="number">0</span>,  <span class="number">2358</span>,    <span class="number">52</span>,     <span class="number">4</span>,    <span class="number">12</span>,     <span class="number">4</span>,     <span class="number">5</span>,     <span class="number">0</span>, <span class="number">19831</span>,    <span class="number">21</span>,</span><br><span class="line">           <span class="number">26</span>,    <span class="number">38</span>,    <span class="number">54</span>,    <span class="number">40</span>,  <span class="number">1589</span>,  <span class="number">3729</span>,  <span class="number">1014</span>,     <span class="number">5</span>,     <span class="number">8</span>,     <span class="number">4</span>,</span><br><span class="line">           <span class="number">33</span>, <span class="number">17597</span>,    <span class="number">33</span>,  <span class="number">1661</span>,    <span class="number">15</span>,     <span class="number">7</span>,     <span class="number">5</span>,     <span class="number">0</span>,     <span class="number">4</span>,   <span class="number">170</span>,</span><br><span class="line">          <span class="number">335</span>,   <span class="number">268</span>,   <span class="number">117</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">4</span>,  <span class="number">3144</span>,  <span class="number">1557</span>,     <span class="number">0</span>,   <span class="number">160</span>,</span><br><span class="line">          <span class="number">106</span>,     <span class="number">4</span>,  <span class="number">4706</span>,  <span class="number">2245</span>,    <span class="number">12</span>,  <span class="number">1074</span>,    <span class="number">13</span>,  <span class="number">2105</span>,     <span class="number">5</span>,    <span class="number">29</span>,</span><br><span class="line">            <span class="number">5</span>, <span class="number">16074</span>,    <span class="number">10</span>,  <span class="number">1087</span>,    <span class="number">12</span>,   <span class="number">137</span>,   <span class="number">251</span>, <span class="number">13238</span>,     <span class="number">8</span>,     <span class="number">4</span>,</span><br><span class="line">          <span class="number">394</span>,   <span class="number">746</span>,     <span class="number">4</span>,     <span class="number">9</span>,    <span class="number">12</span>,  <span class="number">6032</span>,     <span class="number">4</span>,  <span class="number">2190</span>,   <span class="number">303</span>, <span class="number">12651</span>,</span><br><span class="line">            <span class="number">8</span>,   <span class="number">616</span>,  <span class="number">2107</span>,     <span class="number">4</span>,     <span class="number">3</span>,     <span class="number">4</span>,   <span class="number">425</span>,     <span class="number">0</span>,    <span class="number">10</span>,   <span class="number">510</span>,</span><br><span class="line">         <span class="number">1339</span>,   <span class="number">112</span>,    <span class="number">23</span>,   <span class="number">335</span>,     <span class="number">3</span>, <span class="number">22251</span>,  <span class="number">1162</span>,     <span class="number">9</span>,    <span class="number">11</span>,     <span class="number">9</span>,</span><br><span class="line">         <span class="number">1212</span>,   <span class="number">468</span>,     <span class="number">6</span>,   <span class="number">820</span>,     <span class="number">9</span>,     <span class="number">7</span>,  <span class="number">1231</span>,  <span class="number">4202</span>,  <span class="number">2866</span>,   <span class="number">382</span>,</span><br><span class="line">            <span class="number">6</span>,    <span class="number">24</span>,   <span class="number">104</span>,     <span class="number">6</span>,     <span class="number">4</span>,     <span class="number">4</span>,     <span class="number">7</span>,    <span class="number">10</span>,     <span class="number">9</span>,   <span class="number">588</span>,</span><br><span class="line">           <span class="number">31</span>,   <span class="number">190</span>,     <span class="number">0</span>,     <span class="number">0</span>,   <span class="number">230</span>,   <span class="number">267</span>,     <span class="number">4</span>,   <span class="number">273</span>,   <span class="number">278</span>,     <span class="number">6</span>,</span><br><span class="line">           <span class="number">34</span>,    <span class="number">25</span>,    <span class="number">47</span>,    <span class="number">26</span>,  <span class="number">1864</span>,     <span class="number">6</span>,   <span class="number">694</span>,     <span class="number">0</span>,  <span class="number">2112</span>,     <span class="number">3</span>,</span><br><span class="line">           <span class="number">11</span>,     <span class="number">6</span>,    <span class="number">52</span>,   <span class="number">798</span>,     <span class="number">8</span>,    <span class="number">69</span>,    <span class="number">20</span>,    <span class="number">31</span>,    <span class="number">63</span>,     <span class="number">9</span>,</span><br><span class="line">         <span class="number">1800</span>,    <span class="number">25</span>,  <span class="number">2141</span>,  <span class="number">2442</span>,   <span class="number">117</span>,    <span class="number">31</span>,   <span class="number">196</span>,  <span class="number">7290</span>,     <span class="number">4</span>,   <span class="number">298</span>,</span><br><span class="line">           <span class="number">15</span>,   <span class="number">171</span>,    <span class="number">15</span>,    <span class="number">17</span>,  <span class="number">1712</span>,    <span class="number">13</span>,   <span class="number">217</span>,    <span class="number">59</span>,   <span class="number">736</span>,     <span class="number">5</span>,</span><br><span class="line">         <span class="number">4210</span>,   <span class="number">191</span>,   <span class="number">142</span>,    <span class="number">14</span>,  <span class="number">5251</span>,   <span class="number">939</span>,    <span class="number">59</span>,    <span class="number">38</span>, <span class="number">10055</span>, <span class="number">25132</span>,</span><br><span class="line">          <span class="number">302</span>,    <span class="number">23</span>, <span class="number">11718</span>,    <span class="number">11</span>,    <span class="number">11</span>,   <span class="number">599</span>,   <span class="number">382</span>,   <span class="number">317</span>,     <span class="number">8</span>,    <span class="number">13</span>,</span><br><span class="line">           <span class="number">16</span>,  <span class="number">1564</span>,     <span class="number">9</span>,  <span class="number">4808</span>,     <span class="number">6</span>,     <span class="number">0</span>,     <span class="number">6</span>,     <span class="number">6</span>,     <span class="number">4</span>,     <span class="number">4</span>,</span><br><span class="line">            <span class="number">4</span>,     <span class="number">7</span>,    <span class="number">39</span>,     <span class="number">7</span>,  <span class="number">3934</span>,     <span class="number">5</span>,     <span class="number">9</span>,     <span class="number">3</span>,  <span class="number">8047</span>,   <span class="number">557</span>,</span><br><span class="line">          <span class="number">394</span>,     <span class="number">0</span>, <span class="number">10715</span>,  <span class="number">3580</span>,  <span class="number">8682</span>,    <span class="number">31</span>,   <span class="number">242</span>,     <span class="number">0</span>, <span class="number">10055</span>,   <span class="number">170</span>,</span><br><span class="line">           <span class="number">96</span>,     <span class="number">6</span>,   <span class="number">144</span>,  <span class="number">3403</span>,     <span class="number">4</span>,    <span class="number">13</span>,  <span class="number">1014</span>,    <span class="number">14</span>,     <span class="number">6</span>,  <span class="number">2395</span>,</span><br><span class="line">            <span class="number">4</span>,     <span class="number">3</span>, <span class="number">13729</span>,    <span class="number">14</span>,    <span class="number">40</span>,     <span class="number">0</span>,     <span class="number">5</span>,    <span class="number">18</span>,   <span class="number">676</span>,  <span class="number">3267</span>,</span><br><span class="line">         <span class="number">1031</span>,     <span class="number">3</span>,     <span class="number">0</span>,   <span class="number">628</span>,  <span class="number">1589</span>,    <span class="number">22</span>, <span class="number">10916</span>, <span class="number">10969</span>,     <span class="number">5</span>, <span class="number">22548</span>,</span><br><span class="line">            <span class="number">9</span>,    <span class="number">12</span>,     <span class="number">6</span>,    <span class="number">84</span>,    <span class="number">15</span>,    <span class="number">49</span>,  <span class="number">3144</span>,     <span class="number">7</span>,   <span class="number">102</span>,    <span class="number">15</span>,</span><br><span class="line">          <span class="number">916</span>,    <span class="number">12</span>,     <span class="number">4</span>,   <span class="number">203</span>,     <span class="number">0</span>,   <span class="number">273</span>,   <span class="number">303</span>,   <span class="number">333</span>,  <span class="number">4318</span>,     <span class="number">0</span>,</span><br><span class="line">            <span class="number">6</span>,    <span class="number">12</span>,     <span class="number">0</span>,  <span class="number">4842</span>,     <span class="number">5</span>,    <span class="number">17</span>,     <span class="number">4</span>,    <span class="number">47</span>,  <span class="number">4138</span>,  <span class="number">2072</span>,</span><br><span class="line">           <span class="number">38</span>,   <span class="number">237</span>,     <span class="number">5</span>,    <span class="number">50</span>,    <span class="number">35</span>,    <span class="number">27</span>, <span class="number">18530</span>,   <span class="number">244</span>,    <span class="number">20</span>,     <span class="number">6</span>,</span><br><span class="line">           <span class="number">13</span>,  <span class="number">1083</span>,    <span class="number">35</span>,  <span class="number">1990</span>,   <span class="number">653</span>,    <span class="number">13</span>,    <span class="number">10</span>,    <span class="number">11</span>,  <span class="number">1538</span>,    <span class="number">56</span>])</span><br></pre></td></tr></table></figure><hr><h4 id="第四步-构建训练和评估函数-P52"><a href="#第四步-构建训练和评估函数-P52" class="headerlink" title="第四步: 构建训练和评估函数 P52"></a>第四步: 构建训练和评估函数 P52</h4><ul><li>设置模型超参数和初始化模型</li><li>设置模型超参数和初始化模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过TEXT.vocab.stoi方法获得不重复词汇总数</span></span><br><span class="line">ntokens = <span class="built_in">len</span>(TEXT.vocab.stoi)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 词嵌入大小为200</span></span><br><span class="line">emsize = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈全连接层的节点数</span></span><br><span class="line">nhid = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码器层的数量</span></span><br><span class="line">nlayers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头注意力机制的头数</span></span><br><span class="line">nhead = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 置0比率</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将参数输入到TransformerModel中</span></span><br><span class="line">model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型初始化后, 接下来进行损失函数和优化方法的选择.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关于损失函数, 我们使用nn自带的交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率初始值定为5.0</span></span><br><span class="line">lr = <span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器选择torch自带的SGD随机梯度下降方法, 并把lr传入其中</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义学习率调整方法, 使用torch自带的lr_scheduler, 将优化器传入其中.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="number">1.0</span>, gamma=<span class="number">0.95</span>)</span><br></pre></td></tr></table></figure><hr><ul><li>模型训练代码分析: P53</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入时间工具包</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 模型开启训练模式</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 定义初始损失为0</span></span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="comment"># 获得当前时间</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="comment"># 开始遍历批次数据</span></span><br><span class="line">    <span class="keyword">for</span> batch, i <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(<span class="number">0</span>, train_data.size(<span class="number">0</span>) - <span class="number">1</span>, bptt)):</span><br><span class="line">        <span class="comment"># 通过get_batch获得源数据和目标数据</span></span><br><span class="line">        data, targets = get_batch(train_data, i)</span><br><span class="line">        <span class="comment"># 设置优化器初始梯度为0梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 将数据装入model得到输出</span></span><br><span class="line">        output = model(data)</span><br><span class="line">        <span class="comment"># 将输出和目标数据传入损失函数对象</span></span><br><span class="line">        loss = criterion(output.view(-<span class="number">1</span>, ntokens), targets)</span><br><span class="line">        <span class="comment"># 损失进行反向传播以获得总的损失</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 使用nn自带的clip_grad_norm_方法进行梯度规范化, 防止出现梯度消失或爆炸</span></span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        <span class="comment"># 模型参数进行更新</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 将每层的损失相加获得总的损失</span></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="comment"># 日志打印间隔定为200</span></span><br><span class="line">        log_interval = <span class="number">200</span></span><br><span class="line">        <span class="comment"># 如果batch是200的倍数且大于0，则打印相关日志</span></span><br><span class="line">        <span class="keyword">if</span> batch % log_interval == <span class="number">0</span> <span class="keyword">and</span> batch &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 平均损失为总损失除以log_interval</span></span><br><span class="line">            cur_loss = total_loss / log_interval</span><br><span class="line">            <span class="comment"># 需要的时间为当前时间减去开始时间</span></span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            <span class="comment"># 打印轮数, 当前批次和总批次, 当前学习率, 训练速度(每豪秒处理多少批次),</span></span><br><span class="line">            <span class="comment"># 平均损失, 以及困惑度, 困惑度是衡量语言模型的重要指标, 它的计算方法就是</span></span><br><span class="line">            <span class="comment"># 对交叉熵平均损失取自然对数的底数.</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;| epoch &#123;:3d&#125; | &#123;:5d&#125;/&#123;:5d&#125; batches | &#x27;</span></span><br><span class="line">                  <span class="string">&#x27;lr &#123;:02.2f&#125; | ms/batch &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">                  <span class="string">&#x27;loss &#123;:5.2f&#125; | ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch, batch, <span class="built_in">len</span>(train_data) // bptt, scheduler.get_lr()[<span class="number">0</span>],</span><br><span class="line">                    elapsed * <span class="number">1000</span> / log_interval,</span><br><span class="line">                    cur_loss, math.exp(cur_loss)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每个批次结束后, 总损失归0</span></span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 开始时间取当前时间</span></span><br><span class="line">            start_time = time.time()</span><br></pre></td></tr></table></figure><ul><li>模型评估代码分析: <code>P54</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">eval_model, data_source</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估函数, 评估阶段包括验证和测试,</span></span><br><span class="line"><span class="string">       它的两个参数eval_model为每轮训练产生的模型</span></span><br><span class="line"><span class="string">       data_source代表验证或测试数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 模型开启评估模式</span></span><br><span class="line">    eval_model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 总损失归0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 因为评估模式模型参数不变, 因此反向传播不需要求导, 以加快计算</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 与训练过程相同, 但是因为过程不需要打印信息, 因此不需要batch数</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data_source.size(<span class="number">0</span>) - <span class="number">1</span>, bptt):</span><br><span class="line">            <span class="comment"># 首先还是通过通过get_batch获得验证数据集的源数据和目标数据</span></span><br><span class="line">            data, targets = get_batch(data_source, i)</span><br><span class="line">            <span class="comment"># 通过eval_model获得输出</span></span><br><span class="line">            output = eval_model(data)</span><br><span class="line">            <span class="comment"># 对输出形状扁平化, 变为全部词汇的概率分布</span></span><br><span class="line">            output_flat = output.view(-<span class="number">1</span>, ntokens)</span><br><span class="line">            <span class="comment"># 获得评估过程的总损失</span></span><br><span class="line">            total_loss += criterion(output_flat, targets).item()</span><br><span class="line">            <span class="comment"># 计算平均损失</span></span><br><span class="line">            cur_loss = total_loss / ((data_source.size(<span class="number">0</span>) - <span class="number">1</span>) / bptt)            </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回平均损失</span></span><br><span class="line">    <span class="keyword">return</span> cur_loss</span><br></pre></td></tr></table></figure><hr><h4 id="第五步-进行训练和评估-包括验证以及测试-P55"><a href="#第五步-进行训练和评估-包括验证以及测试-P55" class="headerlink" title="第五步: 进行训练和评估(包括验证以及测试) P55"></a>第五步: 进行训练和评估(包括验证以及测试) P55</h4><ul><li>模型的训练与验证代码分析:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先初始化最佳验证损失，初始值为无穷大</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">best_val_loss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练轮数</span></span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义最佳模型变量, 初始值为None</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用for循环遍历轮数</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 首先获得轮数开始时间</span></span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    <span class="comment"># 调用训练函数</span></span><br><span class="line">    train()</span><br><span class="line">    <span class="comment"># 该轮训练后我们的模型参数已经发生了变化</span></span><br><span class="line">    <span class="comment"># 将模型和评估数据传入到评估函数中</span></span><br><span class="line">    val_loss = evaluate(model, val_data)</span><br><span class="line">    <span class="comment"># 之后打印每轮的评估日志，分别有轮数，耗时，验证损失以及验证困惑度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;| end of epoch &#123;:3d&#125; | time: &#123;:5.2f&#125;s | valid loss &#123;:5.2f&#125; | &#x27;</span></span><br><span class="line">          <span class="string">&#x27;valid ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, (time.time() - epoch_start_time),</span><br><span class="line">                                     val_loss, math.exp(val_loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">89</span>)</span><br><span class="line">    <span class="comment"># 我们将比较哪一轮损失最小，赋值给best_val_loss，</span></span><br><span class="line">    <span class="comment"># 并取该损失下的模型为best_model</span></span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">        best_val_loss = val_loss</span><br><span class="line">        <span class="comment"># 使用深拷贝，拷贝最优模型</span></span><br><span class="line">        best_model = copy.deepcopy(model)</span><br><span class="line">    <span class="comment"># 每轮都会对优化方法的学习率做调整</span></span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">| epoch   <span class="number">1</span> |   <span class="number">200</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">30.03</span> | loss  <span class="number">7.68</span> | ppl  <span class="number">2158.52</span></span><br><span class="line">| epoch   <span class="number">1</span> |   <span class="number">400</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.90</span> | loss  <span class="number">5.26</span> | ppl   <span class="number">193.39</span></span><br><span class="line">| epoch   <span class="number">1</span> |   <span class="number">600</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.90</span> | loss  <span class="number">4.07</span> | ppl    <span class="number">58.44</span></span><br><span class="line">| epoch   <span class="number">1</span> |   <span class="number">800</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.88</span> | loss  <span class="number">3.41</span> | ppl    <span class="number">30.26</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">1000</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.89</span> | loss  <span class="number">2.98</span> | ppl    <span class="number">19.72</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">1200</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.90</span> | loss  <span class="number">2.79</span> | ppl    <span class="number">16.30</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">1400</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.91</span> | loss  <span class="number">2.67</span> | ppl    <span class="number">14.38</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">1600</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.92</span> | loss  <span class="number">2.58</span> | ppl    <span class="number">13.19</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">1800</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.91</span> | loss  <span class="number">2.43</span> | ppl    <span class="number">11.32</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">2000</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.92</span> | loss  <span class="number">2.39</span> | ppl    <span class="number">10.93</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">2200</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.91</span> | loss  <span class="number">2.33</span> | ppl    <span class="number">10.24</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">2400</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.91</span> | loss  <span class="number">2.36</span> | ppl    <span class="number">10.59</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">2600</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.90</span> | loss  <span class="number">2.33</span> | ppl    <span class="number">10.31</span></span><br><span class="line">| epoch   <span class="number">1</span> |  <span class="number">2800</span>/ <span class="number">2981</span> batches | lr <span class="number">5.00</span> | ms/batch <span class="number">28.92</span> | loss  <span class="number">2.26</span> | ppl     <span class="number">9.54</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   <span class="number">1</span> | time: <span class="number">90.01</span>s | valid loss  <span class="number">1.32</span> | valid ppl     <span class="number">3.73</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| epoch   <span class="number">2</span> |   <span class="number">200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">29.08</span> | loss  <span class="number">2.18</span> | ppl     <span class="number">8.83</span></span><br><span class="line">| epoch   <span class="number">2</span> |   <span class="number">400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">2.11</span> | ppl     <span class="number">8.24</span></span><br><span class="line">| epoch   <span class="number">2</span> |   <span class="number">600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">1.98</span> | ppl     <span class="number">7.23</span></span><br><span class="line">| epoch   <span class="number">2</span> |   <span class="number">800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">2.00</span> | ppl     <span class="number">7.39</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">1000</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.94</span> | loss  <span class="number">1.94</span> | ppl     <span class="number">6.96</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">1200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.92</span> | loss  <span class="number">1.97</span> | ppl     <span class="number">7.15</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">1400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.94</span> | loss  <span class="number">1.98</span> | ppl     <span class="number">7.28</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">1600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.92</span> | loss  <span class="number">1.97</span> | ppl     <span class="number">7.16</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">1800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">1.92</span> | ppl     <span class="number">6.84</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">2000</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">1.96</span> | ppl     <span class="number">7.11</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">2200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.93</span> | loss  <span class="number">1.92</span> | ppl     <span class="number">6.80</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">2400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.94</span> | loss  <span class="number">1.94</span> | ppl     <span class="number">6.93</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">2600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.76</span> | loss  <span class="number">1.91</span> | ppl     <span class="number">6.76</span></span><br><span class="line">| epoch   <span class="number">2</span> |  <span class="number">2800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.75</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.89</span> | ppl     <span class="number">6.64</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   <span class="number">2</span> | time: <span class="number">89.71</span>s | valid loss  <span class="number">1.01</span> | valid ppl     <span class="number">2.74</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| epoch   <span class="number">3</span> |   <span class="number">200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.88</span> | loss  <span class="number">1.78</span> | ppl     <span class="number">5.96</span></span><br><span class="line">| epoch   <span class="number">3</span> |   <span class="number">400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.89</span> | ppl     <span class="number">6.59</span></span><br><span class="line">| epoch   <span class="number">3</span> |   <span class="number">600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.72</span> | ppl     <span class="number">5.58</span></span><br><span class="line">| epoch   <span class="number">3</span> |   <span class="number">800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.73</span> | ppl     <span class="number">5.63</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">1000</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.73</span> | loss  <span class="number">1.65</span> | ppl     <span class="number">5.22</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">1200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.74</span> | loss  <span class="number">1.69</span> | ppl     <span class="number">5.40</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">1400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.74</span> | loss  <span class="number">1.73</span> | ppl     <span class="number">5.66</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">1600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.75</span> | ppl     <span class="number">5.73</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">1800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.74</span> | loss  <span class="number">1.67</span> | ppl     <span class="number">5.33</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">2000</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.74</span> | loss  <span class="number">1.69</span> | ppl     <span class="number">5.41</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">2200</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.74</span> | loss  <span class="number">1.66</span> | ppl     <span class="number">5.26</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">2400</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.76</span> | loss  <span class="number">1.69</span> | ppl     <span class="number">5.43</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">2600</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.71</span> | ppl     <span class="number">5.55</span></span><br><span class="line">| epoch   <span class="number">3</span> |  <span class="number">2800</span>/ <span class="number">2981</span> batches | lr <span class="number">4.51</span> | ms/batch <span class="number">28.75</span> | loss  <span class="number">1.72</span> | ppl     <span class="number">5.58</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br><span class="line">| end of epoch   <span class="number">3</span> | time: <span class="number">89.26</span>s | valid loss  <span class="number">0.85</span> | valid ppl     <span class="number">2.33</span></span><br><span class="line">-----------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><hr><ul><li>模型测试代码分析: <code>P56</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们仍然使用evaluate函数，这次它的参数是best_model以及测试数据</span></span><br><span class="line">test_loss = evaluate(best_model, test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印测试日志，包括测试损失和测试困惑度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;| End of training | test loss &#123;:5.2f&#125; | test ppl &#123;:8.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">    test_loss, math.exp(test_loss)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">89</span>)</span><br></pre></td></tr></table></figure><hr><ul><li>输出效果:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">=========================================================================================</span><br><span class="line">| End of training | test loss  <span class="number">0.83</span> | test ppl     <span class="number">2.30</span></span><br><span class="line">=========================================================================================</span><br></pre></td></tr></table></figure><hr><h3 id="小节总结"><a href="#小节总结" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了什么是语言模型:<ul><li>以一个符合语言规律的序列为输入，模型将利用序列间关系等特征，输出一个在所有词汇上的概率分布.这样的模型称为语言模型.</li></ul></li></ul><hr><ul><li>学习了语言模型能解决哪些问题:<ul><li>1, 根据语言模型的定义，可以在它的基础上完成机器翻译，文本生成等任务，因为我们通过最后输出的概率分布来预测下一个词汇是什么.</li><li>2, 语言模型可以判断输入的序列是否为一句完整的话，因为我们可以根据输出的概率分布查看最大概率是否落在句子结束符上，来判断完整性.</li><li>3, 语言模型本身的训练目标是预测下一个词，因为它的特征提取部分会抽象很多语言序列之间的关系，这些关系可能同样对其他语言类任务有效果.因此可以作为预训练模型进行迁移学习.</li></ul></li></ul><hr><ul><li>学习并实现了整个案例的五个步骤:<ul><li>第一步: 导入必备的工具包</li><li>第二步: 导入wikiText-2数据集并作基本处理</li><li>第三步: 构建用于模型输入的批次化数据</li><li>第四步: 构建训练和评估函数</li><li>第五步: 进行训练和评估(包括验证以及测试)</li></ul></li></ul><hr><ul><li>第一步: 导入必备的工具包<ul><li>torchtext介绍: 它是torch工具中处理NLP问题的常用数据处理包.</li><li>对文本数据进行处理, 比如文本语料加载, 文本迭代器构建等.</li><li>包含很多经典文本语料的预加载方法. 其中包括的语料有：用于情感分析的SST和IMDB, 用于问题分类的TREC, 用于及其翻译的 WMT14， IWSLT，以及用于语言模型任务wikiText-2, WikiText103, PennTreebank.</li><li>wikiText-2数据集的体量中等, 训练集共有600篇短文, 共208万左右的词汇, 33278个不重复词汇, OvV（有多少正常英文词汇不在该数据集中的占比）为2.6%，数据集中的短文都是维基百科中对一些概念的介绍和描述.</li></ul></li></ul><hr><ul><li>第二步: 导入wikiText-2数据集并作基本处理<ul><li>通过torchtext中的方法获得了train_txt, val_txt, test_txt.</li></ul></li></ul><hr><ul><li>第三步: 构建用于模型输入的批次化数据<ul><li>实现了批次化过程的第一个函数batchify, 用于将文本数据映射成连续数字, 并转换成指定的样式.</li><li>实现了批次化过程的第二个函数get_batch, 用于获得每个批次合理大小的源数据和目标数据.</li></ul></li></ul><hr><ul><li>第四步: 构建训练和评估函数<ul><li>构建了用于训练的函数train()</li><li>构建了用于评估的函数evaluate()</li></ul></li></ul><hr><ul><li>第五步: 进行训练和评估(包括验证以及测试)<ul><li>首先实现了模型训练与验证过程, 并打印了结果.</li><li>最后实现了模型的测试过程, 得到了不错的困惑度指标.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MATLAB代码</title>
      <link href="/2024/04/34872.html"/>
      <url>/2024/04/34872.html</url>
      
        <content type="html"><![CDATA[<h1 id="MATLAB代码"><a href="#MATLAB代码" class="headerlink" title="MATLAB代码"></a>MATLAB代码</h1><h2 id="MATLAB-绘制-2D-3D-图片"><a href="#MATLAB-绘制-2D-3D-图片" class="headerlink" title="MATLAB 绘制 2D&#x2F;3D 图片"></a>MATLAB 绘制 2D&#x2F;3D 图片</h2><p><code>plot_func(dimension,x, y, z,  fontsize, linewidth, x_dis, y_dis, width, height, x_label, y_label, title1)</code></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Y</span> = <span class="title">plot_func</span><span class="params">(dimension,x, y, z,  fontsize, linewidth, x_dis, y_dis, width, height, x_label, y_label, title1)</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dimension == <span class="number">3</span></span><br><span class="line">        <span class="built_in">figure</span>(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>], <span class="string">&#x27;position&#x27;</span>, [x_dis, y_dis, width, height]); <span class="comment">% [位置横纵 宽 高]</span></span><br><span class="line">        h = pcolor(x, y, z); <span class="comment">% 绘制图像，并返回句柄</span></span><br><span class="line">        set(h, <span class="string">&#x27;EdgeColor&#x27;</span>, <span class="string">&#x27;none&#x27;</span>);</span><br><span class="line">        axis tight; <span class="comment">% 自动调整坐标轴的范围，使其紧贴着数据的最小值和最大值</span></span><br><span class="line">        colorbar;colormap jet;</span><br><span class="line">        set(gca, <span class="string">&#x27;TickDir&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;Box&#x27;</span>, <span class="string">&#x27;off&#x27;</span>, <span class="string">&#x27;LineWidth&#x27;</span>, linewidth);</span><br><span class="line">        set(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize); <span class="comment">% Times New Roman</span></span><br><span class="line">        xlabel(x_label,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">        ylabel(y_label,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">        title(title1,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Monospaced&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dimension == <span class="number">2</span></span><br><span class="line">        <span class="built_in">figure</span>(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>], <span class="string">&#x27;position&#x27;</span>, [x_dis, y_dis, width, height]); <span class="comment">% [位置横纵 宽 高]</span></span><br><span class="line">        <span class="built_in">plot</span>([<span class="number">0</span> <span class="number">0</span>],[<span class="number">0</span> <span class="number">0</span>],<span class="string">&#x27;r&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">1.5</span>);<span class="built_in">hold</span> on;</span><br><span class="line">        set(gca,<span class="string">&#x27;FontSize&#x27;</span>,fontsize,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>, <span class="string">&#x27;LineWidth&#x27;</span>, linewidth);</span><br><span class="line">        xlabel(x_label,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">        ylabel(y_label,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">        title(title1,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Monospaced&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,fontsize);</span><br><span class="line">        axis tight;</span><br><span class="line">        <span class="keyword">for</span> ix = <span class="number">1</span>:<span class="built_in">length</span>(y(<span class="number">1</span>,:))</span><br><span class="line">            <span class="built_in">plot</span>(x, y(:,ix),  <span class="string">&#x27;Color&#x27;</span>, <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">3</span>), <span class="string">&#x27;LineWidth&#x27;</span>, linewidth);<span class="built_in">hold</span> on;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        xlim([<span class="built_in">min</span>(x), <span class="built_in">max</span>(x)]);</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><img src="https://cdn.staticaly.com/gh/yangmulao/blogcdn@master/img/image-20230405221005646.png" alt="image-20230405221005646" style="zoom:50%;" /><img src="https://cdn.staticaly.com/gh/yangmulao/blogcdn@master/img/image-20230405220929573.png" alt="image-20230405220929573" style="zoom:50%;" /><h2 id="Comsol-数据导出成矩阵1"><a href="#Comsol-数据导出成矩阵1" class="headerlink" title="Comsol 数据导出成矩阵1"></a>Comsol 数据导出成矩阵1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clear M;</span><br><span class="line">a=<span class="number">0</span>:<span class="number">2</span>:<span class="number">86</span>;</span><br><span class="line">b=<span class="number">486</span>;</span><br><span class="line">M=[];</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:length(a)</span><br><span class="line">    x=df(((i-<span class="number">1</span>)*b+<span class="number">1</span>:(i-<span class="number">1</span>)*b+<span class="number">486</span>),<span class="number">2</span>);</span><br><span class="line">    M=[M x];</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="Comsol-数据导出成矩阵2"><a href="#Comsol-数据导出成矩阵2" class="headerlink" title="Comsol 数据导出成矩阵2"></a>Comsol 数据导出成矩阵2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">wave=<span class="number">400</span>:<span class="number">20</span>:<span class="number">1300</span>;</span><br><span class="line"></span><br><span class="line">a=load(<span class="string">&#x27;C:\Users\Administrator\Desktop\Y.txt&#x27;</span>);</span><br><span class="line">dis=<span class="built_in">round</span>(a(<span class="number">2</span>,<span class="number">1</span>)-a(<span class="number">1</span>,<span class="number">1</span>),<span class="number">0</span>);</span><br><span class="line">x = <span class="built_in">round</span>((a(end,<span class="number">1</span>)-a(<span class="number">1</span>,<span class="number">1</span>))/dis,<span class="number">0</span>)+<span class="number">1</span>;</span><br><span class="line">clear M;</span><br><span class="line">dis=<span class="built_in">round</span>(a(<span class="number">2</span>,<span class="number">1</span>)-a(<span class="number">1</span>,<span class="number">1</span>),<span class="number">0</span>);</span><br><span class="line">x = <span class="built_in">round</span>((a(end,<span class="number">1</span>)-a(<span class="number">1</span>,<span class="number">1</span>))/dis,<span class="number">0</span>)+<span class="number">1</span>;</span><br><span class="line">dis = <span class="number">1</span>;</span><br><span class="line">x = <span class="number">31</span>;</span><br><span class="line"><span class="keyword">for</span> ix =<span class="number">1</span>:length(a(:,<span class="number">1</span>))/x</span><br><span class="line">    M(:,ix) =a((ix-<span class="number">1</span>)*x+<span class="number">1</span>:x*ix,<span class="number">2</span>); </span><br><span class="line">end</span><br><span class="line">plot(wave,M(:,<span class="number">8</span>:<span class="number">14</span>))</span><br><span class="line">plot(wave,M(:,<span class="number">15</span>:<span class="number">21</span>))</span><br><span class="line"></span><br><span class="line">b = reshape(a,<span class="number">217</span>,<span class="number">41</span>)</span><br></pre></td></tr></table></figure><p>MATLAB CRT Ra色坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br></pre></td><td class="code"><pre><span class="line">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span><br><span class="line">%This script calculates chromaticity coordinates, CCT, CRI, GAI, <span class="keyword">and</span> FSCI</span><br><span class="line">%<span class="keyword">for</span> a <span class="number">2856</span> blackbody source (illuminant A).  To use other sources, just</span><br><span class="line">%replace the spd matrix <span class="keyword">with</span> the values <span class="keyword">for</span> the other source.</span><br><span class="line">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span><br><span class="line"></span><br><span class="line">% Light source SPD</span><br><span class="line">%   The light source spd <span class="keyword">is</span> entered <span class="keyword">in</span> a <span class="number">2</span>-column <span class="built_in">format</span> <span class="keyword">as</span> shown below.</span><br><span class="line">%   [wavelength (nm)    value]</span><br><span class="line">spd =[                 <span class="number">380</span>       <span class="number">0.00132948037347175</span>;...</span><br><span class="line">                       <span class="number">390</span>       <span class="number">0.00164023432576807</span>;...</span><br><span class="line">                       <span class="number">400</span>       <span class="number">0.00199607408541092</span>;...</span><br><span class="line">                       <span class="number">410</span>       <span class="number">0.00239863299576014</span>;...</span><br><span class="line">                       <span class="number">420</span>       <span class="number">0.00284898784428011</span>;...</span><br><span class="line">                       <span class="number">430</span>       <span class="number">0.00334764547822949</span>;...</span><br><span class="line">                       <span class="number">440</span>       <span class="number">0.00389454498476917</span>;...</span><br><span class="line">                       <span class="number">450</span>        <span class="number">0.0044890736644276</span>;...</span><br><span class="line">                       <span class="number">460</span>       <span class="number">0.00513009474369056</span>;...</span><br><span class="line">                       <span class="number">470</span>       <span class="number">0.00581598464137424</span>;...</span><br><span class="line">                       <span class="number">480</span>       <span class="number">0.00654467759688934</span>;...</span><br><span class="line">                       <span class="number">490</span>       <span class="number">0.00731371555834619</span>;...</span><br><span class="line">                       <span class="number">500</span>       <span class="number">0.00812030138807687</span>;...</span><br><span class="line">                       <span class="number">510</span>       <span class="number">0.00896135364886274</span>;...</span><br><span class="line">                       <span class="number">520</span>       <span class="number">0.00983356146587939</span>;...</span><br><span class="line">                       <span class="number">530</span>        <span class="number">0.0107334382008557</span>;...</span><br><span class="line">                       <span class="number">540</span>        <span class="number">0.0116573729136429</span>;...</span><br><span class="line">                       <span class="number">550</span>        <span class="number">0.0126016788131223</span>;...</span><br><span class="line">                       <span class="number">560</span>        <span class="number">0.0135626381078808</span>;...</span><br><span class="line">                       <span class="number">570</span>        <span class="number">0.0145365428535139</span>;...</span><br><span class="line">                       <span class="number">580</span>        <span class="number">0.0155197315558729</span>;...</span><br><span class="line">                       <span class="number">590</span>        <span class="number">0.0165086214276558</span>;...</span><br><span class="line">                       <span class="number">600</span>        <span class="number">0.0174997363101301</span>;...</span><br><span class="line">                       <span class="number">610</span>        <span class="number">0.0184897303639286</span>;...</span><br><span class="line">                       <span class="number">620</span>        <span class="number">0.0194754077047119</span>;...</span><br><span class="line">                       <span class="number">630</span>        <span class="number">0.0204537382132532</span>;...</span><br><span class="line">                       <span class="number">640</span>        <span class="number">0.0214218697874947</span>;...</span><br><span class="line">                       <span class="number">650</span>         <span class="number">0.022377137328638</span>;...</span><br><span class="line">                       <span class="number">660</span>        <span class="number">0.0233170687665763</span>;...</span><br><span class="line">                       <span class="number">670</span>        <span class="number">0.0242393884339735</span>;...</span><br><span class="line">                       <span class="number">680</span>        <span class="number">0.0251420180949075</span>;...</span><br><span class="line">                       <span class="number">690</span>        <span class="number">0.0260230759248419</span>;...</span><br><span class="line">                       <span class="number">700</span>         <span class="number">0.026880873725211</span>;...</span><br><span class="line">                       <span class="number">710</span>        <span class="number">0.0277139126393038</span>;...</span><br><span class="line">                       <span class="number">720</span>        <span class="number">0.0285208776174515</span>;...</span><br><span class="line">                       <span class="number">730</span>        <span class="number">0.0293006308595934</span>;...</span><br><span class="line">                       <span class="number">740</span>        <span class="number">0.0300522044428256</span>;...</span><br><span class="line">                       <span class="number">750</span>        <span class="number">0.0307747923210527</span>;...</span><br><span class="line">                       <span class="number">760</span>        <span class="number">0.0314677418638086</span>;...</span><br><span class="line">                       <span class="number">770</span>        <span class="number">0.0321305450820022</span>;...</span><br><span class="line">                       <span class="number">780</span>        <span class="number">0.0327628296700076</span>];</span><br><span class="line">                   </span><br><span class="line">  wavelength_spd = spd(:,<span class="number">1</span>);</span><br><span class="line">  spd = spd(:,<span class="number">2</span>);</span><br><span class="line">       </span><br><span class="line">% First, obtain reference data values</span><br><span class="line">% The CIE <span class="number">1931</span> <span class="number">2</span> degree Standard Colorimetric Observer</span><br><span class="line">% [wavelength(nm)   xbar    ybar zbar]</span><br><span class="line">CIE31Table = [<span class="number">360</span> <span class="number">0.000130</span> <span class="number">0.000004</span> <span class="number">0.000606</span>;...</span><br><span class="line">            <span class="number">361</span> <span class="number">0.000146</span> <span class="number">0.000004</span> <span class="number">0.000681</span>;...</span><br><span class="line">            <span class="number">362</span> <span class="number">0.000164</span> <span class="number">0.000005</span> <span class="number">0.000765</span>;...</span><br><span class="line">            <span class="number">363</span> <span class="number">0.000184</span> <span class="number">0.000006</span> <span class="number">0.000860</span>;...</span><br><span class="line">            <span class="number">364</span> <span class="number">0.000207</span> <span class="number">0.000006</span> <span class="number">0.000967</span>;...</span><br><span class="line">            <span class="number">365</span> <span class="number">0.000232</span> <span class="number">0.000007</span> <span class="number">0.001086</span>;...</span><br><span class="line">            <span class="number">366</span> <span class="number">0.000261</span> <span class="number">0.000008</span> <span class="number">0.001221</span>;...</span><br><span class="line">            <span class="number">367</span> <span class="number">0.000293</span> <span class="number">0.000009</span> <span class="number">0.001373</span>;...</span><br><span class="line">            <span class="number">368</span> <span class="number">0.000329</span> <span class="number">0.000010</span> <span class="number">0.001544</span>;...</span><br><span class="line">            <span class="number">369</span> <span class="number">0.000370</span> <span class="number">0.000011</span> <span class="number">0.001734</span>;...</span><br><span class="line">            <span class="number">370</span> <span class="number">0.000415</span> <span class="number">0.000012</span> <span class="number">0.001946</span>;...</span><br><span class="line">            <span class="number">371</span> <span class="number">0.000464</span> <span class="number">0.000014</span> <span class="number">0.002178</span>;...</span><br><span class="line">            <span class="number">372</span> <span class="number">0.000519</span> <span class="number">0.000016</span> <span class="number">0.002436</span>;...</span><br><span class="line">            <span class="number">373</span> <span class="number">0.000582</span> <span class="number">0.000017</span> <span class="number">0.002732</span>;...</span><br><span class="line">            <span class="number">374</span> <span class="number">0.000655</span> <span class="number">0.000020</span> <span class="number">0.003078</span>;...</span><br><span class="line">            <span class="number">375</span> <span class="number">0.000742</span> <span class="number">0.000022</span> <span class="number">0.003486</span>;...</span><br><span class="line">            <span class="number">376</span> <span class="number">0.000845</span> <span class="number">0.000025</span> <span class="number">0.003975</span>;...</span><br><span class="line">            <span class="number">377</span> <span class="number">0.000965</span> <span class="number">0.000028</span> <span class="number">0.004541</span>;...</span><br><span class="line">            <span class="number">378</span> <span class="number">0.001095</span> <span class="number">0.000032</span> <span class="number">0.005158</span>;...</span><br><span class="line">            <span class="number">379</span> <span class="number">0.001231</span> <span class="number">0.000035</span> <span class="number">0.005803</span>;...</span><br><span class="line">            <span class="number">380</span> <span class="number">0.001368</span> <span class="number">0.000039</span> <span class="number">0.006450</span>;...</span><br><span class="line">            <span class="number">381</span> <span class="number">0.001502</span> <span class="number">0.000043</span> <span class="number">0.007083</span>;...</span><br><span class="line">            <span class="number">382</span> <span class="number">0.001642</span> <span class="number">0.000047</span> <span class="number">0.007745</span>;...</span><br><span class="line">            <span class="number">383</span> <span class="number">0.001802</span> <span class="number">0.000052</span> <span class="number">0.008501</span>;...</span><br><span class="line">            <span class="number">384</span> <span class="number">0.001996</span> <span class="number">0.000057</span> <span class="number">0.009415</span>;...</span><br><span class="line">            <span class="number">385</span> <span class="number">0.002236</span> <span class="number">0.000064</span> <span class="number">0.010550</span>;...</span><br><span class="line">            <span class="number">386</span> <span class="number">0.002535</span> <span class="number">0.000072</span> <span class="number">0.011966</span>;...</span><br><span class="line">            <span class="number">387</span> <span class="number">0.002893</span> <span class="number">0.000082</span> <span class="number">0.013656</span>;...</span><br><span class="line">            <span class="number">388</span> <span class="number">0.003301</span> <span class="number">0.000094</span> <span class="number">0.015588</span>;...</span><br><span class="line">            <span class="number">389</span> <span class="number">0.003753</span> <span class="number">0.000106</span> <span class="number">0.017730</span>;...</span><br><span class="line">            <span class="number">390</span> <span class="number">0.004243</span> <span class="number">0.000120</span> <span class="number">0.020050</span>;...</span><br><span class="line">            <span class="number">391</span> <span class="number">0.004762</span> <span class="number">0.000135</span> <span class="number">0.022511</span>;...</span><br><span class="line">            <span class="number">392</span> <span class="number">0.005330</span> <span class="number">0.000151</span> <span class="number">0.025203</span>;...</span><br><span class="line">            <span class="number">393</span> <span class="number">0.005979</span> <span class="number">0.000170</span> <span class="number">0.028280</span>;...</span><br><span class="line">            <span class="number">394</span> <span class="number">0.006741</span> <span class="number">0.000192</span> <span class="number">0.031897</span>;...</span><br><span class="line">            <span class="number">395</span> <span class="number">0.007650</span> <span class="number">0.000217</span> <span class="number">0.036210</span>;...</span><br><span class="line">            <span class="number">396</span> <span class="number">0.008751</span> <span class="number">0.000247</span> <span class="number">0.041438</span>;...</span><br><span class="line">            <span class="number">397</span> <span class="number">0.010029</span> <span class="number">0.000281</span> <span class="number">0.047504</span>;...</span><br><span class="line">            <span class="number">398</span> <span class="number">0.011422</span> <span class="number">0.000319</span> <span class="number">0.054120</span>;...</span><br><span class="line">            <span class="number">399</span> <span class="number">0.012869</span> <span class="number">0.000357</span> <span class="number">0.060998</span>;...</span><br><span class="line">            <span class="number">400</span> <span class="number">0.014310</span> <span class="number">0.000396</span> <span class="number">0.067850</span>;...</span><br><span class="line">            <span class="number">401</span> <span class="number">0.015704</span> <span class="number">0.000434</span> <span class="number">0.074486</span>;...</span><br><span class="line">            <span class="number">402</span> <span class="number">0.017147</span> <span class="number">0.000473</span> <span class="number">0.081362</span>;...</span><br><span class="line">            <span class="number">403</span> <span class="number">0.018781</span> <span class="number">0.000518</span> <span class="number">0.089154</span>;...</span><br><span class="line">            <span class="number">404</span> <span class="number">0.020748</span> <span class="number">0.000572</span> <span class="number">0.098540</span>;...</span><br><span class="line">            <span class="number">405</span> <span class="number">0.023190</span> <span class="number">0.000640</span> <span class="number">0.110200</span>;...</span><br><span class="line">            <span class="number">406</span> <span class="number">0.026207</span> <span class="number">0.000725</span> <span class="number">0.124613</span>;...</span><br><span class="line">            <span class="number">407</span> <span class="number">0.029782</span> <span class="number">0.000825</span> <span class="number">0.141702</span>;...</span><br><span class="line">            <span class="number">408</span> <span class="number">0.033881</span> <span class="number">0.000941</span> <span class="number">0.161303</span>;...</span><br><span class="line">            <span class="number">409</span> <span class="number">0.038468</span> <span class="number">0.001070</span> <span class="number">0.183257</span>;...</span><br><span class="line">            <span class="number">410</span> <span class="number">0.043510</span> <span class="number">0.001210</span> <span class="number">0.207400</span>;...</span><br><span class="line">            <span class="number">411</span> <span class="number">0.048996</span> <span class="number">0.001362</span> <span class="number">0.233692</span>;...</span><br><span class="line">            <span class="number">412</span> <span class="number">0.055023</span> <span class="number">0.001531</span> <span class="number">0.262611</span>;...</span><br><span class="line">            <span class="number">413</span> <span class="number">0.061719</span> <span class="number">0.001720</span> <span class="number">0.294775</span>;...</span><br><span class="line">            <span class="number">414</span> <span class="number">0.069212</span> <span class="number">0.001935</span> <span class="number">0.330799</span>;...</span><br><span class="line">            <span class="number">415</span> <span class="number">0.077630</span> <span class="number">0.002180</span> <span class="number">0.371300</span>;...</span><br><span class="line">            <span class="number">416</span> <span class="number">0.086958</span> <span class="number">0.002455</span> <span class="number">0.416209</span>;...</span><br><span class="line">            <span class="number">417</span> <span class="number">0.097177</span> <span class="number">0.002764</span> <span class="number">0.465464</span>;...</span><br><span class="line">            <span class="number">418</span> <span class="number">0.108406</span> <span class="number">0.003118</span> <span class="number">0.519695</span>;...</span><br><span class="line">            <span class="number">419</span> <span class="number">0.120767</span> <span class="number">0.003526</span> <span class="number">0.579530</span>;...</span><br><span class="line">            <span class="number">420</span> <span class="number">0.134380</span> <span class="number">0.004000</span> <span class="number">0.645600</span>;...</span><br><span class="line">            <span class="number">421</span> <span class="number">0.149358</span> <span class="number">0.004546</span> <span class="number">0.718484</span>;...</span><br><span class="line">            <span class="number">422</span> <span class="number">0.165396</span> <span class="number">0.005159</span> <span class="number">0.796713</span>;...</span><br><span class="line">            <span class="number">423</span> <span class="number">0.181983</span> <span class="number">0.005829</span> <span class="number">0.877846</span>;...</span><br><span class="line">            <span class="number">424</span> <span class="number">0.198611</span> <span class="number">0.006546</span> <span class="number">0.959439</span>;...</span><br><span class="line">            <span class="number">425</span> <span class="number">0.214770</span> <span class="number">0.007300</span> <span class="number">1.039050</span>;...</span><br><span class="line">            <span class="number">426</span> <span class="number">0.230187</span> <span class="number">0.008087</span> <span class="number">1.115367</span>;...</span><br><span class="line">            <span class="number">427</span> <span class="number">0.244880</span> <span class="number">0.008909</span> <span class="number">1.188497</span>;...</span><br><span class="line">            <span class="number">428</span> <span class="number">0.258777</span> <span class="number">0.009768</span> <span class="number">1.258123</span>;...</span><br><span class="line">            <span class="number">429</span> <span class="number">0.271808</span> <span class="number">0.010664</span> <span class="number">1.323930</span>;...</span><br><span class="line">            <span class="number">430</span> <span class="number">0.283900</span> <span class="number">0.011600</span> <span class="number">1.385600</span>;...</span><br><span class="line">            <span class="number">431</span> <span class="number">0.294944</span> <span class="number">0.012573</span> <span class="number">1.442635</span>;...</span><br><span class="line">            <span class="number">432</span> <span class="number">0.304897</span> <span class="number">0.013583</span> <span class="number">1.494804</span>;...</span><br><span class="line">            <span class="number">433</span> <span class="number">0.313787</span> <span class="number">0.014630</span> <span class="number">1.542190</span>;...</span><br><span class="line">            <span class="number">434</span> <span class="number">0.321645</span> <span class="number">0.015715</span> <span class="number">1.584881</span>;...</span><br><span class="line">            <span class="number">435</span> <span class="number">0.328500</span> <span class="number">0.016840</span> <span class="number">1.622960</span>;...</span><br><span class="line">            <span class="number">436</span> <span class="number">0.334351</span> <span class="number">0.018007</span> <span class="number">1.656405</span>;...</span><br><span class="line">            <span class="number">437</span> <span class="number">0.339210</span> <span class="number">0.019214</span> <span class="number">1.685296</span>;...</span><br><span class="line">            <span class="number">438</span> <span class="number">0.343121</span> <span class="number">0.020454</span> <span class="number">1.709875</span>;...</span><br><span class="line">            <span class="number">439</span> <span class="number">0.346130</span> <span class="number">0.021718</span> <span class="number">1.730382</span>;...</span><br><span class="line">            <span class="number">440</span> <span class="number">0.348280</span> <span class="number">0.023000</span> <span class="number">1.747060</span>;...</span><br><span class="line">            <span class="number">441</span> <span class="number">0.349600</span> <span class="number">0.024295</span> <span class="number">1.760045</span>;...</span><br><span class="line">            <span class="number">442</span> <span class="number">0.350147</span> <span class="number">0.025610</span> <span class="number">1.769623</span>;...</span><br><span class="line">            <span class="number">443</span> <span class="number">0.350013</span> <span class="number">0.026959</span> <span class="number">1.776264</span>;...</span><br><span class="line">            <span class="number">444</span> <span class="number">0.349287</span> <span class="number">0.028351</span> <span class="number">1.780433</span>;...</span><br><span class="line">            <span class="number">445</span> <span class="number">0.348060</span> <span class="number">0.029800</span> <span class="number">1.782600</span>;...</span><br><span class="line">            <span class="number">446</span> <span class="number">0.346373</span> <span class="number">0.031311</span> <span class="number">1.782968</span>;...</span><br><span class="line">            <span class="number">447</span> <span class="number">0.344262</span> <span class="number">0.032884</span> <span class="number">1.781700</span>;...</span><br><span class="line">            <span class="number">448</span> <span class="number">0.341809</span> <span class="number">0.034521</span> <span class="number">1.779198</span>;...</span><br><span class="line">            <span class="number">449</span> <span class="number">0.339094</span> <span class="number">0.036226</span> <span class="number">1.775867</span>;...</span><br><span class="line">            <span class="number">450</span> <span class="number">0.336200</span> <span class="number">0.038000</span> <span class="number">1.772110</span>;...</span><br><span class="line">            <span class="number">451</span> <span class="number">0.333198</span> <span class="number">0.039847</span> <span class="number">1.768259</span>;...</span><br><span class="line">            <span class="number">452</span> <span class="number">0.330041</span> <span class="number">0.041768</span> <span class="number">1.764039</span>;...</span><br><span class="line">            <span class="number">453</span> <span class="number">0.326636</span> <span class="number">0.043766</span> <span class="number">1.758944</span>;...</span><br><span class="line">            <span class="number">454</span> <span class="number">0.322887</span> <span class="number">0.045843</span> <span class="number">1.752466</span>;...</span><br><span class="line">            <span class="number">455</span> <span class="number">0.318700</span> <span class="number">0.048000</span> <span class="number">1.744100</span>;...</span><br><span class="line">            <span class="number">456</span> <span class="number">0.314025</span> <span class="number">0.050244</span> <span class="number">1.733559</span>;...</span><br><span class="line">            <span class="number">457</span> <span class="number">0.308884</span> <span class="number">0.052573</span> <span class="number">1.720858</span>;...</span><br><span class="line">            <span class="number">458</span> <span class="number">0.303290</span> <span class="number">0.054981</span> <span class="number">1.705937</span>;...</span><br><span class="line">            <span class="number">459</span> <span class="number">0.297258</span> <span class="number">0.057459</span> <span class="number">1.688737</span>;...</span><br><span class="line">            <span class="number">460</span> <span class="number">0.290800</span> <span class="number">0.060000</span> <span class="number">1.669200</span>;...</span><br><span class="line">            <span class="number">461</span> <span class="number">0.283970</span> <span class="number">0.062602</span> <span class="number">1.647529</span>;...</span><br><span class="line">            <span class="number">462</span> <span class="number">0.276721</span> <span class="number">0.065278</span> <span class="number">1.623413</span>;...</span><br><span class="line">            <span class="number">463</span> <span class="number">0.268918</span> <span class="number">0.068042</span> <span class="number">1.596022</span>;...</span><br><span class="line">            <span class="number">464</span> <span class="number">0.260423</span> <span class="number">0.070911</span> <span class="number">1.564528</span>;...</span><br><span class="line">            <span class="number">465</span> <span class="number">0.251100</span> <span class="number">0.073900</span> <span class="number">1.528100</span>;...</span><br><span class="line">            <span class="number">466</span> <span class="number">0.240847</span> <span class="number">0.077016</span> <span class="number">1.486111</span>;...</span><br><span class="line">            <span class="number">467</span> <span class="number">0.229851</span> <span class="number">0.080266</span> <span class="number">1.439521</span>;...</span><br><span class="line">            <span class="number">468</span> <span class="number">0.218407</span> <span class="number">0.083667</span> <span class="number">1.389880</span>;...</span><br><span class="line">            <span class="number">469</span> <span class="number">0.206812</span> <span class="number">0.087233</span> <span class="number">1.338736</span>;...</span><br><span class="line">            <span class="number">470</span> <span class="number">0.195360</span> <span class="number">0.090980</span> <span class="number">1.287640</span>;...</span><br><span class="line">            <span class="number">471</span> <span class="number">0.184214</span> <span class="number">0.094918</span> <span class="number">1.237422</span>;...</span><br><span class="line">            <span class="number">472</span> <span class="number">0.173327</span> <span class="number">0.099046</span> <span class="number">1.187824</span>;...</span><br><span class="line">            <span class="number">473</span> <span class="number">0.162688</span> <span class="number">0.103367</span> <span class="number">1.138761</span>;...</span><br><span class="line">            <span class="number">474</span> <span class="number">0.152283</span> <span class="number">0.107885</span> <span class="number">1.090148</span>;...</span><br><span class="line">            <span class="number">475</span> <span class="number">0.142100</span> <span class="number">0.112600</span> <span class="number">1.041900</span>;...</span><br><span class="line">            <span class="number">476</span> <span class="number">0.132179</span> <span class="number">0.117532</span> <span class="number">0.994198</span>;...</span><br><span class="line">            <span class="number">477</span> <span class="number">0.122570</span> <span class="number">0.122674</span> <span class="number">0.947347</span>;...</span><br><span class="line">            <span class="number">478</span> <span class="number">0.113275</span> <span class="number">0.127993</span> <span class="number">0.901453</span>;...</span><br><span class="line">            <span class="number">479</span> <span class="number">0.104298</span> <span class="number">0.133453</span> <span class="number">0.856619</span>;...</span><br><span class="line">            <span class="number">480</span> <span class="number">0.095640</span> <span class="number">0.139020</span> <span class="number">0.812950</span>;...</span><br><span class="line">            <span class="number">481</span> <span class="number">0.087300</span> <span class="number">0.144676</span> <span class="number">0.770517</span>;...</span><br><span class="line">            <span class="number">482</span> <span class="number">0.079308</span> <span class="number">0.150469</span> <span class="number">0.729445</span>;...</span><br><span class="line">            <span class="number">483</span> <span class="number">0.071718</span> <span class="number">0.156462</span> <span class="number">0.689914</span>;...</span><br><span class="line">            <span class="number">484</span> <span class="number">0.064581</span> <span class="number">0.162718</span> <span class="number">0.652105</span>;...</span><br><span class="line">            <span class="number">485</span> <span class="number">0.057950</span> <span class="number">0.169300</span> <span class="number">0.616200</span>;...</span><br><span class="line">            <span class="number">486</span> <span class="number">0.051862</span> <span class="number">0.176243</span> <span class="number">0.582329</span>;...</span><br><span class="line">            <span class="number">487</span> <span class="number">0.046282</span> <span class="number">0.183558</span> <span class="number">0.550416</span>;...</span><br><span class="line">            <span class="number">488</span> <span class="number">0.041151</span> <span class="number">0.191274</span> <span class="number">0.520338</span>;...</span><br><span class="line">            <span class="number">489</span> <span class="number">0.036413</span> <span class="number">0.199418</span> <span class="number">0.491967</span>;...</span><br><span class="line">            <span class="number">490</span> <span class="number">0.032010</span> <span class="number">0.208020</span> <span class="number">0.465180</span>;...</span><br><span class="line">            <span class="number">491</span> <span class="number">0.027917</span> <span class="number">0.217120</span> <span class="number">0.439925</span>;...</span><br><span class="line">            <span class="number">492</span> <span class="number">0.024144</span> <span class="number">0.226735</span> <span class="number">0.416184</span>;...</span><br><span class="line">            <span class="number">493</span> <span class="number">0.020687</span> <span class="number">0.236857</span> <span class="number">0.393882</span>;...</span><br><span class="line">            <span class="number">494</span> <span class="number">0.017540</span> <span class="number">0.247481</span> <span class="number">0.372946</span>;...</span><br><span class="line">            <span class="number">495</span> <span class="number">0.014700</span> <span class="number">0.258600</span> <span class="number">0.353300</span>;...</span><br><span class="line">            <span class="number">496</span> <span class="number">0.012162</span> <span class="number">0.270185</span> <span class="number">0.334858</span>;...</span><br><span class="line">            <span class="number">497</span> <span class="number">0.009920</span> <span class="number">0.282294</span> <span class="number">0.317552</span>;...</span><br><span class="line">            <span class="number">498</span> <span class="number">0.007967</span> <span class="number">0.295050</span> <span class="number">0.301337</span>;...</span><br><span class="line">            <span class="number">499</span> <span class="number">0.006296</span> <span class="number">0.308578</span> <span class="number">0.286169</span>;...</span><br><span class="line">            <span class="number">500</span> <span class="number">0.004900</span> <span class="number">0.323000</span> <span class="number">0.272000</span>;...</span><br><span class="line">            <span class="number">501</span> <span class="number">0.003777</span> <span class="number">0.338402</span> <span class="number">0.258817</span>;...</span><br><span class="line">            <span class="number">502</span> <span class="number">0.002945</span> <span class="number">0.354686</span> <span class="number">0.246484</span>;...</span><br><span class="line">            <span class="number">503</span> <span class="number">0.002425</span> <span class="number">0.371699</span> <span class="number">0.234772</span>;...</span><br><span class="line">            <span class="number">504</span> <span class="number">0.002236</span> <span class="number">0.389288</span> <span class="number">0.223453</span>;...</span><br><span class="line">            <span class="number">505</span> <span class="number">0.002400</span> <span class="number">0.407300</span> <span class="number">0.212300</span>;...</span><br><span class="line">            <span class="number">506</span> <span class="number">0.002926</span> <span class="number">0.425630</span> <span class="number">0.201169</span>;...</span><br><span class="line">            <span class="number">507</span> <span class="number">0.003837</span> <span class="number">0.444310</span> <span class="number">0.190120</span>;...</span><br><span class="line">            <span class="number">508</span> <span class="number">0.005175</span> <span class="number">0.463394</span> <span class="number">0.179225</span>;...</span><br><span class="line">            <span class="number">509</span> <span class="number">0.006982</span> <span class="number">0.482940</span> <span class="number">0.168561</span>;...</span><br><span class="line">            <span class="number">510</span> <span class="number">0.009300</span> <span class="number">0.503000</span> <span class="number">0.158200</span>;...</span><br><span class="line">            <span class="number">511</span> <span class="number">0.012149</span> <span class="number">0.523569</span> <span class="number">0.148138</span>;...</span><br><span class="line">            <span class="number">512</span> <span class="number">0.015536</span> <span class="number">0.544512</span> <span class="number">0.138376</span>;...</span><br><span class="line">            <span class="number">513</span> <span class="number">0.019478</span> <span class="number">0.565690</span> <span class="number">0.128994</span>;...</span><br><span class="line">            <span class="number">514</span> <span class="number">0.023993</span> <span class="number">0.586965</span> <span class="number">0.120075</span>;...</span><br><span class="line">            <span class="number">515</span> <span class="number">0.029100</span> <span class="number">0.608200</span> <span class="number">0.111700</span>;...</span><br><span class="line">            <span class="number">516</span> <span class="number">0.034815</span> <span class="number">0.629346</span> <span class="number">0.103905</span>;...</span><br><span class="line">            <span class="number">517</span> <span class="number">0.041120</span> <span class="number">0.650307</span> <span class="number">0.096667</span>;...</span><br><span class="line">            <span class="number">518</span> <span class="number">0.047985</span> <span class="number">0.670875</span> <span class="number">0.089983</span>;...</span><br><span class="line">            <span class="number">519</span> <span class="number">0.055379</span> <span class="number">0.690842</span> <span class="number">0.083845</span>;...</span><br><span class="line">            <span class="number">520</span> <span class="number">0.063270</span> <span class="number">0.710000</span> <span class="number">0.078250</span>;...</span><br><span class="line">            <span class="number">521</span> <span class="number">0.071635</span> <span class="number">0.728185</span> <span class="number">0.073209</span>;...</span><br><span class="line">            <span class="number">522</span> <span class="number">0.080462</span> <span class="number">0.745464</span> <span class="number">0.068678</span>;...</span><br><span class="line">            <span class="number">523</span> <span class="number">0.089740</span> <span class="number">0.761969</span> <span class="number">0.064568</span>;...</span><br><span class="line">            <span class="number">524</span> <span class="number">0.099456</span> <span class="number">0.777837</span> <span class="number">0.060788</span>;...</span><br><span class="line">            <span class="number">525</span> <span class="number">0.109600</span> <span class="number">0.793200</span> <span class="number">0.057250</span>;...</span><br><span class="line">            <span class="number">526</span> <span class="number">0.120167</span> <span class="number">0.808110</span> <span class="number">0.053904</span>;...</span><br><span class="line">            <span class="number">527</span> <span class="number">0.131114</span> <span class="number">0.822496</span> <span class="number">0.050747</span>;...</span><br><span class="line">            <span class="number">528</span> <span class="number">0.142368</span> <span class="number">0.836307</span> <span class="number">0.047753</span>;...</span><br><span class="line">            <span class="number">529</span> <span class="number">0.153854</span> <span class="number">0.849492</span> <span class="number">0.044899</span>;...</span><br><span class="line">            <span class="number">530</span> <span class="number">0.165500</span> <span class="number">0.862000</span> <span class="number">0.042160</span>;...</span><br><span class="line">            <span class="number">531</span> <span class="number">0.177257</span> <span class="number">0.873811</span> <span class="number">0.039507</span>;...</span><br><span class="line">            <span class="number">532</span> <span class="number">0.189140</span> <span class="number">0.884962</span> <span class="number">0.036936</span>;...</span><br><span class="line">            <span class="number">533</span> <span class="number">0.201169</span> <span class="number">0.895494</span> <span class="number">0.034458</span>;...</span><br><span class="line">            <span class="number">534</span> <span class="number">0.213366</span> <span class="number">0.905443</span> <span class="number">0.032089</span>;...</span><br><span class="line">            <span class="number">535</span> <span class="number">0.225750</span> <span class="number">0.914850</span> <span class="number">0.029840</span>;...</span><br><span class="line">            <span class="number">536</span> <span class="number">0.238321</span> <span class="number">0.923735</span> <span class="number">0.027712</span>;...</span><br><span class="line">            <span class="number">537</span> <span class="number">0.251067</span> <span class="number">0.932092</span> <span class="number">0.025694</span>;...</span><br><span class="line">            <span class="number">538</span> <span class="number">0.263992</span> <span class="number">0.939923</span> <span class="number">0.023787</span>;...</span><br><span class="line">            <span class="number">539</span> <span class="number">0.277102</span> <span class="number">0.947225</span> <span class="number">0.021989</span>;...</span><br><span class="line">            <span class="number">540</span> <span class="number">0.290400</span> <span class="number">0.954000</span> <span class="number">0.020300</span>;...</span><br><span class="line">            <span class="number">541</span> <span class="number">0.303891</span> <span class="number">0.960256</span> <span class="number">0.018718</span>;...</span><br><span class="line">            <span class="number">542</span> <span class="number">0.317573</span> <span class="number">0.966007</span> <span class="number">0.017240</span>;...</span><br><span class="line">            <span class="number">543</span> <span class="number">0.331438</span> <span class="number">0.971261</span> <span class="number">0.015864</span>;...</span><br><span class="line">            <span class="number">544</span> <span class="number">0.345483</span> <span class="number">0.976023</span> <span class="number">0.014585</span>;...</span><br><span class="line">            <span class="number">545</span> <span class="number">0.359700</span> <span class="number">0.980300</span> <span class="number">0.013400</span>;...</span><br><span class="line">            <span class="number">546</span> <span class="number">0.374084</span> <span class="number">0.984092</span> <span class="number">0.012307</span>;...</span><br><span class="line">            <span class="number">547</span> <span class="number">0.388640</span> <span class="number">0.987418</span> <span class="number">0.011302</span>;...</span><br><span class="line">            <span class="number">548</span> <span class="number">0.403378</span> <span class="number">0.990313</span> <span class="number">0.010378</span>;...</span><br><span class="line">            <span class="number">549</span> <span class="number">0.418312</span> <span class="number">0.992812</span> <span class="number">0.009529</span>;...</span><br><span class="line">            <span class="number">550</span> <span class="number">0.433450</span> <span class="number">0.994950</span> <span class="number">0.008750</span>;...</span><br><span class="line">            <span class="number">551</span> <span class="number">0.448795</span> <span class="number">0.996711</span> <span class="number">0.008035</span>;...</span><br><span class="line">            <span class="number">552</span> <span class="number">0.464336</span> <span class="number">0.998098</span> <span class="number">0.007382</span>;...</span><br><span class="line">            <span class="number">553</span> <span class="number">0.480064</span> <span class="number">0.999112</span> <span class="number">0.006785</span>;...</span><br><span class="line">            <span class="number">554</span> <span class="number">0.495971</span> <span class="number">0.999748</span> <span class="number">0.006243</span>;...</span><br><span class="line">            <span class="number">555</span> <span class="number">0.512050</span> <span class="number">1.000000</span> <span class="number">0.005750</span>;...</span><br><span class="line">            <span class="number">556</span> <span class="number">0.528296</span> <span class="number">0.999857</span> <span class="number">0.005304</span>;...</span><br><span class="line">            <span class="number">557</span> <span class="number">0.544692</span> <span class="number">0.999305</span> <span class="number">0.004900</span>;...</span><br><span class="line">            <span class="number">558</span> <span class="number">0.561209</span> <span class="number">0.998325</span> <span class="number">0.004534</span>;...</span><br><span class="line">            <span class="number">559</span> <span class="number">0.577821</span> <span class="number">0.996899</span> <span class="number">0.004202</span>;...</span><br><span class="line">            <span class="number">560</span> <span class="number">0.594500</span> <span class="number">0.995000</span> <span class="number">0.003900</span>;...</span><br><span class="line">            <span class="number">561</span> <span class="number">0.611221</span> <span class="number">0.992601</span> <span class="number">0.003623</span>;...</span><br><span class="line">            <span class="number">562</span> <span class="number">0.627976</span> <span class="number">0.989743</span> <span class="number">0.003371</span>;...</span><br><span class="line">            <span class="number">563</span> <span class="number">0.644760</span> <span class="number">0.986444</span> <span class="number">0.003141</span>;...</span><br><span class="line">            <span class="number">564</span> <span class="number">0.661570</span> <span class="number">0.982724</span> <span class="number">0.002935</span>;...</span><br><span class="line">            <span class="number">565</span> <span class="number">0.678400</span> <span class="number">0.978600</span> <span class="number">0.002750</span>;...</span><br><span class="line">            <span class="number">566</span> <span class="number">0.695239</span> <span class="number">0.974084</span> <span class="number">0.002585</span>;...</span><br><span class="line">            <span class="number">567</span> <span class="number">0.712059</span> <span class="number">0.969171</span> <span class="number">0.002439</span>;...</span><br><span class="line">            <span class="number">568</span> <span class="number">0.728828</span> <span class="number">0.963857</span> <span class="number">0.002309</span>;...</span><br><span class="line">            <span class="number">569</span> <span class="number">0.745519</span> <span class="number">0.958135</span> <span class="number">0.002197</span>;...</span><br><span class="line">            <span class="number">570</span> <span class="number">0.762100</span> <span class="number">0.952000</span> <span class="number">0.002100</span>;...</span><br><span class="line">            <span class="number">571</span> <span class="number">0.778543</span> <span class="number">0.945450</span> <span class="number">0.002018</span>;...</span><br><span class="line">            <span class="number">572</span> <span class="number">0.794826</span> <span class="number">0.938499</span> <span class="number">0.001948</span>;...</span><br><span class="line">            <span class="number">573</span> <span class="number">0.810926</span> <span class="number">0.931163</span> <span class="number">0.001890</span>;...</span><br><span class="line">            <span class="number">574</span> <span class="number">0.826825</span> <span class="number">0.923458</span> <span class="number">0.001841</span>;...</span><br><span class="line">            <span class="number">575</span> <span class="number">0.842500</span> <span class="number">0.915400</span> <span class="number">0.001800</span>;...</span><br><span class="line">            <span class="number">576</span> <span class="number">0.857932</span> <span class="number">0.907006</span> <span class="number">0.001766</span>;...</span><br><span class="line">            <span class="number">577</span> <span class="number">0.873082</span> <span class="number">0.898277</span> <span class="number">0.001738</span>;...</span><br><span class="line">            <span class="number">578</span> <span class="number">0.887894</span> <span class="number">0.889205</span> <span class="number">0.001711</span>;...</span><br><span class="line">            <span class="number">579</span> <span class="number">0.902318</span> <span class="number">0.879782</span> <span class="number">0.001683</span>;...</span><br><span class="line">            <span class="number">580</span> <span class="number">0.916300</span> <span class="number">0.870000</span> <span class="number">0.001650</span>;...</span><br><span class="line">            <span class="number">581</span> <span class="number">0.929800</span> <span class="number">0.859861</span> <span class="number">0.001610</span>;...</span><br><span class="line">            <span class="number">582</span> <span class="number">0.942798</span> <span class="number">0.849392</span> <span class="number">0.001564</span>;...</span><br><span class="line">            <span class="number">583</span> <span class="number">0.955278</span> <span class="number">0.838622</span> <span class="number">0.001514</span>;...</span><br><span class="line">            <span class="number">584</span> <span class="number">0.967218</span> <span class="number">0.827581</span> <span class="number">0.001459</span>;...</span><br><span class="line">            <span class="number">585</span> <span class="number">0.978600</span> <span class="number">0.816300</span> <span class="number">0.001400</span>;...</span><br><span class="line">            <span class="number">586</span> <span class="number">0.989386</span> <span class="number">0.804795</span> <span class="number">0.001337</span>;...</span><br><span class="line">            <span class="number">587</span> <span class="number">0.999549</span> <span class="number">0.793082</span> <span class="number">0.001270</span>;...</span><br><span class="line">            <span class="number">588</span> <span class="number">1.009089</span> <span class="number">0.781192</span> <span class="number">0.001205</span>;...</span><br><span class="line">            <span class="number">589</span> <span class="number">1.018006</span> <span class="number">0.769155</span> <span class="number">0.001147</span>;...</span><br><span class="line">            <span class="number">590</span> <span class="number">1.026300</span> <span class="number">0.757000</span> <span class="number">0.001100</span>;...</span><br><span class="line">            <span class="number">591</span> <span class="number">1.033983</span> <span class="number">0.744754</span> <span class="number">0.001069</span>;...</span><br><span class="line">            <span class="number">592</span> <span class="number">1.040986</span> <span class="number">0.732422</span> <span class="number">0.001049</span>;...</span><br><span class="line">            <span class="number">593</span> <span class="number">1.047188</span> <span class="number">0.720004</span> <span class="number">0.001036</span>;...</span><br><span class="line">            <span class="number">594</span> <span class="number">1.052467</span> <span class="number">0.707496</span> <span class="number">0.001021</span>;...</span><br><span class="line">            <span class="number">595</span> <span class="number">1.056700</span> <span class="number">0.694900</span> <span class="number">0.001000</span>;...</span><br><span class="line">            <span class="number">596</span> <span class="number">1.059794</span> <span class="number">0.682219</span> <span class="number">0.000969</span>;...</span><br><span class="line">            <span class="number">597</span> <span class="number">1.061799</span> <span class="number">0.669472</span> <span class="number">0.000930</span>;...</span><br><span class="line">            <span class="number">598</span> <span class="number">1.062807</span> <span class="number">0.656674</span> <span class="number">0.000887</span>;...</span><br><span class="line">            <span class="number">599</span> <span class="number">1.062910</span> <span class="number">0.643845</span> <span class="number">0.000843</span>;...</span><br><span class="line">            <span class="number">600</span> <span class="number">1.062200</span> <span class="number">0.631000</span> <span class="number">0.000800</span>;...</span><br><span class="line">            <span class="number">601</span> <span class="number">1.060735</span> <span class="number">0.618155</span> <span class="number">0.000761</span>;...</span><br><span class="line">            <span class="number">602</span> <span class="number">1.058444</span> <span class="number">0.605314</span> <span class="number">0.000724</span>;...</span><br><span class="line">            <span class="number">603</span> <span class="number">1.055224</span> <span class="number">0.592476</span> <span class="number">0.000686</span>;...</span><br><span class="line">            <span class="number">604</span> <span class="number">1.050977</span> <span class="number">0.579638</span> <span class="number">0.000645</span>;...</span><br><span class="line">            <span class="number">605</span> <span class="number">1.045600</span> <span class="number">0.566800</span> <span class="number">0.000600</span>;...</span><br><span class="line">            <span class="number">606</span> <span class="number">1.039037</span> <span class="number">0.553961</span> <span class="number">0.000548</span>;...</span><br><span class="line">            <span class="number">607</span> <span class="number">1.031361</span> <span class="number">0.541137</span> <span class="number">0.000492</span>;...</span><br><span class="line">            <span class="number">608</span> <span class="number">1.022666</span> <span class="number">0.528353</span> <span class="number">0.000435</span>;...</span><br><span class="line">            <span class="number">609</span> <span class="number">1.013048</span> <span class="number">0.515632</span> <span class="number">0.000383</span>;...</span><br><span class="line">            <span class="number">610</span> <span class="number">1.002600</span> <span class="number">0.503000</span> <span class="number">0.000340</span>;...</span><br><span class="line">            <span class="number">611</span> <span class="number">0.991367</span> <span class="number">0.490469</span> <span class="number">0.000307</span>;...</span><br><span class="line">            <span class="number">612</span> <span class="number">0.979331</span> <span class="number">0.478030</span> <span class="number">0.000283</span>;...</span><br><span class="line">            <span class="number">613</span> <span class="number">0.966492</span> <span class="number">0.465678</span> <span class="number">0.000265</span>;...</span><br><span class="line">            <span class="number">614</span> <span class="number">0.952848</span> <span class="number">0.453403</span> <span class="number">0.000252</span>;...</span><br><span class="line">            <span class="number">615</span> <span class="number">0.938400</span> <span class="number">0.441200</span> <span class="number">0.000240</span>;...</span><br><span class="line">            <span class="number">616</span> <span class="number">0.923194</span> <span class="number">0.429080</span> <span class="number">0.000230</span>;...</span><br><span class="line">            <span class="number">617</span> <span class="number">0.907244</span> <span class="number">0.417036</span> <span class="number">0.000221</span>;...</span><br><span class="line">            <span class="number">618</span> <span class="number">0.890502</span> <span class="number">0.405032</span> <span class="number">0.000212</span>;...</span><br><span class="line">            <span class="number">619</span> <span class="number">0.872920</span> <span class="number">0.393032</span> <span class="number">0.000202</span>;...</span><br><span class="line">            <span class="number">620</span> <span class="number">0.854450</span> <span class="number">0.381000</span> <span class="number">0.000190</span>;...</span><br><span class="line">            <span class="number">621</span> <span class="number">0.835084</span> <span class="number">0.368918</span> <span class="number">0.000174</span>;...</span><br><span class="line">            <span class="number">622</span> <span class="number">0.814946</span> <span class="number">0.356827</span> <span class="number">0.000156</span>;...</span><br><span class="line">            <span class="number">623</span> <span class="number">0.794186</span> <span class="number">0.344777</span> <span class="number">0.000136</span>;...</span><br><span class="line">            <span class="number">624</span> <span class="number">0.772954</span> <span class="number">0.332818</span> <span class="number">0.000117</span>;...</span><br><span class="line">            <span class="number">625</span> <span class="number">0.751400</span> <span class="number">0.321000</span> <span class="number">0.000100</span>;...</span><br><span class="line">            <span class="number">626</span> <span class="number">0.729584</span> <span class="number">0.309338</span> <span class="number">0.000086</span>;...</span><br><span class="line">            <span class="number">627</span> <span class="number">0.707589</span> <span class="number">0.297850</span> <span class="number">0.000075</span>;...</span><br><span class="line">            <span class="number">628</span> <span class="number">0.685602</span> <span class="number">0.286594</span> <span class="number">0.000065</span>;...</span><br><span class="line">            <span class="number">629</span> <span class="number">0.663810</span> <span class="number">0.275624</span> <span class="number">0.000057</span>;...</span><br><span class="line">            <span class="number">630</span> <span class="number">0.642400</span> <span class="number">0.265000</span> <span class="number">0.000050</span>;...</span><br><span class="line">            <span class="number">631</span> <span class="number">0.621515</span> <span class="number">0.254763</span> <span class="number">0.000044</span>;...</span><br><span class="line">            <span class="number">632</span> <span class="number">0.601114</span> <span class="number">0.244890</span> <span class="number">0.000039</span>;...</span><br><span class="line">            <span class="number">633</span> <span class="number">0.581105</span> <span class="number">0.235334</span> <span class="number">0.000036</span>;...</span><br><span class="line">            <span class="number">634</span> <span class="number">0.561398</span> <span class="number">0.226053</span> <span class="number">0.000033</span>;...</span><br><span class="line">            <span class="number">635</span> <span class="number">0.541900</span> <span class="number">0.217000</span> <span class="number">0.000030</span>;...</span><br><span class="line">            <span class="number">636</span> <span class="number">0.522599</span> <span class="number">0.208162</span> <span class="number">0.000028</span>;...</span><br><span class="line">            <span class="number">637</span> <span class="number">0.503546</span> <span class="number">0.199549</span> <span class="number">0.000026</span>;...</span><br><span class="line">            <span class="number">638</span> <span class="number">0.484744</span> <span class="number">0.191155</span> <span class="number">0.000024</span>;...</span><br><span class="line">            <span class="number">639</span> <span class="number">0.466194</span> <span class="number">0.182974</span> <span class="number">0.000022</span>;...</span><br><span class="line">            <span class="number">640</span> <span class="number">0.447900</span> <span class="number">0.175000</span> <span class="number">0.000020</span>;...</span><br><span class="line">            <span class="number">641</span> <span class="number">0.429861</span> <span class="number">0.167224</span> <span class="number">0.000018</span>;...</span><br><span class="line">            <span class="number">642</span> <span class="number">0.412098</span> <span class="number">0.159646</span> <span class="number">0.000016</span>;...</span><br><span class="line">            <span class="number">643</span> <span class="number">0.394644</span> <span class="number">0.152278</span> <span class="number">0.000014</span>;...</span><br><span class="line">            <span class="number">644</span> <span class="number">0.377533</span> <span class="number">0.145126</span> <span class="number">0.000012</span>;...</span><br><span class="line">            <span class="number">645</span> <span class="number">0.360800</span> <span class="number">0.138200</span> <span class="number">0.000010</span>;...</span><br><span class="line">            <span class="number">646</span> <span class="number">0.344456</span> <span class="number">0.131500</span> <span class="number">0.000008</span>;...</span><br><span class="line">            <span class="number">647</span> <span class="number">0.328517</span> <span class="number">0.125025</span> <span class="number">0.000005</span>;...</span><br><span class="line">            <span class="number">648</span> <span class="number">0.313019</span> <span class="number">0.118779</span> <span class="number">0.000003</span>;...</span><br><span class="line">            <span class="number">649</span> <span class="number">0.298001</span> <span class="number">0.112769</span> <span class="number">0.000001</span>;...</span><br><span class="line">            <span class="number">650</span> <span class="number">0.283500</span> <span class="number">0.107000</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">651</span> <span class="number">0.269545</span> <span class="number">0.101476</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">652</span> <span class="number">0.256118</span> <span class="number">0.096189</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">653</span> <span class="number">0.243190</span> <span class="number">0.091123</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">654</span> <span class="number">0.230727</span> <span class="number">0.086265</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">655</span> <span class="number">0.218700</span> <span class="number">0.081600</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">656</span> <span class="number">0.207097</span> <span class="number">0.077121</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">657</span> <span class="number">0.195923</span> <span class="number">0.072826</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">658</span> <span class="number">0.185171</span> <span class="number">0.068710</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">659</span> <span class="number">0.174832</span> <span class="number">0.064770</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">660</span> <span class="number">0.164900</span> <span class="number">0.061000</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">661</span> <span class="number">0.155367</span> <span class="number">0.057396</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">662</span> <span class="number">0.146230</span> <span class="number">0.053955</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">663</span> <span class="number">0.137490</span> <span class="number">0.050674</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">664</span> <span class="number">0.129147</span> <span class="number">0.047550</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">665</span> <span class="number">0.121200</span> <span class="number">0.044580</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">666</span> <span class="number">0.113640</span> <span class="number">0.041759</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">667</span> <span class="number">0.106465</span> <span class="number">0.039085</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">668</span> <span class="number">0.099690</span> <span class="number">0.036564</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">669</span> <span class="number">0.093331</span> <span class="number">0.034200</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">670</span> <span class="number">0.087400</span> <span class="number">0.032000</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">671</span> <span class="number">0.081901</span> <span class="number">0.029963</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">672</span> <span class="number">0.076804</span> <span class="number">0.028077</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">673</span> <span class="number">0.072077</span> <span class="number">0.026329</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">674</span> <span class="number">0.067687</span> <span class="number">0.024708</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">675</span> <span class="number">0.063600</span> <span class="number">0.023200</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">676</span> <span class="number">0.059807</span> <span class="number">0.021801</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">677</span> <span class="number">0.056282</span> <span class="number">0.020501</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">678</span> <span class="number">0.052971</span> <span class="number">0.019281</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">679</span> <span class="number">0.049819</span> <span class="number">0.018121</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">680</span> <span class="number">0.046770</span> <span class="number">0.017000</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">681</span> <span class="number">0.043784</span> <span class="number">0.015904</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">682</span> <span class="number">0.040875</span> <span class="number">0.014837</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">683</span> <span class="number">0.038073</span> <span class="number">0.013811</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">684</span> <span class="number">0.035405</span> <span class="number">0.012835</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">685</span> <span class="number">0.032900</span> <span class="number">0.011920</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">686</span> <span class="number">0.030564</span> <span class="number">0.011068</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">687</span> <span class="number">0.028381</span> <span class="number">0.010273</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">688</span> <span class="number">0.026345</span> <span class="number">0.009533</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">689</span> <span class="number">0.024453</span> <span class="number">0.008846</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">690</span> <span class="number">0.022700</span> <span class="number">0.008210</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">691</span> <span class="number">0.021084</span> <span class="number">0.007624</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">692</span> <span class="number">0.019600</span> <span class="number">0.007085</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">693</span> <span class="number">0.018237</span> <span class="number">0.006591</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">694</span> <span class="number">0.016987</span> <span class="number">0.006138</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">695</span> <span class="number">0.015840</span> <span class="number">0.005723</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">696</span> <span class="number">0.014791</span> <span class="number">0.005343</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">697</span> <span class="number">0.013831</span> <span class="number">0.004996</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">698</span> <span class="number">0.012949</span> <span class="number">0.004676</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">699</span> <span class="number">0.012129</span> <span class="number">0.004380</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">700</span> <span class="number">0.011359</span> <span class="number">0.004102</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">701</span> <span class="number">0.010629</span> <span class="number">0.003838</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">702</span> <span class="number">0.009939</span> <span class="number">0.003589</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">703</span> <span class="number">0.009288</span> <span class="number">0.003354</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">704</span> <span class="number">0.008679</span> <span class="number">0.003134</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">705</span> <span class="number">0.008111</span> <span class="number">0.002929</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">706</span> <span class="number">0.007582</span> <span class="number">0.002738</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">707</span> <span class="number">0.007089</span> <span class="number">0.002560</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">708</span> <span class="number">0.006627</span> <span class="number">0.002393</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">709</span> <span class="number">0.006195</span> <span class="number">0.002237</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">710</span> <span class="number">0.005790</span> <span class="number">0.002091</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">711</span> <span class="number">0.005410</span> <span class="number">0.001954</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">712</span> <span class="number">0.005053</span> <span class="number">0.001825</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">713</span> <span class="number">0.004718</span> <span class="number">0.001704</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">714</span> <span class="number">0.004404</span> <span class="number">0.001590</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">715</span> <span class="number">0.004109</span> <span class="number">0.001484</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">716</span> <span class="number">0.003834</span> <span class="number">0.001384</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">717</span> <span class="number">0.003576</span> <span class="number">0.001291</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">718</span> <span class="number">0.003334</span> <span class="number">0.001204</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">719</span> <span class="number">0.003109</span> <span class="number">0.001123</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">720</span> <span class="number">0.002899</span> <span class="number">0.001047</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">721</span> <span class="number">0.002704</span> <span class="number">0.000977</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">722</span> <span class="number">0.002523</span> <span class="number">0.000911</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">723</span> <span class="number">0.002354</span> <span class="number">0.000850</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">724</span> <span class="number">0.002197</span> <span class="number">0.000793</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">725</span> <span class="number">0.002049</span> <span class="number">0.000740</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">726</span> <span class="number">0.001911</span> <span class="number">0.000690</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">727</span> <span class="number">0.001781</span> <span class="number">0.000643</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">728</span> <span class="number">0.001660</span> <span class="number">0.000599</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">729</span> <span class="number">0.001546</span> <span class="number">0.000558</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">730</span> <span class="number">0.001440</span> <span class="number">0.000520</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">731</span> <span class="number">0.001340</span> <span class="number">0.000484</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">732</span> <span class="number">0.001246</span> <span class="number">0.000450</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">733</span> <span class="number">0.001158</span> <span class="number">0.000418</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">734</span> <span class="number">0.001076</span> <span class="number">0.000389</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">735</span> <span class="number">0.001000</span> <span class="number">0.000361</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">736</span> <span class="number">0.000929</span> <span class="number">0.000335</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">737</span> <span class="number">0.000862</span> <span class="number">0.000311</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">738</span> <span class="number">0.000801</span> <span class="number">0.000289</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">739</span> <span class="number">0.000743</span> <span class="number">0.000268</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">740</span> <span class="number">0.000690</span> <span class="number">0.000249</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">741</span> <span class="number">0.000641</span> <span class="number">0.000231</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">742</span> <span class="number">0.000595</span> <span class="number">0.000215</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">743</span> <span class="number">0.000552</span> <span class="number">0.000199</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">744</span> <span class="number">0.000512</span> <span class="number">0.000185</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">745</span> <span class="number">0.000476</span> <span class="number">0.000172</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">746</span> <span class="number">0.000442</span> <span class="number">0.000160</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">747</span> <span class="number">0.000412</span> <span class="number">0.000149</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">748</span> <span class="number">0.000383</span> <span class="number">0.000138</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">749</span> <span class="number">0.000357</span> <span class="number">0.000129</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">750</span> <span class="number">0.000332</span> <span class="number">0.000120</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">751</span> <span class="number">0.000310</span> <span class="number">0.000112</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">752</span> <span class="number">0.000289</span> <span class="number">0.000104</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">753</span> <span class="number">0.000270</span> <span class="number">0.000097</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">754</span> <span class="number">0.000252</span> <span class="number">0.000091</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">755</span> <span class="number">0.000235</span> <span class="number">0.000085</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">756</span> <span class="number">0.000219</span> <span class="number">0.000079</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">757</span> <span class="number">0.000205</span> <span class="number">0.000074</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">758</span> <span class="number">0.000191</span> <span class="number">0.000069</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">759</span> <span class="number">0.000178</span> <span class="number">0.000064</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">760</span> <span class="number">0.000166</span> <span class="number">0.000060</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">761</span> <span class="number">0.000155</span> <span class="number">0.000056</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">762</span> <span class="number">0.000145</span> <span class="number">0.000052</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">763</span> <span class="number">0.000135</span> <span class="number">0.000049</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">764</span> <span class="number">0.000126</span> <span class="number">0.000045</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">765</span> <span class="number">0.000117</span> <span class="number">0.000042</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">766</span> <span class="number">0.000110</span> <span class="number">0.000040</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">767</span> <span class="number">0.000102</span> <span class="number">0.000037</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">768</span> <span class="number">0.000095</span> <span class="number">0.000034</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">769</span> <span class="number">0.000089</span> <span class="number">0.000032</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">770</span> <span class="number">0.000083</span> <span class="number">0.000030</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">771</span> <span class="number">0.000078</span> <span class="number">0.000028</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">772</span> <span class="number">0.000072</span> <span class="number">0.000026</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">773</span> <span class="number">0.000067</span> <span class="number">0.000024</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">774</span> <span class="number">0.000063</span> <span class="number">0.000023</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">775</span> <span class="number">0.000059</span> <span class="number">0.000021</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">776</span> <span class="number">0.000055</span> <span class="number">0.000020</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">777</span> <span class="number">0.000051</span> <span class="number">0.000018</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">778</span> <span class="number">0.000048</span> <span class="number">0.000017</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">779</span> <span class="number">0.000044</span> <span class="number">0.000016</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">780</span> <span class="number">0.000042</span> <span class="number">0.000015</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">781</span> <span class="number">0.000039</span> <span class="number">0.000014</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">782</span> <span class="number">0.000036</span> <span class="number">0.000013</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">783</span> <span class="number">0.000034</span> <span class="number">0.000012</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">784</span> <span class="number">0.000031</span> <span class="number">0.000011</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">785</span> <span class="number">0.000029</span> <span class="number">0.000011</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">786</span> <span class="number">0.000027</span> <span class="number">0.000010</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">787</span> <span class="number">0.000026</span> <span class="number">0.000009</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">788</span> <span class="number">0.000024</span> <span class="number">0.000009</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">789</span> <span class="number">0.000022</span> <span class="number">0.000008</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">790</span> <span class="number">0.000021</span> <span class="number">0.000007</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">791</span> <span class="number">0.000019</span> <span class="number">0.000007</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">792</span> <span class="number">0.000018</span> <span class="number">0.000006</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">793</span> <span class="number">0.000017</span> <span class="number">0.000006</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">794</span> <span class="number">0.000016</span> <span class="number">0.000006</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">795</span> <span class="number">0.000015</span> <span class="number">0.000005</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">796</span> <span class="number">0.000014</span> <span class="number">0.000005</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">797</span> <span class="number">0.000013</span> <span class="number">0.000005</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">798</span> <span class="number">0.000012</span> <span class="number">0.000004</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">799</span> <span class="number">0.000011</span> <span class="number">0.000004</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">800</span> <span class="number">0.000010</span> <span class="number">0.000004</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">801</span> <span class="number">0.000010</span> <span class="number">0.000003</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">802</span> <span class="number">0.000009</span> <span class="number">0.000003</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">803</span> <span class="number">0.000008</span> <span class="number">0.000003</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">804</span> <span class="number">0.000008</span> <span class="number">0.000003</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">805</span> <span class="number">0.000007</span> <span class="number">0.000003</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">806</span> <span class="number">0.000007</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">807</span> <span class="number">0.000006</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">808</span> <span class="number">0.000006</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">809</span> <span class="number">0.000005</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">810</span> <span class="number">0.000005</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">811</span> <span class="number">0.000005</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">812</span> <span class="number">0.000004</span> <span class="number">0.000002</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">813</span> <span class="number">0.000004</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">814</span> <span class="number">0.000004</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">815</span> <span class="number">0.000004</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">816</span> <span class="number">0.000003</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">817</span> <span class="number">0.000003</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">818</span> <span class="number">0.000003</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">819</span> <span class="number">0.000003</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">820</span> <span class="number">0.000003</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">821</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">822</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">823</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">824</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">825</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">826</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">827</span> <span class="number">0.000002</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">828</span> <span class="number">0.000001</span> <span class="number">0.000001</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">829</span> <span class="number">0.000001</span> <span class="number">0.000000</span> <span class="number">0.000000</span>;...</span><br><span class="line">            <span class="number">830</span> <span class="number">0.000001</span> <span class="number">0.000000</span> <span class="number">0.000000</span>];</span><br><span class="line">        wavelength = CIE31Table(:,<span class="number">1</span>);</span><br><span class="line">        xbar = CIE31Table(:,<span class="number">2</span>);</span><br><span class="line">        ybar = CIE31Table(:,<span class="number">3</span>);</span><br><span class="line">        zbar = CIE31Table(:,<span class="number">4</span>);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">% The spectral reflectance data of <span class="number">14</span> color test samples <span class="keyword">for</span> CRI</span><br><span class="line">% [wavelength (nm) TCS1 TCS2 TCS3 ... TCS14]</span><br><span class="line">TCS =  [<span class="number">360</span> <span class="number">116</span>  <span class="number">53</span>  <span class="number">58</span>  <span class="number">57</span> <span class="number">143</span>  <span class="number">79</span> <span class="number">150</span>  <span class="number">75</span>  <span class="number">69</span>  <span class="number">42</span>  <span class="number">74</span> <span class="number">189</span>  <span class="number">71</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">365</span> <span class="number">136</span>  <span class="number">55</span>  <span class="number">59</span>  <span class="number">59</span> <span class="number">187</span>  <span class="number">81</span> <span class="number">177</span>  <span class="number">78</span>  <span class="number">72</span>  <span class="number">43</span>  <span class="number">79</span> <span class="number">175</span>  <span class="number">76</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">370</span> <span class="number">159</span>  <span class="number">59</span>  <span class="number">61</span>  <span class="number">62</span> <span class="number">233</span>  <span class="number">89</span> <span class="number">218</span>  <span class="number">84</span>  <span class="number">73</span>  <span class="number">45</span>  <span class="number">86</span> <span class="number">158</span>  <span class="number">82</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">375</span> <span class="number">190</span>  <span class="number">64</span>  <span class="number">63</span>  <span class="number">67</span> <span class="number">269</span> <span class="number">113</span> <span class="number">293</span>  <span class="number">90</span>  <span class="number">70</span>  <span class="number">47</span>  <span class="number">98</span> <span class="number">139</span>  <span class="number">90</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">380</span> <span class="number">219</span>  <span class="number">70</span>  <span class="number">65</span>  <span class="number">74</span> <span class="number">295</span> <span class="number">151</span> <span class="number">378</span> <span class="number">104</span>  <span class="number">66</span>  <span class="number">50</span> <span class="number">111</span> <span class="number">120</span> <span class="number">104</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">385</span> <span class="number">239</span>  <span class="number">79</span>  <span class="number">68</span>  <span class="number">83</span> <span class="number">306</span> <span class="number">203</span> <span class="number">459</span> <span class="number">129</span>  <span class="number">62</span>  <span class="number">54</span> <span class="number">121</span> <span class="number">103</span> <span class="number">127</span>  <span class="number">36</span>;...</span><br><span class="line">        <span class="number">390</span> <span class="number">252</span>  <span class="number">89</span>  <span class="number">70</span>  <span class="number">93</span> <span class="number">310</span> <span class="number">265</span> <span class="number">524</span> <span class="number">170</span>  <span class="number">58</span>  <span class="number">59</span> <span class="number">127</span>  <span class="number">90</span> <span class="number">161</span>  <span class="number">37</span>;...</span><br><span class="line">        <span class="number">395</span> <span class="number">256</span> <span class="number">101</span>  <span class="number">72</span> <span class="number">105</span> <span class="number">312</span> <span class="number">339</span> <span class="number">546</span> <span class="number">240</span>  <span class="number">55</span>  <span class="number">63</span> <span class="number">129</span>  <span class="number">82</span> <span class="number">211</span>  <span class="number">38</span>;...</span><br><span class="line">        <span class="number">400</span> <span class="number">256</span> <span class="number">111</span>  <span class="number">73</span> <span class="number">116</span> <span class="number">313</span> <span class="number">410</span> <span class="number">551</span> <span class="number">319</span>  <span class="number">52</span>  <span class="number">66</span> <span class="number">127</span>  <span class="number">76</span> <span class="number">264</span>  <span class="number">39</span>;...</span><br><span class="line">        <span class="number">405</span> <span class="number">254</span> <span class="number">116</span>  <span class="number">73</span> <span class="number">121</span> <span class="number">315</span> <span class="number">464</span> <span class="number">555</span> <span class="number">416</span>  <span class="number">52</span>  <span class="number">67</span> <span class="number">121</span>  <span class="number">68</span> <span class="number">313</span>  <span class="number">39</span>;...</span><br><span class="line">        <span class="number">410</span> <span class="number">252</span> <span class="number">118</span>  <span class="number">74</span> <span class="number">124</span> <span class="number">319</span> <span class="number">492</span> <span class="number">559</span> <span class="number">462</span>  <span class="number">51</span>  <span class="number">68</span> <span class="number">116</span>  <span class="number">64</span> <span class="number">341</span>  <span class="number">40</span>;...</span><br><span class="line">        <span class="number">415</span> <span class="number">248</span> <span class="number">120</span>  <span class="number">74</span> <span class="number">126</span> <span class="number">322</span> <span class="number">508</span> <span class="number">560</span> <span class="number">482</span>  <span class="number">50</span>  <span class="number">69</span> <span class="number">112</span>  <span class="number">65</span> <span class="number">352</span>  <span class="number">41</span>;...</span><br><span class="line">        <span class="number">420</span> <span class="number">244</span> <span class="number">121</span>  <span class="number">74</span> <span class="number">128</span> <span class="number">326</span> <span class="number">517</span> <span class="number">561</span> <span class="number">490</span>  <span class="number">50</span>  <span class="number">69</span> <span class="number">108</span>  <span class="number">75</span> <span class="number">359</span>  <span class="number">42</span>;...</span><br><span class="line">        <span class="number">425</span> <span class="number">240</span> <span class="number">122</span>  <span class="number">73</span> <span class="number">131</span> <span class="number">330</span> <span class="number">524</span> <span class="number">558</span> <span class="number">488</span>  <span class="number">49</span>  <span class="number">70</span> <span class="number">105</span>  <span class="number">93</span> <span class="number">361</span>  <span class="number">42</span>;...</span><br><span class="line">        <span class="number">430</span> <span class="number">237</span> <span class="number">122</span>  <span class="number">73</span> <span class="number">135</span> <span class="number">334</span> <span class="number">531</span> <span class="number">556</span> <span class="number">482</span>  <span class="number">48</span>  <span class="number">72</span> <span class="number">104</span> <span class="number">123</span> <span class="number">364</span>  <span class="number">43</span>;...</span><br><span class="line">        <span class="number">435</span> <span class="number">232</span> <span class="number">122</span>  <span class="number">73</span> <span class="number">139</span> <span class="number">339</span> <span class="number">538</span> <span class="number">551</span> <span class="number">473</span>  <span class="number">47</span>  <span class="number">73</span> <span class="number">104</span> <span class="number">160</span> <span class="number">365</span>  <span class="number">44</span>;...</span><br><span class="line">        <span class="number">440</span> <span class="number">230</span> <span class="number">123</span>  <span class="number">73</span> <span class="number">144</span> <span class="number">346</span> <span class="number">544</span> <span class="number">544</span> <span class="number">462</span>  <span class="number">46</span>  <span class="number">76</span> <span class="number">105</span> <span class="number">207</span> <span class="number">367</span>  <span class="number">44</span>;...</span><br><span class="line">        <span class="number">445</span> <span class="number">226</span> <span class="number">124</span>  <span class="number">73</span> <span class="number">151</span> <span class="number">352</span> <span class="number">551</span> <span class="number">535</span> <span class="number">450</span>  <span class="number">44</span>  <span class="number">78</span> <span class="number">106</span> <span class="number">256</span> <span class="number">369</span>  <span class="number">45</span>;...</span><br><span class="line">        <span class="number">450</span> <span class="number">225</span> <span class="number">127</span>  <span class="number">74</span> <span class="number">161</span> <span class="number">360</span> <span class="number">556</span> <span class="number">522</span> <span class="number">439</span>  <span class="number">42</span>  <span class="number">83</span> <span class="number">110</span> <span class="number">300</span> <span class="number">372</span>  <span class="number">45</span>;...</span><br><span class="line">        <span class="number">455</span> <span class="number">222</span> <span class="number">128</span>  <span class="number">75</span> <span class="number">172</span> <span class="number">369</span> <span class="number">556</span> <span class="number">506</span> <span class="number">426</span>  <span class="number">41</span>  <span class="number">88</span> <span class="number">115</span> <span class="number">331</span> <span class="number">374</span>  <span class="number">46</span>;...</span><br><span class="line">        <span class="number">460</span> <span class="number">220</span> <span class="number">131</span>  <span class="number">77</span> <span class="number">186</span> <span class="number">381</span> <span class="number">554</span> <span class="number">488</span> <span class="number">413</span>  <span class="number">38</span>  <span class="number">95</span> <span class="number">123</span> <span class="number">346</span> <span class="number">376</span>  <span class="number">47</span>;...</span><br><span class="line">        <span class="number">465</span> <span class="number">218</span> <span class="number">134</span>  <span class="number">80</span> <span class="number">205</span> <span class="number">394</span> <span class="number">549</span> <span class="number">469</span> <span class="number">397</span>  <span class="number">35</span> <span class="number">103</span> <span class="number">134</span> <span class="number">347</span> <span class="number">379</span>  <span class="number">48</span>;...</span><br><span class="line">        <span class="number">470</span> <span class="number">216</span> <span class="number">138</span>  <span class="number">85</span> <span class="number">229</span> <span class="number">403</span> <span class="number">541</span> <span class="number">448</span> <span class="number">382</span>  <span class="number">33</span> <span class="number">113</span> <span class="number">148</span> <span class="number">341</span> <span class="number">384</span>  <span class="number">50</span>;...</span><br><span class="line">        <span class="number">475</span> <span class="number">214</span> <span class="number">143</span>  <span class="number">94</span> <span class="number">254</span> <span class="number">410</span> <span class="number">531</span> <span class="number">429</span> <span class="number">366</span>  <span class="number">31</span> <span class="number">125</span> <span class="number">167</span> <span class="number">328</span> <span class="number">389</span>  <span class="number">52</span>;...</span><br><span class="line">        <span class="number">480</span> <span class="number">214</span> <span class="number">150</span> <span class="number">109</span> <span class="number">281</span> <span class="number">415</span> <span class="number">519</span> <span class="number">408</span> <span class="number">352</span>  <span class="number">30</span> <span class="number">142</span> <span class="number">192</span> <span class="number">307</span> <span class="number">397</span>  <span class="number">55</span>;...</span><br><span class="line">        <span class="number">485</span> <span class="number">214</span> <span class="number">159</span> <span class="number">126</span> <span class="number">308</span> <span class="number">418</span> <span class="number">504</span> <span class="number">385</span> <span class="number">337</span>  <span class="number">29</span> <span class="number">162</span> <span class="number">219</span> <span class="number">282</span> <span class="number">405</span>  <span class="number">57</span>;...</span><br><span class="line">        <span class="number">490</span> <span class="number">216</span> <span class="number">174</span> <span class="number">148</span> <span class="number">332</span> <span class="number">419</span> <span class="number">488</span> <span class="number">363</span> <span class="number">325</span>  <span class="number">28</span> <span class="number">189</span> <span class="number">252</span> <span class="number">257</span> <span class="number">416</span>  <span class="number">62</span>;...</span><br><span class="line">        <span class="number">495</span> <span class="number">218</span> <span class="number">190</span> <span class="number">172</span> <span class="number">352</span> <span class="number">417</span> <span class="number">469</span> <span class="number">341</span> <span class="number">310</span>  <span class="number">28</span> <span class="number">219</span> <span class="number">291</span> <span class="number">230</span> <span class="number">429</span>  <span class="number">67</span>;...</span><br><span class="line">        <span class="number">500</span> <span class="number">223</span> <span class="number">207</span> <span class="number">198</span> <span class="number">370</span> <span class="number">413</span> <span class="number">450</span> <span class="number">324</span> <span class="number">299</span>  <span class="number">28</span> <span class="number">262</span> <span class="number">325</span> <span class="number">204</span> <span class="number">443</span>  <span class="number">75</span>;...</span><br><span class="line">        <span class="number">505</span> <span class="number">225</span> <span class="number">225</span> <span class="number">221</span> <span class="number">383</span> <span class="number">409</span> <span class="number">431</span> <span class="number">311</span> <span class="number">289</span>  <span class="number">29</span> <span class="number">305</span> <span class="number">347</span> <span class="number">178</span> <span class="number">454</span>  <span class="number">83</span>;...</span><br><span class="line">        <span class="number">510</span> <span class="number">226</span> <span class="number">242</span> <span class="number">241</span> <span class="number">390</span> <span class="number">403</span> <span class="number">414</span> <span class="number">301</span> <span class="number">283</span>  <span class="number">30</span> <span class="number">365</span> <span class="number">356</span> <span class="number">154</span> <span class="number">461</span>  <span class="number">92</span>;...</span><br><span class="line">        <span class="number">515</span> <span class="number">226</span> <span class="number">253</span> <span class="number">260</span> <span class="number">394</span> <span class="number">396</span> <span class="number">395</span> <span class="number">291</span> <span class="number">276</span>  <span class="number">30</span> <span class="number">416</span> <span class="number">353</span> <span class="number">129</span> <span class="number">466</span> <span class="number">100</span>;...</span><br><span class="line">        <span class="number">520</span> <span class="number">225</span> <span class="number">260</span> <span class="number">278</span> <span class="number">395</span> <span class="number">389</span> <span class="number">377</span> <span class="number">283</span> <span class="number">270</span>  <span class="number">31</span> <span class="number">465</span> <span class="number">346</span> <span class="number">109</span> <span class="number">469</span> <span class="number">108</span>;...</span><br><span class="line">        <span class="number">525</span> <span class="number">225</span> <span class="number">264</span> <span class="number">302</span> <span class="number">392</span> <span class="number">381</span> <span class="number">358</span> <span class="number">273</span> <span class="number">262</span>  <span class="number">31</span> <span class="number">509</span> <span class="number">333</span>  <span class="number">90</span> <span class="number">471</span> <span class="number">121</span>;...</span><br><span class="line">        <span class="number">530</span> <span class="number">227</span> <span class="number">267</span> <span class="number">339</span> <span class="number">385</span> <span class="number">372</span> <span class="number">341</span> <span class="number">265</span> <span class="number">256</span>  <span class="number">32</span> <span class="number">546</span> <span class="number">314</span>  <span class="number">75</span> <span class="number">474</span> <span class="number">133</span>;...</span><br><span class="line">        <span class="number">535</span> <span class="number">230</span> <span class="number">269</span> <span class="number">370</span> <span class="number">377</span> <span class="number">363</span> <span class="number">325</span> <span class="number">260</span> <span class="number">251</span>  <span class="number">32</span> <span class="number">581</span> <span class="number">294</span>  <span class="number">62</span> <span class="number">476</span> <span class="number">142</span>;...</span><br><span class="line">        <span class="number">540</span> <span class="number">236</span> <span class="number">272</span> <span class="number">392</span> <span class="number">367</span> <span class="number">353</span> <span class="number">309</span> <span class="number">257</span> <span class="number">250</span>  <span class="number">33</span> <span class="number">610</span> <span class="number">271</span>  <span class="number">51</span> <span class="number">483</span> <span class="number">150</span>;...</span><br><span class="line">        <span class="number">545</span> <span class="number">245</span> <span class="number">276</span> <span class="number">399</span> <span class="number">354</span> <span class="number">342</span> <span class="number">293</span> <span class="number">257</span> <span class="number">251</span>  <span class="number">34</span> <span class="number">634</span> <span class="number">248</span>  <span class="number">41</span> <span class="number">490</span> <span class="number">154</span>;...</span><br><span class="line">        <span class="number">550</span> <span class="number">253</span> <span class="number">282</span> <span class="number">400</span> <span class="number">341</span> <span class="number">331</span> <span class="number">279</span> <span class="number">259</span> <span class="number">254</span>  <span class="number">35</span> <span class="number">653</span> <span class="number">227</span>  <span class="number">35</span> <span class="number">506</span> <span class="number">155</span>;...</span><br><span class="line">        <span class="number">555</span> <span class="number">262</span> <span class="number">289</span> <span class="number">393</span> <span class="number">327</span> <span class="number">320</span> <span class="number">265</span> <span class="number">260</span> <span class="number">258</span>  <span class="number">37</span> <span class="number">666</span> <span class="number">206</span>  <span class="number">29</span> <span class="number">526</span> <span class="number">152</span>;...</span><br><span class="line">        <span class="number">560</span> <span class="number">272</span> <span class="number">299</span> <span class="number">380</span> <span class="number">312</span> <span class="number">308</span> <span class="number">253</span> <span class="number">260</span> <span class="number">264</span>  <span class="number">41</span> <span class="number">678</span> <span class="number">188</span>  <span class="number">25</span> <span class="number">553</span> <span class="number">147</span>;...</span><br><span class="line">        <span class="number">565</span> <span class="number">283</span> <span class="number">309</span> <span class="number">365</span> <span class="number">296</span> <span class="number">296</span> <span class="number">241</span> <span class="number">258</span> <span class="number">269</span>  <span class="number">44</span> <span class="number">687</span> <span class="number">170</span>  <span class="number">22</span> <span class="number">582</span> <span class="number">140</span>;...</span><br><span class="line">        <span class="number">570</span> <span class="number">298</span> <span class="number">322</span> <span class="number">349</span> <span class="number">280</span> <span class="number">284</span> <span class="number">234</span> <span class="number">256</span> <span class="number">272</span>  <span class="number">48</span> <span class="number">693</span> <span class="number">153</span>  <span class="number">19</span> <span class="number">618</span> <span class="number">133</span>;...</span><br><span class="line">        <span class="number">575</span> <span class="number">318</span> <span class="number">329</span> <span class="number">332</span> <span class="number">263</span> <span class="number">271</span> <span class="number">227</span> <span class="number">254</span> <span class="number">274</span>  <span class="number">52</span> <span class="number">698</span> <span class="number">138</span>  <span class="number">17</span> <span class="number">651</span> <span class="number">125</span>;...</span><br><span class="line">        <span class="number">580</span> <span class="number">341</span> <span class="number">335</span> <span class="number">315</span> <span class="number">247</span> <span class="number">260</span> <span class="number">225</span> <span class="number">254</span> <span class="number">278</span>  <span class="number">60</span> <span class="number">701</span> <span class="number">125</span>  <span class="number">17</span> <span class="number">680</span> <span class="number">118</span>;...</span><br><span class="line">        <span class="number">585</span> <span class="number">367</span> <span class="number">339</span> <span class="number">299</span> <span class="number">229</span> <span class="number">247</span> <span class="number">222</span> <span class="number">259</span> <span class="number">284</span>  <span class="number">76</span> <span class="number">704</span> <span class="number">114</span>  <span class="number">17</span> <span class="number">701</span> <span class="number">112</span>;...</span><br><span class="line">        <span class="number">590</span> <span class="number">390</span> <span class="number">341</span> <span class="number">285</span> <span class="number">214</span> <span class="number">232</span> <span class="number">221</span> <span class="number">270</span> <span class="number">295</span> <span class="number">102</span> <span class="number">705</span> <span class="number">106</span>  <span class="number">16</span> <span class="number">717</span> <span class="number">106</span>;...</span><br><span class="line">        <span class="number">595</span> <span class="number">409</span> <span class="number">341</span> <span class="number">272</span> <span class="number">198</span> <span class="number">220</span> <span class="number">220</span> <span class="number">284</span> <span class="number">316</span> <span class="number">136</span> <span class="number">705</span> <span class="number">100</span>  <span class="number">16</span> <span class="number">729</span> <span class="number">101</span>;...</span><br><span class="line">        <span class="number">600</span> <span class="number">424</span> <span class="number">342</span> <span class="number">264</span> <span class="number">185</span> <span class="number">210</span> <span class="number">220</span> <span class="number">302</span> <span class="number">348</span> <span class="number">190</span> <span class="number">706</span>  <span class="number">96</span>  <span class="number">16</span> <span class="number">736</span>  <span class="number">98</span>;...</span><br><span class="line">        <span class="number">605</span> <span class="number">435</span> <span class="number">342</span> <span class="number">257</span> <span class="number">175</span> <span class="number">200</span> <span class="number">220</span> <span class="number">324</span> <span class="number">384</span> <span class="number">256</span> <span class="number">707</span>  <span class="number">92</span>  <span class="number">16</span> <span class="number">742</span>  <span class="number">95</span>;...</span><br><span class="line">        <span class="number">610</span> <span class="number">442</span> <span class="number">342</span> <span class="number">252</span> <span class="number">169</span> <span class="number">194</span> <span class="number">220</span> <span class="number">344</span> <span class="number">434</span> <span class="number">336</span> <span class="number">707</span>  <span class="number">90</span>  <span class="number">16</span> <span class="number">745</span>  <span class="number">93</span>;...</span><br><span class="line">        <span class="number">615</span> <span class="number">448</span> <span class="number">341</span> <span class="number">247</span> <span class="number">164</span> <span class="number">189</span> <span class="number">220</span> <span class="number">362</span> <span class="number">482</span> <span class="number">418</span> <span class="number">707</span>  <span class="number">87</span>  <span class="number">16</span> <span class="number">747</span>  <span class="number">90</span>;...</span><br><span class="line">        <span class="number">620</span> <span class="number">450</span> <span class="number">341</span> <span class="number">241</span> <span class="number">160</span> <span class="number">185</span> <span class="number">223</span> <span class="number">377</span> <span class="number">528</span> <span class="number">505</span> <span class="number">708</span>  <span class="number">85</span>  <span class="number">16</span> <span class="number">748</span>  <span class="number">89</span>;...</span><br><span class="line">        <span class="number">625</span> <span class="number">451</span> <span class="number">339</span> <span class="number">235</span> <span class="number">156</span> <span class="number">183</span> <span class="number">227</span> <span class="number">389</span> <span class="number">568</span> <span class="number">581</span> <span class="number">708</span>  <span class="number">82</span>  <span class="number">16</span> <span class="number">748</span>  <span class="number">87</span>;...</span><br><span class="line">        <span class="number">630</span> <span class="number">451</span> <span class="number">339</span> <span class="number">229</span> <span class="number">154</span> <span class="number">180</span> <span class="number">233</span> <span class="number">400</span> <span class="number">604</span> <span class="number">641</span> <span class="number">710</span>  <span class="number">80</span>  <span class="number">18</span> <span class="number">748</span>  <span class="number">86</span>;...</span><br><span class="line">        <span class="number">635</span> <span class="number">451</span> <span class="number">338</span> <span class="number">224</span> <span class="number">152</span> <span class="number">177</span> <span class="number">239</span> <span class="number">410</span> <span class="number">629</span> <span class="number">682</span> <span class="number">711</span>  <span class="number">79</span>  <span class="number">18</span> <span class="number">748</span>  <span class="number">85</span>;...</span><br><span class="line">        <span class="number">640</span> <span class="number">451</span> <span class="number">338</span> <span class="number">220</span> <span class="number">151</span> <span class="number">176</span> <span class="number">244</span> <span class="number">420</span> <span class="number">648</span> <span class="number">717</span> <span class="number">712</span>  <span class="number">78</span>  <span class="number">18</span> <span class="number">748</span>  <span class="number">84</span>;...</span><br><span class="line">        <span class="number">645</span> <span class="number">451</span> <span class="number">337</span> <span class="number">217</span> <span class="number">149</span> <span class="number">175</span> <span class="number">251</span> <span class="number">429</span> <span class="number">663</span> <span class="number">740</span> <span class="number">714</span>  <span class="number">78</span>  <span class="number">18</span> <span class="number">748</span>  <span class="number">84</span>;...</span><br><span class="line">        <span class="number">650</span> <span class="number">450</span> <span class="number">336</span> <span class="number">216</span> <span class="number">148</span> <span class="number">175</span> <span class="number">258</span> <span class="number">438</span> <span class="number">676</span> <span class="number">758</span> <span class="number">716</span>  <span class="number">78</span>  <span class="number">19</span> <span class="number">748</span>  <span class="number">84</span>;...</span><br><span class="line">        <span class="number">655</span> <span class="number">450</span> <span class="number">335</span> <span class="number">216</span> <span class="number">148</span> <span class="number">175</span> <span class="number">263</span> <span class="number">445</span> <span class="number">685</span> <span class="number">770</span> <span class="number">718</span>  <span class="number">78</span>  <span class="number">20</span> <span class="number">748</span>  <span class="number">84</span>;...</span><br><span class="line">        <span class="number">660</span> <span class="number">451</span> <span class="number">334</span> <span class="number">219</span> <span class="number">148</span> <span class="number">175</span> <span class="number">268</span> <span class="number">452</span> <span class="number">693</span> <span class="number">781</span> <span class="number">720</span>  <span class="number">81</span>  <span class="number">23</span> <span class="number">747</span>  <span class="number">85</span>;...</span><br><span class="line">        <span class="number">665</span> <span class="number">451</span> <span class="number">332</span> <span class="number">224</span> <span class="number">149</span> <span class="number">177</span> <span class="number">273</span> <span class="number">457</span> <span class="number">700</span> <span class="number">790</span> <span class="number">722</span>  <span class="number">83</span>  <span class="number">24</span> <span class="number">747</span>  <span class="number">87</span>;...</span><br><span class="line">        <span class="number">670</span> <span class="number">453</span> <span class="number">332</span> <span class="number">230</span> <span class="number">151</span> <span class="number">180</span> <span class="number">278</span> <span class="number">462</span> <span class="number">705</span> <span class="number">797</span> <span class="number">725</span>  <span class="number">88</span>  <span class="number">26</span> <span class="number">747</span>  <span class="number">92</span>;...</span><br><span class="line">        <span class="number">675</span> <span class="number">454</span> <span class="number">331</span> <span class="number">238</span> <span class="number">154</span> <span class="number">183</span> <span class="number">281</span> <span class="number">466</span> <span class="number">709</span> <span class="number">803</span> <span class="number">729</span>  <span class="number">93</span>  <span class="number">30</span> <span class="number">747</span>  <span class="number">96</span>;...</span><br><span class="line">        <span class="number">680</span> <span class="number">455</span> <span class="number">331</span> <span class="number">251</span> <span class="number">158</span> <span class="number">186</span> <span class="number">283</span> <span class="number">468</span> <span class="number">712</span> <span class="number">809</span> <span class="number">731</span> <span class="number">102</span>  <span class="number">35</span> <span class="number">747</span> <span class="number">102</span>;...</span><br><span class="line">        <span class="number">685</span> <span class="number">457</span> <span class="number">330</span> <span class="number">269</span> <span class="number">162</span> <span class="number">189</span> <span class="number">286</span> <span class="number">470</span> <span class="number">715</span> <span class="number">814</span> <span class="number">735</span> <span class="number">112</span>  <span class="number">43</span> <span class="number">747</span> <span class="number">110</span>;...</span><br><span class="line">        <span class="number">690</span> <span class="number">458</span> <span class="number">329</span> <span class="number">288</span> <span class="number">165</span> <span class="number">192</span> <span class="number">291</span> <span class="number">473</span> <span class="number">717</span> <span class="number">819</span> <span class="number">739</span> <span class="number">125</span>  <span class="number">56</span> <span class="number">747</span> <span class="number">123</span>;...</span><br><span class="line">        <span class="number">695</span> <span class="number">460</span> <span class="number">328</span> <span class="number">312</span> <span class="number">168</span> <span class="number">195</span> <span class="number">296</span> <span class="number">477</span> <span class="number">719</span> <span class="number">824</span> <span class="number">742</span> <span class="number">141</span>  <span class="number">74</span> <span class="number">746</span> <span class="number">137</span>;...</span><br><span class="line">        <span class="number">700</span> <span class="number">462</span> <span class="number">328</span> <span class="number">340</span> <span class="number">170</span> <span class="number">199</span> <span class="number">302</span> <span class="number">483</span> <span class="number">721</span> <span class="number">828</span> <span class="number">746</span> <span class="number">161</span>  <span class="number">97</span> <span class="number">746</span> <span class="number">152</span>;...</span><br><span class="line">        <span class="number">705</span> <span class="number">463</span> <span class="number">327</span> <span class="number">366</span> <span class="number">171</span> <span class="number">200</span> <span class="number">313</span> <span class="number">489</span> <span class="number">720</span> <span class="number">830</span> <span class="number">748</span> <span class="number">182</span> <span class="number">128</span> <span class="number">746</span> <span class="number">169</span>;...</span><br><span class="line">        <span class="number">710</span> <span class="number">464</span> <span class="number">326</span> <span class="number">390</span> <span class="number">170</span> <span class="number">199</span> <span class="number">325</span> <span class="number">496</span> <span class="number">719</span> <span class="number">831</span> <span class="number">749</span> <span class="number">203</span> <span class="number">166</span> <span class="number">745</span> <span class="number">188</span>;...</span><br><span class="line">        <span class="number">715</span> <span class="number">465</span> <span class="number">325</span> <span class="number">412</span> <span class="number">168</span> <span class="number">198</span> <span class="number">338</span> <span class="number">503</span> <span class="number">722</span> <span class="number">833</span> <span class="number">751</span> <span class="number">223</span> <span class="number">210</span> <span class="number">744</span> <span class="number">207</span>;...</span><br><span class="line">        <span class="number">720</span> <span class="number">466</span> <span class="number">324</span> <span class="number">431</span> <span class="number">166</span> <span class="number">196</span> <span class="number">351</span> <span class="number">511</span> <span class="number">725</span> <span class="number">835</span> <span class="number">753</span> <span class="number">242</span> <span class="number">257</span> <span class="number">743</span> <span class="number">226</span>;...</span><br><span class="line">        <span class="number">725</span> <span class="number">466</span> <span class="number">324</span> <span class="number">447</span> <span class="number">164</span> <span class="number">195</span> <span class="number">364</span> <span class="number">518</span> <span class="number">727</span> <span class="number">836</span> <span class="number">754</span> <span class="number">257</span> <span class="number">305</span> <span class="number">744</span> <span class="number">243</span>;...</span><br><span class="line">        <span class="number">730</span> <span class="number">466</span> <span class="number">324</span> <span class="number">460</span> <span class="number">164</span> <span class="number">195</span> <span class="number">376</span> <span class="number">525</span> <span class="number">729</span> <span class="number">836</span> <span class="number">755</span> <span class="number">270</span> <span class="number">354</span> <span class="number">745</span> <span class="number">260</span>;...</span><br><span class="line">        <span class="number">735</span> <span class="number">466</span> <span class="number">323</span> <span class="number">472</span> <span class="number">165</span> <span class="number">196</span> <span class="number">389</span> <span class="number">532</span> <span class="number">730</span> <span class="number">837</span> <span class="number">755</span> <span class="number">282</span> <span class="number">401</span> <span class="number">748</span> <span class="number">277</span>;...</span><br><span class="line">        <span class="number">740</span> <span class="number">467</span> <span class="number">322</span> <span class="number">481</span> <span class="number">168</span> <span class="number">197</span> <span class="number">401</span> <span class="number">539</span> <span class="number">730</span> <span class="number">838</span> <span class="number">755</span> <span class="number">292</span> <span class="number">446</span> <span class="number">750</span> <span class="number">294</span>;...</span><br><span class="line">        <span class="number">745</span> <span class="number">467</span> <span class="number">321</span> <span class="number">488</span> <span class="number">172</span> <span class="number">200</span> <span class="number">413</span> <span class="number">546</span> <span class="number">730</span> <span class="number">839</span> <span class="number">755</span> <span class="number">302</span> <span class="number">485</span> <span class="number">750</span> <span class="number">310</span>;...</span><br><span class="line">        <span class="number">750</span> <span class="number">467</span> <span class="number">320</span> <span class="number">493</span> <span class="number">177</span> <span class="number">203</span> <span class="number">425</span> <span class="number">553</span> <span class="number">730</span> <span class="number">839</span> <span class="number">756</span> <span class="number">310</span> <span class="number">520</span> <span class="number">749</span> <span class="number">325</span>;...</span><br><span class="line">        <span class="number">755</span> <span class="number">467</span> <span class="number">318</span> <span class="number">497</span> <span class="number">181</span> <span class="number">205</span> <span class="number">436</span> <span class="number">559</span> <span class="number">730</span> <span class="number">839</span> <span class="number">757</span> <span class="number">314</span> <span class="number">551</span> <span class="number">748</span> <span class="number">339</span>;...</span><br><span class="line">        <span class="number">760</span> <span class="number">467</span> <span class="number">316</span> <span class="number">500</span> <span class="number">185</span> <span class="number">208</span> <span class="number">447</span> <span class="number">565</span> <span class="number">730</span> <span class="number">839</span> <span class="number">758</span> <span class="number">317</span> <span class="number">577</span> <span class="number">748</span> <span class="number">353</span>;...</span><br><span class="line">        <span class="number">765</span> <span class="number">467</span> <span class="number">315</span> <span class="number">502</span> <span class="number">189</span> <span class="number">212</span> <span class="number">458</span> <span class="number">570</span> <span class="number">730</span> <span class="number">839</span> <span class="number">759</span> <span class="number">323</span> <span class="number">599</span> <span class="number">747</span> <span class="number">366</span>;...</span><br><span class="line">        <span class="number">770</span> <span class="number">467</span> <span class="number">315</span> <span class="number">505</span> <span class="number">192</span> <span class="number">215</span> <span class="number">469</span> <span class="number">575</span> <span class="number">730</span> <span class="number">839</span> <span class="number">759</span> <span class="number">330</span> <span class="number">618</span> <span class="number">747</span> <span class="number">379</span>;...</span><br><span class="line">        <span class="number">775</span> <span class="number">467</span> <span class="number">314</span> <span class="number">510</span> <span class="number">194</span> <span class="number">217</span> <span class="number">477</span> <span class="number">578</span> <span class="number">730</span> <span class="number">839</span> <span class="number">759</span> <span class="number">334</span> <span class="number">633</span> <span class="number">747</span> <span class="number">390</span>;...</span><br><span class="line">        <span class="number">780</span> <span class="number">467</span> <span class="number">314</span> <span class="number">516</span> <span class="number">197</span> <span class="number">219</span> <span class="number">485</span> <span class="number">581</span> <span class="number">730</span> <span class="number">839</span> <span class="number">759</span> <span class="number">338</span> <span class="number">645</span> <span class="number">747</span> <span class="number">399</span>;...</span><br><span class="line">        <span class="number">785</span> <span class="number">467</span> <span class="number">313</span> <span class="number">520</span> <span class="number">200</span> <span class="number">222</span> <span class="number">493</span> <span class="number">583</span> <span class="number">730</span> <span class="number">839</span> <span class="number">759</span> <span class="number">343</span> <span class="number">656</span> <span class="number">746</span> <span class="number">408</span>;...</span><br><span class="line">        <span class="number">790</span> <span class="number">467</span> <span class="number">313</span> <span class="number">524</span> <span class="number">204</span> <span class="number">226</span> <span class="number">500</span> <span class="number">585</span> <span class="number">731</span> <span class="number">839</span> <span class="number">759</span> <span class="number">348</span> <span class="number">666</span> <span class="number">746</span> <span class="number">416</span>;...</span><br><span class="line">        <span class="number">795</span> <span class="number">466</span> <span class="number">312</span> <span class="number">527</span> <span class="number">210</span> <span class="number">231</span> <span class="number">506</span> <span class="number">587</span> <span class="number">731</span> <span class="number">839</span> <span class="number">759</span> <span class="number">353</span> <span class="number">674</span> <span class="number">746</span> <span class="number">422</span>;...</span><br><span class="line">        <span class="number">800</span> <span class="number">466</span> <span class="number">312</span> <span class="number">531</span> <span class="number">218</span> <span class="number">237</span> <span class="number">512</span> <span class="number">588</span> <span class="number">731</span> <span class="number">839</span> <span class="number">759</span> <span class="number">359</span> <span class="number">680</span> <span class="number">746</span> <span class="number">428</span>;...</span><br><span class="line">        <span class="number">805</span> <span class="number">466</span> <span class="number">311</span> <span class="number">535</span> <span class="number">225</span> <span class="number">243</span> <span class="number">517</span> <span class="number">589</span> <span class="number">731</span> <span class="number">839</span> <span class="number">759</span> <span class="number">365</span> <span class="number">686</span> <span class="number">745</span> <span class="number">434</span>;...</span><br><span class="line">        <span class="number">810</span> <span class="number">466</span> <span class="number">311</span> <span class="number">539</span> <span class="number">233</span> <span class="number">249</span> <span class="number">521</span> <span class="number">590</span> <span class="number">731</span> <span class="number">838</span> <span class="number">758</span> <span class="number">372</span> <span class="number">691</span> <span class="number">745</span> <span class="number">439</span>;...</span><br><span class="line">        <span class="number">815</span> <span class="number">466</span> <span class="number">311</span> <span class="number">544</span> <span class="number">243</span> <span class="number">257</span> <span class="number">525</span> <span class="number">590</span> <span class="number">731</span> <span class="number">837</span> <span class="number">757</span> <span class="number">380</span> <span class="number">694</span> <span class="number">745</span> <span class="number">444</span>;...</span><br><span class="line">        <span class="number">820</span> <span class="number">465</span> <span class="number">311</span> <span class="number">548</span> <span class="number">254</span> <span class="number">265</span> <span class="number">529</span> <span class="number">590</span> <span class="number">731</span> <span class="number">837</span> <span class="number">757</span> <span class="number">388</span> <span class="number">697</span> <span class="number">745</span> <span class="number">448</span>;...</span><br><span class="line">        <span class="number">825</span> <span class="number">464</span> <span class="number">311</span> <span class="number">552</span> <span class="number">264</span> <span class="number">273</span> <span class="number">532</span> <span class="number">591</span> <span class="number">731</span> <span class="number">836</span> <span class="number">756</span> <span class="number">396</span> <span class="number">700</span> <span class="number">745</span> <span class="number">451</span>;...</span><br><span class="line">        <span class="number">830</span> <span class="number">464</span> <span class="number">310</span> <span class="number">555</span> <span class="number">274</span> <span class="number">280</span> <span class="number">535</span> <span class="number">592</span> <span class="number">731</span> <span class="number">836</span> <span class="number">756</span> <span class="number">403</span> <span class="number">702</span> <span class="number">745</span> <span class="number">454</span>];</span><br><span class="line">TCS(:,<span class="number">2</span>:end) = TCS(:,<span class="number">2</span>:end)/<span class="number">1000</span>;        </span><br><span class="line">        </span><br><span class="line">% Data <span class="keyword">for</span> isotemperature lines needed <span class="keyword">for</span> calculating correlated color temperature</span><br><span class="line"></span><br><span class="line">% The following provides a table of isotemperature lines <span class="keyword">for</span> use <span class="keyword">with</span> the Robertson Method</span><br><span class="line">% (Robertson, <span class="number">1968</span>) to interpolate isotemperature lines <span class="keyword">from</span> the CIE <span class="number">1960</span> UCS.</span><br><span class="line">% The spacing of the isotemp lines <span class="keyword">is</span> very small (<span class="number">1</span> <span class="number">1</span>/MK) so very little</span><br><span class="line">% interpolation <span class="keyword">is</span> actually needed <span class="keyword">for</span> determining CCT. The latest (<span class="number">2002</span>)</span><br><span class="line">% recommended values <span class="keyword">for</span> the physical constants determining blackbody</span><br><span class="line">% radiation spectra are used</span><br><span class="line"></span><br><span class="line">dwave = wavelength(<span class="number">2</span>)-wavelength(<span class="number">1</span>); % wavelength increment = <span class="number">1</span> nm</span><br><span class="line"></span><br><span class="line">ubar = (<span class="number">2</span>/<span class="number">3</span>)*xbar;</span><br><span class="line">vbar = ybar;</span><br><span class="line">wbar = -<span class="number">0.5</span>*xbar + (<span class="number">3</span>/<span class="number">2</span>)*ybar + <span class="number">0.5</span>*zbar;</span><br><span class="line"></span><br><span class="line">% <span class="number">2002</span> CODATA recommended values</span><br><span class="line">h = <span class="number">6.6260693e-34</span>;</span><br><span class="line">c = <span class="number">299792458</span>;</span><br><span class="line">k = <span class="number">1.3806505e-23</span>;</span><br><span class="line">c1 = <span class="number">2</span>*pi*h*c^<span class="number">2</span>;</span><br><span class="line">c2 = h*c/k;</span><br><span class="line"></span><br><span class="line">MrecpK = [<span class="number">0.01</span> <span class="number">1</span>:<span class="number">600</span>]; % mega reciprical Kelvin values of isotemperature lines</span><br><span class="line">T = <span class="number">1.</span>/(MrecpK*<span class="number">1e-6</span>);</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:length(T)</span><br><span class="line">   spdref = c1 * (<span class="number">1e-9</span>*wavelength).^-<span class="number">5</span> ./ (exp(c2./(T(i).* <span class="number">1e-9</span>*wavelength)) - <span class="number">1</span>);</span><br><span class="line">   spdref = spdref/<span class="built_in">max</span>(spdref);</span><br><span class="line">   wave = wavelength*<span class="number">1e-9</span>;</span><br><span class="line">   </span><br><span class="line">   % Equations <span class="keyword">from</span> Wyszecki <span class="keyword">and</span> Sitles, Color Science, 2nd ed. <span class="number">1982</span>, page</span><br><span class="line">   % <span class="number">226</span> <span class="keyword">and</span> <span class="number">227</span></span><br><span class="line">   U = <span class="built_in">sum</span>(spdref.*ubar);</span><br><span class="line">   V = <span class="built_in">sum</span>(spdref.*vbar);</span><br><span class="line">   W = <span class="built_in">sum</span>(spdref.*wbar);</span><br><span class="line">   R = U+V+W;</span><br><span class="line">   u(i) = U/R;</span><br><span class="line">   v(i) = V/R;</span><br><span class="line">   </span><br><span class="line">   Uprime = c1*c2*(T(i))^-<span class="number">2</span>*<span class="built_in">sum</span>(wave.^-<span class="number">6.</span>*ubar.*exp(c2./(wave.*T(i))).*(exp(c2./(wave.*(T(i))))-<span class="number">1</span>).^-<span class="number">2</span>)*dwave;</span><br><span class="line">   Vprime = <span class="built_in">sum</span>(c1*c2*T(i)^-<span class="number">2</span>*wave.^-<span class="number">6.</span>*vbar.*exp(c2./(wave.*T(i))).*(exp(c2./(wave.*(T(i))))-<span class="number">1</span>).^-<span class="number">2</span>)*dwave;</span><br><span class="line">   Wprime = <span class="built_in">sum</span>(c1*c2*T(i)^-<span class="number">2</span>*wave.^-<span class="number">6.</span>*wbar.*exp(c2./(wave.*T(i))).*(exp(c2./(wave.*(T(i))))-<span class="number">1</span>).^-<span class="number">2</span>)*dwave;</span><br><span class="line">   Rprime = Uprime+Vprime+Wprime;</span><br><span class="line">   </span><br><span class="line">   sl(i) = (Vprime*R-V*Rprime)/(Uprime*R-U*Rprime);</span><br><span class="line">   m(i) = -<span class="number">1</span>/sl(i);</span><br><span class="line">end</span><br><span class="line">ut = u;</span><br><span class="line">vt = v;</span><br><span class="line">tt = m;</span><br><span class="line">isoTempLinesTable = [T<span class="string">&#x27; u&#x27;</span> v<span class="string">&#x27; m&#x27;</span>];</span><br><span class="line">%save isoTempLinesNewestFine.txt isoTempLinesTable -<span class="built_in">ascii</span>; % Optionally save file</span><br><span class="line"></span><br><span class="line">% Second, calculate Correlated Color Temperature (CCT), Tc</span><br><span class="line"></span><br><span class="line">%load (<span class="string">&#x27;isoTempLinesNewestFine.mat&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;ut&#x27;</span>, <span class="string">&#x27;vt&#x27;</span>, <span class="string">&#x27;tt&#x27;</span>); % If read <span class="keyword">from</span> previously saved file</span><br><span class="line"></span><br><span class="line">% Interpolate CIE functions to spd increments</span><br><span class="line">xbar = interp1(wavelength,xbar,wavelength_spd);</span><br><span class="line">xbar(isnan(xbar)) = <span class="number">0.0</span>;</span><br><span class="line">ybar = interp1(wavelength,ybar,wavelength_spd);</span><br><span class="line">ybar(isnan(ybar)) = <span class="number">0.0</span>;</span><br><span class="line">zbar = interp1(wavelength,zbar,wavelength_spd);</span><br><span class="line">zbar(isnan(zbar)) = <span class="number">0.0</span>;</span><br><span class="line">% Calculate Chromaticity Coordinates</span><br><span class="line">X = trapz(wavelength_spd,spd.*xbar);</span><br><span class="line">Y = trapz(wavelength_spd,spd.*ybar);</span><br><span class="line">Z = trapz(wavelength_spd,spd.*zbar);</span><br><span class="line">x = X/(X+Y+Z);</span><br><span class="line">y = Y/(X+Y+Z);</span><br><span class="line">u = <span class="number">4</span>*x/(-<span class="number">2</span>*x+<span class="number">12</span>*y+<span class="number">3</span>);</span><br><span class="line">v = <span class="number">6</span>*y/(-<span class="number">2</span>*x+<span class="number">12</span>*y+<span class="number">3</span>);</span><br><span class="line">fprintf(<span class="number">1</span>,<span class="string">&#x27;x = %.4f\ty = %.4f\n&#x27;</span>,x,y);</span><br><span class="line"></span><br><span class="line">% Find adjacent lines to (us, vs) </span><br><span class="line">n = length (T); </span><br><span class="line">index = <span class="number">0</span>; </span><br><span class="line">d1 = ((v-vt(<span class="number">1</span>)) - tt(<span class="number">1</span>)*(u-ut(<span class="number">1</span>)))/sqrt(<span class="number">1</span>+tt(<span class="number">1</span>)*tt(<span class="number">1</span>)); </span><br><span class="line"><span class="keyword">for</span> i=<span class="number">2</span>:n</span><br><span class="line">    d2 = ((v-vt(i)) - tt(i)*(u-ut(i)))/sqrt(<span class="number">1</span>+tt(i)*tt(i));</span><br><span class="line">    <span class="keyword">if</span> (d1/d2 &lt; <span class="number">0</span>)</span><br><span class="line">        index = i;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        d1 = d2;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"><span class="keyword">if</span> index == <span class="number">0</span></span><br><span class="line">    Tc = -<span class="number">1</span>; % Not able to calculate CCT, u, v coordinates outside <span class="built_in">range</span>.</span><br><span class="line">    fprintf(<span class="number">1</span>,<span class="string">&#x27;Not able to calculate CCT, u, v coordinates outside range.\n&#x27;</span>);</span><br><span class="line">    %<span class="keyword">return</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    % Calculate CCT by interpolation between isotemperature lines</span><br><span class="line">    Tc = <span class="number">1</span>/(<span class="number">1</span>/T(index-<span class="number">1</span>)+d1/(d1-d2)*(<span class="number">1</span>/T(index)-<span class="number">1</span>/T(index-<span class="number">1</span>)));</span><br><span class="line">    fprintf(<span class="number">1</span>,<span class="string">&#x27;CCT = %.1f\n&#x27;</span>,Tc);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Third, calculate the Color Rendering Indices (CRI <span class="keyword">and</span> its <span class="number">14</span> indices)</span><br><span class="line">% Calculate Reference Source Spectrum, spdref. </span><br><span class="line"><span class="keyword">if</span> (Tc &lt; <span class="number">5000</span>)</span><br><span class="line">    c1 = <span class="number">3.7418e-16</span>;</span><br><span class="line">    c2 = <span class="number">1.4388e-2</span>;</span><br><span class="line">    spdref = c1 * (<span class="number">1e-9</span>*wavelength_spd).^-<span class="number">5</span> ./ (exp(c2./(Tc.* <span class="number">1e-9</span>*wavelength_spd)) - <span class="number">1</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">if</span> (Tc &lt;= <span class="number">25000</span>)</span><br><span class="line">        load(<span class="string">&#x27;CIEDaySn&#x27;</span>,<span class="string">&#x27;wavelength&#x27;</span>,<span class="string">&#x27;S0&#x27;</span>,<span class="string">&#x27;S1&#x27;</span>,<span class="string">&#x27;S2&#x27;</span>);</span><br><span class="line">        <span class="keyword">if</span> (Tc &lt;= <span class="number">7000</span>)</span><br><span class="line">            xd = -<span class="number">4.6070e9</span> / Tc.^<span class="number">3</span> + <span class="number">2.9678e6</span> / Tc.^<span class="number">2</span> + <span class="number">0.09911e3</span> / Tc + <span class="number">0.244063</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            xd = -<span class="number">2.0064e9</span> / Tc.^<span class="number">3</span> + <span class="number">1.9018e6</span> / Tc.^<span class="number">2</span> + <span class="number">0.24748e3</span> / Tc + <span class="number">0.237040</span>;</span><br><span class="line">        end</span><br><span class="line">        yd = -<span class="number">3.000</span>*xd*xd + <span class="number">2.870</span>*xd - <span class="number">0.275</span>;</span><br><span class="line">        M1 = (-<span class="number">1.3515</span> - <span class="number">1.7703</span>*xd + <span class="number">5.9114</span>*yd) / (<span class="number">0.0241</span> + <span class="number">0.2562</span>*xd - <span class="number">0.7341</span>*yd);</span><br><span class="line">        M2 = (<span class="number">0.0300</span> - <span class="number">31.4424</span>*xd + <span class="number">30.0717</span>*yd) / (<span class="number">0.0241</span> + <span class="number">0.2562</span>*xd - <span class="number">0.7341</span>*yd);</span><br><span class="line">        spdref = S0 + M1*S1 + M2*S2;</span><br><span class="line">        spdref = interp1(wavelength,spdref,wavelength_spd);</span><br><span class="line">        spdref(isnan(spdref)) = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        R = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Interpolate TCS values <span class="keyword">from</span> <span class="number">5</span> nm to spd nm increments</span><br><span class="line">TCS_1 = zeros(length(wavelength_spd),<span class="number">14</span>);</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">14</span></span><br><span class="line">    TCS_1(:,i) = interp1(TCS(:,<span class="number">1</span>),TCS(:,i+<span class="number">1</span>),wavelength_spd,<span class="string">&#x27;linear&#x27;</span>,<span class="number">0</span>);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Calculate u, v chromaticity coordinates of samples under test illuminant, uk, vk <span class="keyword">and</span></span><br><span class="line">% reference illuminant, ur, vr.</span><br><span class="line">uki = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line">vki = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line">uri = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line">vri = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line">X = trapz(wavelength_spd,spd .* xbar);</span><br><span class="line">Y = trapz(wavelength_spd,spd .* ybar);</span><br><span class="line">Z = trapz(wavelength_spd,spd .* zbar);</span><br><span class="line">Yknormal = <span class="number">100</span> / Y;</span><br><span class="line">Yk = Y*Yknormal;</span><br><span class="line">uk = <span class="number">4</span>*X/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">vk = <span class="number">6</span>*Y/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">X = trapz(wavelength_spd,spdref .* xbar);</span><br><span class="line">Y = trapz(wavelength_spd,spdref .* ybar);</span><br><span class="line">Z = trapz(wavelength_spd,spdref .* zbar);</span><br><span class="line">Yrnormal = <span class="number">100</span> / Y;</span><br><span class="line">Yr = Y*Yrnormal;</span><br><span class="line">ur = <span class="number">4</span>*X/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">vr = <span class="number">6</span>*Y/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">14</span></span><br><span class="line">X = trapz(wavelength_spd,spd .* TCS_1(:,i) .* xbar);</span><br><span class="line">Y = trapz(wavelength_spd,spd .* TCS_1(:,i) .* ybar);</span><br><span class="line">Z = trapz(wavelength_spd,spd .* TCS_1(:,i) .* zbar);</span><br><span class="line">Yki(i) = Y*Yknormal;</span><br><span class="line">uki(i) = <span class="number">4</span>*X/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">vki(i) = <span class="number">6</span>*Y/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">X = trapz(wavelength_spd,spdref .* TCS_1(:,i) .* xbar);</span><br><span class="line">Y = trapz(wavelength_spd,spdref .* TCS_1(:,i) .* ybar);</span><br><span class="line">Z = trapz(wavelength_spd,spdref .* TCS_1(:,i) .* zbar);</span><br><span class="line">Yri(i) = Y*Yrnormal;</span><br><span class="line">uri(i) = <span class="number">4</span>*X/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">vri(i) = <span class="number">6</span>*Y/(X+<span class="number">15</span>*Y+<span class="number">3</span>*Z);</span><br><span class="line">end</span><br><span class="line">% Check tolorence <span class="keyword">for</span> reference illuminant</span><br><span class="line">DC = sqrt((uk-ur).^<span class="number">2</span> + (vk-vr).^<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">% Apply adaptive (perceived) color shift.</span><br><span class="line">ck = (<span class="number">4</span> - uk - <span class="number">10</span>*vk) / vk;</span><br><span class="line">dk = (<span class="number">1.708</span>*vk + <span class="number">0.404</span> - <span class="number">1.481</span>*uk) / vk;</span><br><span class="line">cr = (<span class="number">4</span> - ur - <span class="number">10</span>*vr) / vr;</span><br><span class="line">dr = (<span class="number">1.708</span>*vr + <span class="number">0.404</span> - <span class="number">1.481</span>*ur) / vr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">14</span></span><br><span class="line">cki = (<span class="number">4</span> - uki(i) - <span class="number">10</span>*vki(i)) / vki(i);</span><br><span class="line">dki = (<span class="number">1.708</span>*vki(i) + <span class="number">0.404</span> - <span class="number">1.481</span>*uki(i)) / vki(i);</span><br><span class="line">ukip(i) = (<span class="number">10.872</span> + <span class="number">0.404</span>*cr/ck*cki - <span class="number">4</span>*dr/dk*dki) / (<span class="number">16.518</span> + <span class="number">1.481</span>*cr/ck*cki - dr/dk*dki);</span><br><span class="line">vkip(i) = <span class="number">5.520</span> / (<span class="number">16.518</span> + <span class="number">1.481</span>*cr/ck*cki - dr/dk*dki);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%  Transformation into <span class="number">1964</span> Uniform space coordinates.</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">14</span></span><br><span class="line">Wstarr(i) = <span class="number">25</span>*Yri(i).^<span class="number">.333333</span> - <span class="number">17</span>;</span><br><span class="line">Ustarr(i) = <span class="number">13</span>*Wstarr(i)*(uri(i) - ur);</span><br><span class="line">Vstarr(i) = <span class="number">13</span>*Wstarr(i)*(vri(i) - vr);</span><br><span class="line"></span><br><span class="line">Wstark(i) = <span class="number">25</span>*Yki(i).^<span class="number">.333333</span> - <span class="number">17</span>;</span><br><span class="line">Ustark(i) = <span class="number">13</span>*Wstark(i)*(ukip(i) - ur); % after applying the adaptive color shift, <span class="string">u&#x27;k = ur</span></span><br><span class="line"><span class="string">Vstark(i) = 13*Wstark(i)*(vkip(i) - vr); % after applying the adaptive color shift, v&#x27;</span>k = vr</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Determination of resultant color shift, delta E.</span><br><span class="line">deltaE = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line">R = zeros(<span class="number">1</span>,<span class="number">14</span>);</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:<span class="number">14</span></span><br><span class="line">deltaE(i) = sqrt((Ustarr(i) - Ustark(i)).^<span class="number">2</span> + (Vstarr(i) - Vstark(i)).^<span class="number">2</span> + (Wstarr(i) - Wstark(i)).^<span class="number">2</span>);</span><br><span class="line">R(i) = <span class="number">100</span> - <span class="number">4.6</span>*deltaE(i);</span><br><span class="line">end</span><br><span class="line">Ra = <span class="built_in">sum</span>(R(<span class="number">1</span>:<span class="number">8</span>))/<span class="number">8</span>;</span><br><span class="line">fprintf(<span class="number">1</span>,<span class="string">&#x27;CRIra = %.1f\n&#x27;</span>,Ra);</span><br><span class="line"></span><br><span class="line">% fourth, calculate the gamut area formed by the <span class="number">8</span> CIE standard color samples</span><br><span class="line">ukii=[uki(:,<span class="number">1</span>:<span class="number">8</span>),uki(<span class="number">1</span>)];</span><br><span class="line">vkii=<span class="number">1.5</span>*[vki(:,<span class="number">1</span>:<span class="number">8</span>),vki(<span class="number">1</span>)];</span><br><span class="line">Ga=polyarea(ukii,vkii);</span><br><span class="line">% Normalize gamut area to equal energy source </span><br><span class="line">Ga=Ga/<span class="number">0.00728468</span>*<span class="number">100</span>;</span><br><span class="line">fprintf(<span class="number">1</span>,<span class="string">&#x27;Gamut Area Index = %.1f\n&#x27;</span>,Ga);</span><br><span class="line"></span><br><span class="line">% Fifth, calculate the FSI (full spectrum index)</span><br><span class="line">% Calculates the Full-spectrum Index</span><br><span class="line"></span><br><span class="line">% Interpolate to wavelength interval of 1nm <span class="keyword">from</span> 380nm to 730nm</span><br><span class="line">numWave = <span class="number">351</span>;</span><br><span class="line">t=(<span class="number">380</span>:<span class="number">1</span>:<span class="number">730</span>)<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">spd=interp1(wavelength_spd,spd,t,&#x27;</span>spline<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">spd(isnan(spd)) = 0.0; </span></span><br><span class="line"><span class="string">spd = spd/sum(spd); % Normalize the relative spd so that the total power equals 1</span></span><br><span class="line"><span class="string">%Equal energy cumulative spd</span></span><br><span class="line"><span class="string">EEcum=(1/numWave:1/numWave:1)&#x27;</span>;</span><br><span class="line">%Calculate FSI</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j=<span class="number">1</span>:numWave </span><br><span class="line">cum = cumsum(spd); % A MatLab function <span class="keyword">for</span> cumulative sums </span><br><span class="line">sqrDiff = (cum-EEcum).^<span class="number">2</span>; </span><br><span class="line">sumSqrDiff(j)=<span class="built_in">sum</span>(sqrDiff); </span><br><span class="line">spd=circshift(spd,<span class="number">1</span>); </span><br><span class="line">end </span><br><span class="line">FSI=mean(sumSqrDiff); </span><br><span class="line">FSCI=<span class="number">100</span>-<span class="number">5.1</span>*FSI;</span><br><span class="line">fprintf(<span class="number">1</span>,<span class="string">&#x27;FSCI = %.3f\n&#x27;</span>,FSCI);</span><br></pre></td></tr></table></figure><h2 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helmholtz</span><br><span class="line">axis equal</span><br></pre></td></tr></table></figure><h2 id="MATLAB绘制多条曲线"><a href="#MATLAB绘制多条曲线" class="headerlink" title="MATLAB绘制多条曲线"></a>MATLAB绘制多条曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n = length(a(<span class="number">1</span>,:))-<span class="number">1</span>;  %how much number of color to use</span><br><span class="line">c = colormap(jet(n));  %number of color <span class="keyword">in</span> figure</span><br><span class="line">c1 = <span class="number">0</span>;</span><br><span class="line">figure(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); </span><br><span class="line"><span class="keyword">for</span> ix = <span class="number">2</span>:length(a(<span class="number">1</span>,:))</span><br><span class="line">    plot(a(:,<span class="number">1</span>),a(:,ix),<span class="string">&#x27;Color&#x27;</span>, c(ix-<span class="number">1</span>,:),  <span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line">    hold on;</span><br><span class="line">end</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">20</span>);axis([<span class="built_in">min</span>(a(:,<span class="number">1</span>)),<span class="built_in">max</span>(a(:,<span class="number">1</span>)),<span class="number">0</span>  , <span class="number">0.7</span>]);</span><br><span class="line">xlabel(<span class="string">&#x27;wavelength (nm)&#x27;</span>);ylabel(<span class="string">&#x27;Intensity&#x27;</span>);</span><br></pre></td></tr></table></figure><p>&#96;&#96;python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">%% <span class="built_in">input</span> a(wavelength,curve)</span><br><span class="line">number=length(a(<span class="number">1</span>,:))-<span class="number">1</span>;%总的绘制曲线的条数</span><br><span class="line">photo = <span class="number">6</span> ; %图片个数</span><br><span class="line">curve_n=number/photo; %%每个图片中的曲线</span><br><span class="line">b=a(:,<span class="number">2</span>:number+<span class="number">1</span>); %% curve</span><br><span class="line"></span><br><span class="line">x =a(:,<span class="number">1</span>); %% wavelength</span><br><span class="line">TempNum = <span class="number">6</span> ;%% 颜色的数量</span><br><span class="line">cmap = hsv(TempNum);</span><br><span class="line">figure(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line"><span class="keyword">for</span> j=<span class="number">1</span>:photo</span><br><span class="line">        subplot(<span class="number">3</span>, <span class="number">2</span>, j);</span><br><span class="line">        k=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> i=(j-<span class="number">1</span>)*curve_n+<span class="number">1</span>:j*curve_n</span><br><span class="line">            y = b(:,i);</span><br><span class="line">            plot(x, y,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">3</span>,<span class="string">&#x27;Color&#x27;</span>,cmap(k,:));hold on;</span><br><span class="line">            k=k+<span class="number">1</span>;</span><br><span class="line">        end</span><br><span class="line">        hold off;</span><br><span class="line">        xlabel(<span class="string">&#x27;Wavelength(nm)&#x27;</span>);</span><br><span class="line">        axis([<span class="number">400</span> <span class="number">1300</span> <span class="number">0</span> <span class="number">1</span>]);</span><br><span class="line">        <span class="built_in">set</span>(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Calibri&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">30</span>);</span><br><span class="line">        h = legend(<span class="string">&#x27;50&#x27;</span>,<span class="string">&#x27;100&#x27;</span>,<span class="string">&#x27;150&#x27;</span>,<span class="string">&#x27;200&#x27;</span>,<span class="string">&#x27;250&#x27;</span>);</span><br><span class="line">        <span class="built_in">set</span>(h, <span class="string">&#x27;Box&#x27;</span>, <span class="string">&#x27;off&#x27;</span>);</span><br><span class="line">end</span><br><span class="line">%         plot(wave,R,<span class="string">&#x27;r&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">3</span>);hold on;plot(wave,T,<span class="string">&#x27;b--&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">3</span>);</span><br><span class="line">%         xlabel(<span class="string">&#x27;Wavelength(nm)&#x27;</span>,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Calibri&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">%         ylabel(<span class="string">&#x27;R&amp;T&#x27;</span>,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Calibri&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">%         <span class="built_in">set</span>(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Calibri&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">%          n=<span class="number">32</span>; colormap(hsv(n)); pcolor([<span class="number">1</span>:n+<span class="number">1</span>;<span class="number">1</span>:n+<span class="number">1</span>]);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>MATLAB 控制FDTD程序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">function Y = objectivefunction(X)</span><br><span class="line">    % Note: X <span class="keyword">is</span> a [<span class="number">1</span>*<span class="number">2</span>] vector, i.e. X=[x1 x2]</span><br><span class="line">     x1=X(<span class="number">1</span>,<span class="number">1</span>);x2=X(<span class="number">1</span>,<span class="number">2</span>); % x2=second variable</span><br><span class="line">    path(path,<span class="string">&#x27;C:\Program Files\Lumerical\FDTD\api\matlab&#x27;</span>);%Add Lumerical Matlab API path</span><br><span class="line">    sim_file_path=(<span class="string">&#x27;D:\files\FDTD files\MATLABcomsol\multi&#x27;</span>); % update this path to use<span class="string">r&#x27;s folder 文件路径</span></span><br><span class="line"><span class="string">    sim_file_name=(&#x27;</span>Wu.fsp<span class="string">&#x27;);% 文件名字</span></span><br><span class="line"><span class="string">    h=appopen(&#x27;</span>fdtd<span class="string">&#x27;); %Open FDTD session</span></span><br><span class="line"><span class="string">    %Pass the path variables to FDTD</span></span><br><span class="line"><span class="string">    appputvar(h,&#x27;</span>sim_file_path<span class="string">&#x27;,sim_file_path);</span></span><br><span class="line"><span class="string">    appputvar(h,&#x27;</span>sim_file_name<span class="string">&#x27;,sim_file_name);</span></span><br><span class="line"><span class="string">    appevalscript(h,&#x27;</span>load(<span class="string">&quot;Wu.fsp&quot;</span>);<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">%   appevalscript(h,&#x27;</span>switchtolayout;<span class="string">&#x27;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%%  set parameter</span></span><br><span class="line"><span class="string">    code=strcat(&#x27;</span>switchtolayout;<span class="string">&#x27;,&#x27;</span>select(<span class="string">&quot;group&quot;</span>);<span class="string">&#x27;,...</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="built_in">set</span>(<span class="string">&quot;h1&quot;</span>,<span class="string">&#x27;,num2str(x1*1e-9,16),&#x27;</span>);<span class="string">&#x27;,...</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="built_in">set</span>(<span class="string">&quot;h3&quot;</span>,<span class="string">&#x27;,num2str(x2*1e-9,16),&#x27;</span>);<span class="string">&#x27;,...</span></span><br><span class="line"><span class="string">    &#x27;</span>run;<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">    appevalscript(h,code);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%     appevalscript(h,&#x27;</span>run;<span class="string">&#x27;);%跑</span></span><br><span class="line"><span class="string">    appevalscript(h,&#x27;</span>A=getresult(<span class="string">&quot;monitor&quot;</span>,<span class="string">&quot;T&quot;</span>);<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">    appevalscript(h,&#x27;</span>A_s=A.getattribute(<span class="string">&quot;T&quot;</span>);<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">    A_spectrum =appgetvar(h,&#x27;</span>A_s<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    A=10; % parameter of the optimization problem</span></span><br><span class="line"><span class="string">    n=2; % parameter of the optimization problem</span></span><br><span class="line"><span class="string">    appclose(h);</span></span><br><span class="line"><span class="string">    Y=sum(A_spectrum);</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">% Y=A*n+[(x1)^2-A*cos(2*pi*x1)]+[(x2)^2-A*cos(2*pi*x2)];</span></span><br><span class="line"><span class="string">% Y=Rastrigin function</span></span><br></pre></td></tr></table></figure><h2 id="MATLAB-绘制多条曲线"><a href="#MATLAB-绘制多条曲线" class="headerlink" title="MATLAB 绘制多条曲线"></a>MATLAB 绘制多条曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n = length(a(<span class="number">1</span>,:))-<span class="number">1</span>;  %how much number of color to use</span><br><span class="line">c = colormap(jet(n));  %number of color <span class="keyword">in</span> figure</span><br><span class="line">c1 = <span class="number">0</span>;</span><br><span class="line">figure(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); </span><br><span class="line"><span class="keyword">for</span> ix = <span class="number">2</span>:length(a(<span class="number">1</span>,:))</span><br><span class="line">    plot(a(:,<span class="number">1</span>),a(:,ix),<span class="string">&#x27;Color&#x27;</span>, c(ix-<span class="number">1</span>,:),  <span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line">    hold on;</span><br><span class="line">end</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">20</span>);axis([<span class="built_in">min</span>(a(:,<span class="number">1</span>)),<span class="built_in">max</span>(a(:,<span class="number">1</span>)),<span class="number">0</span>  , <span class="number">0.7</span>]);</span><br><span class="line">xlabel(<span class="string">&#x27;wavelength (nm)&#x27;</span>);ylabel(<span class="string">&#x27;Intensity&#x27;</span>);</span><br></pre></td></tr></table></figure><h2 id="picture函数导出透明图片"><a href="#picture函数导出透明图片" class="headerlink" title="picture函数导出透明图片"></a>picture函数导出透明图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = imread(<span class="string">&#x27;mypicture.png&#x27;</span>);</span><br><span class="line">min_x = <span class="number">0</span>;max_x = <span class="number">0.8</span>;min_y = <span class="number">0</span>;max_y = <span class="number">0.9</span>;</span><br><span class="line">imagesc([min_x max_x], [min_y max_y], flipdim(img,<span class="number">1</span>)); </span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;ydir&#x27;</span>,<span class="string">&#x27;normal&#x27;</span>);</span><br><span class="line">hold on;</span><br><span class="line">plot(<span class="number">0.3333</span>,<span class="number">0.3333</span>,<span class="string">&#x27;r*&#x27;</span>);</span><br></pre></td></tr></table></figure><h2 id="时间计算-tc-m"><a href="#时间计算-tc-m" class="headerlink" title="时间计算 tc.m"></a>时间计算 tc.m</h2><p>用法：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t1 = clock;</span><br><span class="line"><span class="comment">% 程序</span></span><br><span class="line">t2 = clock;</span><br><span class="line">tc(t2, t1);</span><br></pre></td></tr></table></figure><p>函数：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[Frame_Name]</span>= <span class="title">tc</span><span class="params">(t2,t1)</span></span></span><br><span class="line">    time_distance = etime(t2,t1);</span><br><span class="line">    <span class="comment">% 保留小数点后两位</span></span><br><span class="line">    <span class="keyword">if</span> time_distance &lt;= <span class="number">60</span></span><br><span class="line">        Frame_Name = [ num2str(<span class="built_in">round</span>(time_distance,<span class="number">2</span>)) <span class="string">&#x27;s&#x27;</span>]</span><br><span class="line">        time_distance = time_distance/<span class="number">60</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(time_distance &gt;<span class="number">60</span> )&amp;&amp;(time_distance &lt;<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">        Frame_Name = [ num2str(<span class="built_in">round</span>(time_distance/<span class="number">60</span>,<span class="number">2</span>)) <span class="string">&#x27;min&#x27;</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> time_distance &gt;= <span class="number">60</span>*<span class="number">60</span></span><br><span class="line">        Frame_Name = [ num2str(<span class="built_in">round</span>(time_distance/<span class="number">60</span>/<span class="number">60</span>,<span class="number">2</span>)) <span class="string">&#x27;h&#x27;</span>]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">end_time = time.time()  <span class="comment"># calculate time</span></span><br><span class="line"><span class="keyword">if</span> end_time - start_time &gt;= <span class="number">3600</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; h&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time) / <span class="number">3600</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">elif</span> end_time - start_time &gt;= <span class="number">60</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; min&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time) / <span class="number">60</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; s&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time), <span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h2 id="黑体辐射强度"><a href="#黑体辐射强度" class="headerlink" title="黑体辐射强度"></a>黑体辐射强度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% simple model of the black body using MATLAB/Octave/Freemat</span><br><span class="line">wavelength = <span class="number">0.01</span>:<span class="number">0.01</span>:<span class="number">5.0</span>;     % microns sweep over a <span class="built_in">range</span> of wavelengths</span><br><span class="line">T = <span class="number">5000</span>;                       % temperature <span class="keyword">in</span> kelvin</span><br><span class="line">F = <span class="number">3.742</span>./ ((wavelength.^<span class="number">5</span>).*(exp(<span class="number">1.439e4</span>./(wavelength*T))-<span class="number">1</span>)) ; % gives the result <span class="keyword">in</span> W/m2/um x <span class="number">1e8</span></span><br><span class="line">plot(wavelength,F);</span><br><span class="line">xlabel(<span class="string">&#x27;wavelength (\mum)&#x27;</span>);       %  add axis labels <span class="keyword">and</span> plot title</span><br><span class="line">ylabel(<span class="string">&#x27;spectral intensity (W/m^2/\mum) x 10^8&#x27;</span>);</span><br><span class="line">title(<span class="string">&#x27;Blackbody Radiation&#x27;</span>);</span><br><span class="line">legend(sprintf(<span class="string">&#x27;T = %.0f K&#x27;</span>,T));</span><br></pre></td></tr></table></figure><h2 id="画彩色图"><a href="#画彩色图" class="headerlink" title="画彩色图"></a>画彩色图</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">figure</span>(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line">x = <span class="built_in">linspace</span>(-grating_period, grating_period, <span class="number">100</span>); <span class="comment">% 绘制的范围 两个周期</span></span><br><span class="line">[tab1, z, o] = res3(x, aa, profil1, <span class="number">1</span>, parm); </span><br><span class="line">h = pcolor(x, z, <span class="built_in">real</span>(o)); <span class="comment">% 绘制图像，并返回句柄</span></span><br><span class="line">set(h, <span class="string">&#x27;EdgeColor&#x27;</span>, <span class="string">&#x27;none&#x27;</span>);</span><br><span class="line">axis tight; <span class="comment">% 自动调整坐标轴的范围，使其紧贴着数据的最小值和最大值</span></span><br><span class="line">colorbar;colormap jet;</span><br><span class="line">set(gca, <span class="string">&#x27;TickDir&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;Box&#x27;</span>, <span class="string">&#x27;off&#x27;</span>, <span class="string">&#x27;LineWidth&#x27;</span>, <span class="number">1.5</span>);</span><br><span class="line">set(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>); <span class="comment">% Times New Roman</span></span><br><span class="line">xlabel(<span class="string">&#x27;X label range (um)&#x27;</span>,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">ylabel(<span class="string">&#x27;Z label (um)&#x27;</span>,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Times New Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">title(<span class="string">&#x27;两个周期 截面折射率&#x27;</span>,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;Monospaced&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">36</span>);</span><br><span class="line">set(gcf,<span class="string">&#x27;unit&#x27;</span>,<span class="string">&#x27;centimeters&#x27;</span>,<span class="string">&#x27;position&#x27;</span>,[<span class="number">-25</span>,<span class="number">10</span>,<span class="number">20</span>,<span class="number">15</span>]) <span class="comment">% [位置横纵 宽 高]</span></span><br></pre></td></tr></table></figure><img src="https://cdn.staticaly.com/gh/yangmulao/blogcdn@master/img/image-20230405173146881.png" alt="image-20230405173146881" style="zoom:50%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">%<span class="built_in">input</span> a</span><br><span class="line">[ax,ay]=size(a);</span><br><span class="line">B=a(<span class="number">1</span>,<span class="number">2</span>:ay);</span><br><span class="line">A=a(<span class="number">2</span>:ax,<span class="number">1</span>);</span><br><span class="line">C=a(<span class="number">2</span>:ax,<span class="number">2</span>:ay);</span><br><span class="line">%  C=flipud(C);</span><br><span class="line">[ax,ay]=<span class="built_in">max</span>(<span class="built_in">max</span>(C));</span><br><span class="line">% C=C/ax;</span><br><span class="line">[x,y]=meshgrid(A,B);</span><br><span class="line">figure(<span class="string">&#x27;color&#x27;</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line">surf(y,x,C<span class="string">&#x27;);shading interp;colorbar;</span></span><br><span class="line"><span class="string"> pcolor(A,B,C&#x27;</span>)</span><br><span class="line">% pcolor(B,A,C)</span><br><span class="line">colorbar,shading interp, hold on;</span><br><span class="line">[c,h] = contour(A,B,C<span class="string">&#x27;,[0.9],&#x27;</span>k--<span class="string">&#x27;,&#x27;</span>LineWidth<span class="string">&#x27;,1.2);%k 黑色</span></span><br><span class="line"><span class="string">[c,h] = contour(A,B,C&#x27;</span>,[<span class="number">0.8</span>],<span class="string">&#x27;b--&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">1.2</span>);%k 蓝色</span><br><span class="line">[c,h] = contour(A,B,C<span class="string">&#x27;,[0.6 0.6],&#x27;</span>k--<span class="string">&#x27;,&#x27;</span>LineWidth<span class="string">&#x27;,1.2);%k 红色</span></span><br><span class="line"><span class="string">set(gca,&#x27;</span>FontName<span class="string">&#x27;,&#x27;</span>Times New Roman<span class="string">&#x27;,&#x27;</span>FontSize<span class="string">&#x27;,36);</span></span><br><span class="line"><span class="string">colormap hot;</span></span><br><span class="line"><span class="string">% colormap jet;</span></span><br><span class="line"><span class="string">xlabel(&#x27;</span>Wavelength(μm)<span class="string">&#x27;,&#x27;</span>FontName<span class="string">&#x27;,&#x27;</span>Times New Roman<span class="string">&#x27;,&#x27;</span>FontSize<span class="string">&#x27;,36);</span></span><br><span class="line"><span class="string">ylabel(&#x27;</span>Incident angle(degree)<span class="string">&#x27;,&#x27;</span>FontName<span class="string">&#x27;,&#x27;</span>Times New Roman<span class="string">&#x27;,&#x27;</span>FontSize<span class="string">&#x27;,36);</span></span><br><span class="line"><span class="string">set(gcf,&#x27;</span>unit<span class="string">&#x27;,&#x27;</span>centimeters<span class="string">&#x27;,&#x27;</span>position<span class="string">&#x27;,[5 5 25 20]) % 长//宽</span></span><br><span class="line"><span class="string">% set(gca,&#x27;</span>Position<span class="string">&#x27;,[.1 .3 .2 .5]);</span></span><br><span class="line"><span class="string">set(gca, &#x27;</span>yTick<span class="string">&#x27;, [0:30:90]);   %设置轴坐标刻度</span></span><br></pre></td></tr></table></figure><h2 id="螺丝程序"><a href="#螺丝程序" class="headerlink" title="螺丝程序"></a>螺丝程序</h2><p>·&#96;python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% a = <span class="number">0</span>:<span class="number">0.02</span>:<span class="number">1</span>*<span class="number">2</span>*pi;</span><br><span class="line">% z = sin(a);z=z<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">% le = length(a);</span></span><br><span class="line"><span class="string">% y1 =0*cos(a);</span></span><br><span class="line"><span class="string">% y1 = y1&#x27;</span>;</span><br><span class="line">% y2=cos(a);</span><br><span class="line">% y2 = y2<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">% yy = [y1 ;y2];</span></span><br><span class="line"><span class="string">% YY = yy;</span></span><br><span class="line"><span class="string">% for ix = 1:3</span></span><br><span class="line"><span class="string">% YY = [YY ;yy];</span></span><br><span class="line"><span class="string">% end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">a = -1:0.02:1;</span></span><br><span class="line"><span class="string">a1 = 0.98:-0.02:-1;</span></span><br><span class="line"><span class="string">b = [a a1];</span></span><br><span class="line"><span class="string">y = asin(b);</span></span><br><span class="line"><span class="string">y = y&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="全排列组合"><a href="#全排列组合" class="headerlink" title="全排列组合"></a>全排列组合</h2><p>··&#96;python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cs1 = <span class="number">1</span>:<span class="number">4</span>;</span><br><span class="line">cs2 = <span class="number">1</span>:<span class="number">3</span>;</span><br><span class="line">cs3 = <span class="number">2</span>:<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">[j1,j2,j3]=ndgrid(cs1,cs2,cs3);</span><br><span class="line">COM=[j1(:),j2(:),j3(:)];clear cs1 cs2 cs3 j1 j2 j3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scale = <span class="number">10</span>; % 分辨率</span><br><span class="line">column = DomainElementsY / scale ; % <span class="number">10</span> 划分的个数 矩阵的列数 </span><br><span class="line">A = double(logical(de2bi(<span class="number">0</span>:<span class="number">2</span>^column-<span class="number">1</span>, column)));</span><br></pre></td></tr></table></figure><h2 id="西瓜函数"><a href="#西瓜函数" class="headerlink" title="西瓜函数"></a>西瓜函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">function SolvePde_FrozenWatermelon</span><br><span class="line">% 把西瓜放冰箱里，多长时间才能冰透？</span><br><span class="line">% 求解冰镇西瓜过程中的温度扩散问题（热传导方程）</span><br><span class="line">%       <span class="number">1</span>   ∂T     <span class="number">1</span>   ∂        ∂</span><br><span class="line">%      ---.---- = ---.----( r2.----)</span><br><span class="line">%       a   ∂t     r2  ∂r       ∂r</span><br><span class="line">%</span><br><span class="line">%   初始条件：</span><br><span class="line">%            t = <span class="number">0</span>, <span class="number">0</span> ≤ r ≤ R, T = T0</span><br><span class="line">%   边值条件：</span><br><span class="line">%            t &gt; <span class="number">0</span>, r = <span class="number">0</span>, ∂T/∂r = <span class="number">0</span></span><br><span class="line">%            t &gt; <span class="number">0</span>, r = R, h(T - Tinf) = -k.∂T/∂r</span><br><span class="line">%</span><br><span class="line">%   <span class="number">1.</span> 假设西瓜的外形是一个半径为R的完美的球体</span><br><span class="line">%   <span class="number">2.</span> 假设西瓜没有瓜皮，没有瓜籽，瓜瓤也是完全均匀的，其密度、导热能力、比热容</span><br><span class="line">%      全是均匀的，一切物性参数都不随温度变化而变化</span><br><span class="line">%   <span class="number">3.</span> 冰箱是一个恒温为Tinf ℃的环境</span><br><span class="line">%</span><br><span class="line">%   August <span class="number">4</span>  <span class="number">2017</span>, editted by ZhongHua-Xie, Tianjin University of Science <span class="keyword">and</span> Technology.</span><br><span class="line"></span><br><span class="line">% 相关参数</span><br><span class="line">h = <span class="number">5</span>;%        % 西瓜与静止冷空气的对流换热系数 [w/(㎡.K)]</span><br><span class="line">k = <span class="number">0.48</span>;%     % 西瓜的导热系数 [w/(m.K)]</span><br><span class="line">p = <span class="number">918</span>;      % 西瓜的密度 [kg/m3]</span><br><span class="line">Cp = <span class="number">3990</span>;    % 热容 [J/(kg.K)]</span><br><span class="line">a = k/(p*Cp); % 西瓜的热扩散系数 [㎡/s]</span><br><span class="line">T0 = <span class="number">32</span>;      % 初始温度 [℃]</span><br><span class="line">Tinf = <span class="number">6</span>;     % 终极温度 [℃]</span><br><span class="line"></span><br><span class="line">m = <span class="number">2</span>;                                     % 方程中的m参数</span><br><span class="line">r = linspace(<span class="number">0</span>,<span class="number">9</span>,<span class="number">50</span>)/<span class="number">100</span>;                  % 定义距离向量r</span><br><span class="line">t = linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">50</span>)*<span class="number">3600</span>;                % 定义时间向量t</span><br><span class="line">Td = pdepe(m,@pdefun,@pdeic,@pdebc,r,t);   % 方程求解</span><br><span class="line"></span><br><span class="line">% 结果可视化</span><br><span class="line">figure;</span><br><span class="line">surf(r*<span class="number">100</span>,t/<span class="number">3600</span>,Td);                     % 绘制温度T关于距离r和时间t的三维曲面</span><br><span class="line">colorbar;</span><br><span class="line">zlim([<span class="built_in">min</span>(Td(:))-<span class="number">2</span>,<span class="built_in">max</span>(Td(:))+<span class="number">2</span>]);</span><br><span class="line">title(<span class="string">&#x27;T(r,t)&#x27;</span>); </span><br><span class="line">xlabel(<span class="string">&#x27;Distance r&#x27;</span>);</span><br><span class="line">ylabel(<span class="string">&#x27;Time t&#x27;</span>)</span><br><span class="line">view(<span class="number">116</span>,<span class="number">33</span>);</span><br><span class="line">text(<span class="number">8</span>,-<span class="number">0.5</span>,<span class="number">34.5</span>,<span class="string">&#x27;开始冷藏&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">18</span>,<span class="string">&#x27;Rotation&#x27;</span>,<span class="number">40</span>)</span><br><span class="line">text(<span class="number">0</span>,<span class="number">5</span>,<span class="number">26</span>,<span class="string">&#x27;瓜心&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">18</span>)</span><br><span class="line">text(<span class="number">7</span>,<span class="number">2</span>,<span class="number">10.5</span>,<span class="string">&#x27;瓜皮&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">18</span>)</span><br><span class="line"></span><br><span class="line">% 绘制西瓜等效切面温度扩散动画</span><br><span class="line">num = size(Td,<span class="number">2</span>);</span><br><span class="line">thetai = linspace(<span class="number">0</span>,<span class="number">2</span>*pi,<span class="number">50</span>);</span><br><span class="line">[R,Theta] = meshgrid(r,thetai);</span><br><span class="line">X = <span class="number">100</span>*R.*cos(Theta);</span><br><span class="line">Y = <span class="number">100</span>*R.*sin(Theta);</span><br><span class="line">Tdi = ones(size(thetai<span class="string">&#x27;))*Td(1,:);</span></span><br><span class="line"><span class="string">X_bac = linspace(min(X(:))-1,max(X(:))+1,50);</span></span><br><span class="line"><span class="string">Y_bac = linspace(min(Y(:))-1,max(Y(:))+1,50);</span></span><br><span class="line"><span class="string">[X_bac,Y_bac] = meshgrid(X_bac,Y_bac);</span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">% 绘制背景温度面</span></span><br><span class="line"><span class="string">surf(X_bac,Y_bac,Tinf*ones(size(X_bac)),&#x27;</span>FaceColo<span class="string">r&#x27;,&#x27;</span>interp<span class="string">&#x27;,&#x27;</span>EdgeColo<span class="string">r&#x27;,&#x27;</span>none<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">hold on;</span></span><br><span class="line"><span class="string">% 绘制零时刻西瓜等效温度切面</span></span><br><span class="line"><span class="string">hs = surf(X,Y,Tdi,&#x27;</span>FaceColo<span class="string">r&#x27;,&#x27;</span>interp<span class="string">&#x27;,&#x27;</span>EdgeColo<span class="string">r&#x27;,&#x27;</span>none<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">axis equal;</span></span><br><span class="line"><span class="string">axis([min(X_bac(:)),max(X_bac(:)),min(Y_bac(:)),max(Y_bac(:))])</span></span><br><span class="line"><span class="string">xlabel(&#x27;</span>X<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">ylabel(&#x27;</span>Y<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">caxis([Tinf,max(Td(:))]);</span></span><br><span class="line"><span class="string">colorbar;</span></span><br><span class="line"><span class="string">view(2);</span></span><br><span class="line"><span class="string">ht = title([&#x27;</span>冷藏<span class="string">&#x27;,num2str(t(1)/3600,&#x27;</span>%<span class="number">2.1</span><span class="string">f&#x27;),&#x27;</span>小时后<span class="string">&#x27;]);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% filename = &#x27;</span>冰镇西瓜温度扩散动画.gi<span class="string">f&#x27;;</span></span><br><span class="line"><span class="string">% f = getframe(gcf);</span></span><br><span class="line"><span class="string">% IM = frame2im(f);</span></span><br><span class="line"><span class="string">% [IM,map] = rgb2ind(IM,256);</span></span><br><span class="line"><span class="string">% imwrite(IM,map,filename,&#x27;</span>gi<span class="string">f&#x27;, &#x27;</span>Loopcount<span class="string">&#x27;,inf,&#x27;</span>DelayTime<span class="string">&#x27;,0.2);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">for i = 2:num</span></span><br><span class="line"><span class="string">    Tdi = ones(size(thetai&#x27;</span>))*Td(i,:);</span><br><span class="line">    <span class="built_in">set</span>(hs,<span class="string">&#x27;ZData&#x27;</span>,Tdi);</span><br><span class="line">    <span class="built_in">set</span>(ht,<span class="string">&#x27;String&#x27;</span>,[<span class="string">&#x27;冷藏&#x27;</span>,num2str(t(i)/<span class="number">3600</span>,<span class="string">&#x27;%2.1f&#x27;</span>),<span class="string">&#x27;小时后&#x27;</span>]);</span><br><span class="line">    pause(<span class="number">0.2</span>);</span><br><span class="line"></span><br><span class="line">%     f = getframe(gcf);</span><br><span class="line">%     IM = frame2im(f);</span><br><span class="line">%     [IM,<span class="built_in">map</span>] = rgb2ind(IM,<span class="number">256</span>);</span><br><span class="line">%     imwrite(IM,<span class="built_in">map</span>,filename,<span class="string">&#x27;gif&#x27;</span>,<span class="string">&#x27;WriteMode&#x27;</span>,<span class="string">&#x27;append&#x27;</span>,<span class="string">&#x27;DelayTime&#x27;</span>,<span class="number">0.2</span>);   </span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">    function [c,f,s] = pdefun(r,t,T,dT)</span><br><span class="line">        % 偏微分方程函数</span><br><span class="line">        c = <span class="number">1</span>/a;</span><br><span class="line">        f = dT;</span><br><span class="line">        s = <span class="number">0</span>;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    function u0 = pdeic(r)</span><br><span class="line">        % 初始条件函数</span><br><span class="line">        u0 = T0;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    function [pa,qa,pb,qb] = pdebc(ra,Ta,rb,Tb,t)</span><br><span class="line">        % 边值条件函数</span><br><span class="line">        pa = <span class="number">0</span>;</span><br><span class="line">        qa = <span class="number">1</span>;</span><br><span class="line">        pb = h*(Tb - Tinf);</span><br><span class="line">        qb = k;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="有效折射理论"><a href="#有效折射理论" class="headerlink" title="有效折射理论"></a>有效折射理论</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">clear;</span><br><span class="line">S11R=importdata(<span class="string">&#x27;real11.txt&#x27;</span>);</span><br><span class="line">S11I=importdata(<span class="string">&#x27;imag11.txt&#x27;</span>);</span><br><span class="line">S21R=importdata(<span class="string">&#x27;real21.txt&#x27;</span>);</span><br><span class="line">S21I=importdata(<span class="string">&#x27;imag21.txt&#x27;</span>);%导入数据</span><br><span class="line">real11=S11R.data;</span><br><span class="line">imag11=S11I.data;</span><br><span class="line">real21=S21R.data;</span><br><span class="line">imag21=S21I.data;</span><br><span class="line">N=<span class="number">1001</span>;</span><br><span class="line">d=<span class="number">0.00002</span>;</span><br><span class="line">c=<span class="number">3e8</span>;</span><br><span class="line"></span><br><span class="line">f=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">s11=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">s21=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">z1=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">z=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">kd=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">n1=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">n=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">e=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">miu=zeros(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:N</span><br><span class="line">    f(i)=real11(i,<span class="number">1</span>);</span><br><span class="line">    s11(i)=real11(i,<span class="number">2</span>)+<span class="number">1j</span>*imag11(i,<span class="number">2</span>);</span><br><span class="line">    s21(i)=real21(i,<span class="number">2</span>)+<span class="number">1j</span>*imag21(i,<span class="number">2</span>);</span><br><span class="line">end</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:N</span><br><span class="line">    z1(i)=sqrt(((<span class="number">1</span>+s11(i))^<span class="number">2</span>-s21(i)^<span class="number">2</span>)/((<span class="number">1</span>-s11(i))^<span class="number">2</span>-s21(i)^<span class="number">2</span>));</span><br><span class="line">    <span class="keyword">if</span> real(z1(i))&gt;<span class="number">0</span></span><br><span class="line">        z(i)=z1(i);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        z(i)=-z1(i);</span><br><span class="line">    end</span><br><span class="line">    kd(i)=d*<span class="number">2</span>*pi*f(i)*<span class="number">1e9</span>/c;</span><br><span class="line">    n1(i)=acos((<span class="number">1</span>-s11(i)^<span class="number">2</span>+s21(i)^<span class="number">2</span>)/<span class="number">2</span>/s21(i))/kd(i);</span><br><span class="line">    <span class="keyword">if</span> imag(n1(i))&gt;<span class="number">0</span></span><br><span class="line">        n(i)=n1(i);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        n(i)=-n1(i);</span><br><span class="line">    end</span><br><span class="line">    e(i)=n(i)/z(i);</span><br><span class="line">    miu(i)=n(i)*z(i);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">figure(<span class="number">1</span>)</span><br><span class="line"> subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"> plot(f,real(e),f,imag(e),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Permittivity (\epsilon)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(\epsilon)&#x27;</span>,<span class="string">&#x27;Im(\epsilon)&#x27;</span>);</span><br><span class="line"> </span><br><span class="line"> subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"> plot(f,real(miu),f,imag(miu),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Permeability (\mu)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(\mu)&#x27;</span>,<span class="string">&#x27;Im(\mu))&#x27;</span>);</span><br><span class="line"> </span><br><span class="line"> subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"> plot(f,real(z),f,imag(z),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Impedance (z)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(Z)&#x27;</span>,<span class="string">&#x27;Im(Z))&#x27;</span>);</span><br><span class="line"> </span><br><span class="line"> subplot(<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"> plot(f,real(n),f,imag(n),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Refractive Index (n)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(n)&#x27;</span>,<span class="string">&#x27;Im(n))&#x27;</span>);</span><br><span class="line"> hold on</span><br><span class="line"> plot(f,f-f,<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line"> %---------------------------------------------------------</span><br><span class="line"> figure(<span class="number">2</span>)</span><br><span class="line"> plot(f,real(miu),f,imag(miu),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Permeability (\mu)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(\mu)&#x27;</span>,<span class="string">&#x27;Im(\mu))&#x27;</span>);</span><br><span class="line"></span><br><span class="line"> figure(<span class="number">3</span>)</span><br><span class="line"> plot(f,real(e),f,imag(e),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Permittivity (\epsilon)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(\epsilon)&#x27;</span>,<span class="string">&#x27;Im(\epsilon)&#x27;</span>);</span><br><span class="line"> </span><br><span class="line"> figure(<span class="number">4</span>)</span><br><span class="line"> plot(f,real(z),f,imag(z),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27;Impedance (z)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(Z)&#x27;</span>,<span class="string">&#x27;Im(Z))&#x27;</span>);</span><br><span class="line"> </span><br><span class="line">  figure(<span class="number">5</span>)</span><br><span class="line"> plot(f,real(n),f,imag(n),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"> xlabel(<span class="string">&#x27;Frequency/THz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> ylabel(<span class="string">&#x27; refractive index(n)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"> legend(<span class="string">&#x27;Re(n)&#x27;</span>,<span class="string">&#x27;Im(n))&#x27;</span>);</span><br><span class="line"> yy=interp1(f,n,<span class="number">1</span>);</span><br><span class="line"> </span><br><span class="line">% figure(<span class="number">2</span>)</span><br><span class="line">% subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">% plot(f,real(epsilon1),f,imag(epsilon1),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>); xlabel(<span class="string">&#x27;Frequency/GHz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);ylabel(<span class="string">&#x27;Permittivity (\epsilon)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);legend(<span class="string">&#x27;Re(\epsilon1)&#x27;</span>,<span class="string">&#x27;Im(\epsilon1)&#x27;</span>,<span class="number">4</span>);</span><br><span class="line">% subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">% plot(f,real(epsilon2),f,imag(epsilon2),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>); xlabel(<span class="string">&#x27;Frequency/GHz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);ylabel(<span class="string">&#x27;Permittivity (\epsilon)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>); legend(<span class="string">&#x27;Re(\epsilon2)&#x27;</span>,<span class="string">&#x27;Im(\epsilon2)&#x27;</span>,<span class="number">4</span>);</span><br><span class="line">% subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">% plot(f,real(epsilon3),f,imag(epsilon3),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>); xlabel(<span class="string">&#x27;Frequency/GHz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>); ylabel(<span class="string">&#x27;Permittivity (\epsilon)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>); legend(<span class="string">&#x27;Re(\epsilon3)&#x27;</span>,<span class="string">&#x27;Im(\epsilon3)&#x27;</span>,<span class="number">4</span>);</span><br><span class="line">% subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">% plot(f,real(mur),f,imag(mur),<span class="string">&#x27;r:&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">2</span>); xlabel(<span class="string">&#x27;Frequency/GHz&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>); ylabel(<span class="string">&#x27;Permeability (\mu)&#x27;</span>,<span class="string">&#x27;fontsize&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;fontweight&#x27;</span>,<span class="string">&#x27;b&#x27;</span>);legend(<span class="string">&#x27;Re(\mu)&#x27;</span>,<span class="string">&#x27;Im(\mu))&#x27;</span>,<span class="number">4</span>);</span><br></pre></td></tr></table></figure><h2 id="折射率等差-插值"><a href="#折射率等差-插值" class="headerlink" title="折射率等差 插值"></a>折射率等差 插值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">clear Z;%%%%%%%%%%% a:原始数据</span><br><span class="line">z1=<span class="number">0.34</span>:<span class="number">0.02</span>:<span class="number">2.5</span>;z1=z1*<span class="number">1000</span>;</span><br><span class="line">n=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:length(a(:,<span class="number">1</span>)) % 原x数据 <span class="number">528</span></span><br><span class="line">    <span class="keyword">for</span> m=<span class="number">1</span>:length(z1) % 等差后的数据 <span class="number">979</span></span><br><span class="line">        <span class="keyword">if</span> (a(i,<span class="number">1</span>)-z1(m)&lt;=<span class="number">0</span>)&amp;&amp;(a(i+<span class="number">1</span>,<span class="number">1</span>)-z1(m)&gt;<span class="number">0</span>)</span><br><span class="line">            y11(n,<span class="number">1</span>)=a(i,<span class="number">2</span>)+(z1(n)-a(i,<span class="number">1</span>))*(a(i+<span class="number">1</span>,<span class="number">2</span>)-a(i,<span class="number">2</span>))/(a(i+<span class="number">1</span>,<span class="number">1</span>)-a(i,<span class="number">1</span>));</span><br><span class="line">            y22(n,<span class="number">1</span>)=a(i,<span class="number">3</span>)+(z1(n)-a(i,<span class="number">1</span>))*(a(i+<span class="number">1</span>,<span class="number">3</span>)-a(i,<span class="number">3</span>))/(a(i+<span class="number">1</span>,<span class="number">1</span>)-a(i,<span class="number">1</span>));</span><br><span class="line">            n=n+<span class="number">1</span>;</span><br><span class="line">        end</span><br><span class="line">    end  </span><br><span class="line">end</span><br><span class="line">%y11=y11<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">z1=z1&#x27;</span>;</span><br><span class="line"> %y22=y22<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">%Z=[z1,y11,y22];</span></span><br><span class="line"><span class="string">clear i;clear m;clear n;</span></span><br><span class="line"><span class="string">% clear y11;clear y22;clear z1;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% c=roundn(Z,-4); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[tmp I] = unique(a(:,1), &#x27;</span>first<span class="string">&#x27;);a = a(I,:); % 删除矩阵中第一列重复项</span></span><br><span class="line"><span class="string">wave = 300:20:2500;</span></span><br><span class="line"><span class="string">for ix = 2:length(a(1,:))</span></span><br><span class="line"><span class="string">    b(ix,:) = interp1(a(:,1),a(:,ix),wave,&#x27;</span>spline<span class="string">&#x27;);%  &#x27;</span>nearest<span class="string">&#x27;是最邻近插值， &#x27;</span>linea<span class="string">r&#x27;线性插值； &#x27;</span>spline<span class="string">&#x27;三次样条插值； &#x27;</span>pchip<span class="string">&#x27;立方插值．</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">b=b&#x27;</span>;</span><br><span class="line">wave=wave<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">b(:,1) = wave;</span></span><br></pre></td></tr></table></figure><h2 id="阻抗分析"><a href="#阻抗分析" class="headerlink" title="阻抗分析"></a>阻抗分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">function [R] = reflection_disp_norm(n_in, n_out, n_disp, d, <span class="keyword">lambda</span>)</span><br><span class="line">%REFLECTION_DISP_NORM Calculates the normal reflection spectrum of a multi-layer structure</span><br><span class="line">%   R: reflection spectrum</span><br><span class="line">% </span><br><span class="line">%   n_in: scalar/vector that specifies the refractive index of the incident material</span><br><span class="line">%   n_out: scalar/vector that specifies the refractive index of the substrate material</span><br><span class="line">%   n_disp: matrix that specifies the dispersive refractive indices of the structure</span><br><span class="line">%   d: vector that specifies the thicknesses of each layer</span><br><span class="line">%   theta_in: vector that specifies the wavelength of interest</span><br><span class="line">%%</span><br><span class="line">d = <span class="built_in">abs</span>(d);  %<span class="built_in">abs</span>取绝对值</span><br><span class="line">K = length(d); </span><br><span class="line">Z_out = <span class="number">1.</span>/n_out; </span><br><span class="line">Z_in = <span class="number">1</span>/n_in; </span><br><span class="line"></span><br><span class="line">%% Iteratively use the impedance method迭代地使用阻抗法</span><br><span class="line">Z_inter = Z_out; </span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span>:K</span><br><span class="line">    j = K-i+<span class="number">1</span>; </span><br><span class="line">    nj = n_disp(:, j); </span><br><span class="line">    dj = d(j); </span><br><span class="line">    Z_inter = <span class="number">1.</span>/nj .* (Z_inter + 1i*<span class="number">1.</span>/nj .* tan(<span class="number">2</span>*pi*nj./<span class="keyword">lambda</span> * dj)) ./ (<span class="number">1.</span>/nj + 1i.*Z_inter .* tan(<span class="number">2</span>*pi*nj./<span class="keyword">lambda</span> * dj));   </span><br><span class="line">end</span><br><span class="line">R = <span class="built_in">abs</span>((Z_inter - Z_in) ./ (Z_inter + Z_in)).^<span class="number">2</span>; </span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>计算ART 吸收&#x2F;透射&#x2F;反射</p><p>&#96;&#96;python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">% % M = reflectance_TE(<span class="number">1</span>,<span class="number">1</span>,[<span class="number">3</span> <span class="number">4</span>],[<span class="number">100</span> <span class="number">50</span>]*<span class="number">1e-9</span>,<span class="number">0</span>,<span class="number">400</span>*<span class="number">1e-9</span>)</span><br><span class="line">% n0 = <span class="number">1</span>;</span><br><span class="line">% nsubs = <span class="number">1</span>;</span><br><span class="line">% n_layers = [<span class="number">3</span> <span class="number">4</span>];</span><br><span class="line">% d_layers = [<span class="number">100</span> <span class="number">50</span>]*<span class="number">1e-9</span>;</span><br><span class="line">% theta_in = <span class="number">0</span>;</span><br><span class="line">% <span class="keyword">lambda</span> = <span class="number">400</span>*<span class="number">1e-9</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function [R_TE,T_TE] = reflectance_TE(n0,nsubs,n_layers,d_layers,theta_in,<span class="keyword">lambda</span>)</span><br><span class="line"></span><br><span class="line">% Mohammad Asif Zaman</span><br><span class="line">% June <span class="number">3</span> <span class="keyword">and</span> <span class="number">4</span>, <span class="number">2018</span></span><br><span class="line">% Reflectance calculaiton</span><br><span class="line">% Arguments: </span><br><span class="line">% n0 = refractive index of the <span class="built_in">input</span> medium</span><br><span class="line">% nsubs = refractive index of the substrate</span><br><span class="line">% n_layers = refractive indices of the thin-film layers</span><br><span class="line">% theta_in = incident angle (<span class="keyword">in</span> radians)</span><br><span class="line">% <span class="keyword">lambda</span> = wavelength</span><br><span class="line">% R_TE = output = reflectivity</span><br><span class="line">% -------------------------------- %</span><br><span class="line">%                                  %</span><br><span class="line">%               Air                %  </span><br><span class="line">%                                  %  </span><br><span class="line">% -------------------------------- % </span><br><span class="line">%             Layer <span class="number">1</span>              %</span><br><span class="line">% -------------------------------- %</span><br><span class="line">%             Layer <span class="number">2</span>              %</span><br><span class="line">% -------------------------------- %</span><br><span class="line">%                .                 %</span><br><span class="line">%                .                 %</span><br><span class="line">%                .                 %</span><br><span class="line">% -------------------------------- %</span><br><span class="line">%             Layer N              %  </span><br><span class="line">% -------------------------------- %</span><br><span class="line">%                                  %</span><br><span class="line">%            Substrate             %   </span><br><span class="line">%                                  %   </span><br><span class="line">% -------------------------------- %</span><br><span class="line">% We calculate the parameter theta, Z, <span class="keyword">and</span> eta on <span class="built_in">all</span> layers</span><br><span class="line">% There are N dielectric layers. The air (<span class="keyword">or</span> other) layer on top. And a</span><br><span class="line">% substrate layer on bottom. So, total N+<span class="number">2</span> layers.</span><br><span class="line">% We define (N+<span class="number">2</span>)x <span class="number">1</span> matrices <span class="keyword">for</span> <span class="built_in">all</span> quantities.</span><br><span class="line">% Haus book.</span><br><span class="line"></span><br><span class="line">Z0 = <span class="number">377</span>;</span><br><span class="line"></span><br><span class="line">theta_out = asin(n0*sin(theta_in)/nsubs); % 正入射=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">N = length(n_layers);   % 结构层数</span><br><span class="line">n = zeros(N+<span class="number">2</span>,<span class="number">1</span>);       % 材料折射率</span><br><span class="line">theta =  zeros(N+<span class="number">2</span>,<span class="number">1</span>);  % 入射角度</span><br><span class="line">Z = zeros(N+<span class="number">2</span>,<span class="number">1</span>);       % 阻抗</span><br><span class="line">eta = zeros(N+<span class="number">2</span>,<span class="number">1</span>);     % 导纳</span><br><span class="line"></span><br><span class="line">n(<span class="number">1</span>) = n0;              % 空气折射率</span><br><span class="line">n(end) = nsubs;         % 最底层 基底折射率</span><br><span class="line">n(<span class="number">2</span>:end-<span class="number">1</span>) = n_layers;% 中间层为多层薄膜结构</span><br><span class="line"></span><br><span class="line">theta(<span class="number">1</span>) = theta_in;    % 入射角度=<span class="number">0</span></span><br><span class="line">theta(end) = theta_out; % 出射角度=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">Z(<span class="number">1</span>) = Z0./(n(<span class="number">1</span>)*cos(theta(<span class="number">1</span>)));        % 第一层阻抗</span><br><span class="line">Z(end) = Z0./(n(end)*cos(theta(end)));  % 最底层阻抗</span><br><span class="line"></span><br><span class="line">eta(<span class="number">1</span>) = Z0./(cos(theta(<span class="number">1</span>)) .*n(<span class="number">1</span>));        % 媒质<span class="number">1</span>波阻抗</span><br><span class="line">eta(end) = Z0./(cos(theta(end)) .*n(end));  % 媒质end波阻抗</span><br><span class="line"></span><br><span class="line">Matrices = [<span class="number">1</span>,<span class="number">0</span>;<span class="number">0</span>,<span class="number">1</span>];</span><br><span class="line">Mcon = Matrices(:,:,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> k = N:-<span class="number">1</span>:<span class="number">1</span>    % Work backwards <span class="keyword">from</span> the last layer. </span><br><span class="line">%   k represents index over the dielectric layers.</span><br><span class="line">    m = k + <span class="number">1</span>;    % Layer index going <span class="keyword">from</span> N+<span class="number">1</span> to <span class="number">2.</span> </span><br><span class="line">                  % since we know the parameters <span class="keyword">for</span> layer <span class="number">1</span> <span class="keyword">and</span> layer N+<span class="number">2.</span></span><br><span class="line">    </span><br><span class="line">    % Calculate parameters at m layer using data <span class="keyword">from</span> (m+<span class="number">1</span>) layer </span><br><span class="line">    theta(m) = asin( n(m+<span class="number">1</span>)*sin( theta(m+<span class="number">1</span>) ) / n(m)  );        % 入射角度=<span class="number">0</span></span><br><span class="line">    eta(m) = Z0./( cos(theta(m)) .*n(m) );</span><br><span class="line">    </span><br><span class="line">    tmp = <span class="number">2</span>*pi*n(m)/<span class="keyword">lambda</span> * cos(theta(m)) * d_layers(k);       % delta</span><br><span class="line">    Z(m) = eta(m)*( Z(m+<span class="number">1</span>) + i*eta(m)*tan(tmp) ) ./ (eta(m) + i*Z(m+<span class="number">1</span>)*tan(tmp));</span><br><span class="line">    %%</span><br><span class="line">    eta1(m) = n(m)*cos(theta(m));</span><br><span class="line">    Matrices(:,:,m) = [cos(tmp),-1i*sin(tmp)/eta1(m);</span><br><span class="line">        -1i*eta1(m)*sin(tmp),cos(tmp)]; % matrices de cada capa</span><br><span class="line">    Mcon = Mcon*Matrices(:,:,m);</span><br><span class="line">end</span><br><span class="line">BC = Mcon*[<span class="number">1</span>;n(end)*cos(theta(end))];</span><br><span class="line">% BC = Mcon*[<span class="number">1</span>;<span class="number">1</span>];</span><br><span class="line">Y = BC(<span class="number">2</span>)/BC(<span class="number">1</span>);</span><br><span class="line">eta_o = n0*cos(theta_in);</span><br><span class="line">% theta</span><br><span class="line">% eta</span><br><span class="line">% Z</span><br><span class="line">% R_TE = ((eta_o-Y)/(eta_o+Y))*conj((eta_o-Y)/(eta_o+Y)); % reflectancia</span><br><span class="line">% R_TE = <span class="built_in">abs</span>( (Z(<span class="number">2</span>)-Z(<span class="number">1</span>) ) ./ ( Z(<span class="number">2</span>)+Z(<span class="number">1</span>) ) ).^<span class="number">2</span>;</span><br><span class="line">R_TE = ( (Z(<span class="number">2</span>)-Z(<span class="number">1</span>) ) / ( Z(<span class="number">2</span>)+Z(<span class="number">1</span>) ) )    *conj( (Z(<span class="number">2</span>)-Z(<span class="number">1</span>) ) / ( Z(<span class="number">2</span>)+Z(<span class="number">1</span>) ) );</span><br><span class="line">T_TE = real(eta(end)/eta(<span class="number">1</span>))* <span class="built_in">abs</span>( (   <span class="number">2</span>*Z(<span class="number">2</span>) ) ./ ( Z(<span class="number">2</span>)+Z(<span class="number">1</span>) ) ).^<span class="number">2</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">clear <span class="built_in">all</span>;</span><br><span class="line">%% Constantes</span><br><span class="line">c_speed = <span class="number">3e17</span>;% 光速/nm</span><br><span class="line">hbar = <span class="number">6.582119514e-16</span>; %eV*s</span><br><span class="line">J = <span class="number">2.6544e-3</span>;% factor admitancia</span><br><span class="line"></span><br><span class="line">Cavidad = &#123;<span class="string">&#x27;sustrato&#x27;</span>,<span class="string">&#x27;plata&#x27;</span>,<span class="string">&#x27;silica&#x27;</span>,<span class="string">&#x27;TDBC&#x27;</span>,<span class="string">&#x27;silica&#x27;</span>,<span class="string">&#x27;plata&#x27;</span>,<span class="string">&#x27;aire&#x27;</span>&#125;;</span><br><span class="line">Ncapas = length(Cavidad);           % layer = <span class="number">7</span></span><br><span class="line">polarizacion = <span class="number">2</span>; %TM = <span class="number">1</span>, TE = <span class="number">2</span>   % TE-polarization</span><br><span class="line"></span><br><span class="line">dEntrada = <span class="number">0</span>;                       % thickness</span><br><span class="line">dAg = <span class="number">30</span>;</span><br><span class="line">dTDBC = <span class="number">20</span>;</span><br><span class="line">dSilicaL = <span class="number">50</span>;</span><br><span class="line">dSilicaR = <span class="number">50</span>;</span><br><span class="line">dS = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">n0 = <span class="number">1</span>;                             % 空气折射率</span><br><span class="line">nEntrada = <span class="number">1.4587</span>;                  % 入射折射率</span><br><span class="line">nS = <span class="number">1</span>;                             % 基底折射率</span><br><span class="line"></span><br><span class="line">%% Definici髇 de par醡etros</span><br><span class="line">x = <span class="number">200</span>;</span><br><span class="line">y = <span class="number">200</span>;</span><br><span class="line">Emin = <span class="number">1.5</span>; % eV</span><br><span class="line">Emax = <span class="number">3.2</span>; % eV</span><br><span class="line"><span class="keyword">lambda</span> = linspace(<span class="number">2</span>*pi*c_speed*hbar/Emax,<span class="number">2</span>*pi*c_speed*hbar/Emin,y); % 波长/nm </span><br><span class="line"></span><br><span class="line">theta = <span class="number">0</span>;%(pi/<span class="number">180</span>)*linspace(<span class="number">0</span>,<span class="number">90</span>,x); % 入射角度-<span class="number">200</span>扫描点</span><br><span class="line">w1 = <span class="number">2</span>*pi*c_speed./<span class="keyword">lambda</span>;</span><br><span class="line">E = hbar*w1;</span><br><span class="line">ko = <span class="number">2</span>*pi./<span class="keyword">lambda</span>;</span><br><span class="line"></span><br><span class="line">kx = zeros(y,x);</span><br><span class="line">R = zeros(length(theta),length(<span class="keyword">lambda</span>));</span><br><span class="line">T = zeros(length(theta),length(<span class="keyword">lambda</span>));</span><br><span class="line">A = zeros(length(theta),length(<span class="keyword">lambda</span>));</span><br><span class="line"></span><br><span class="line">% <span class="keyword">for</span> i = <span class="number">1</span>:y</span><br><span class="line">%     <span class="keyword">for</span> ii = <span class="number">1</span>:x</span><br><span class="line">%         kx(i,ii) = n0*ko(i)*sin(theta(ii)); % 入射角度</span><br><span class="line">%     end</span><br><span class="line">% end</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:length(theta) % incident-angle</span><br><span class="line"><span class="keyword">for</span> l = <span class="number">1</span>:length(<span class="keyword">lambda</span>) % wavelength</span><br><span class="line">    %% Variables</span><br><span class="line">    w = <span class="number">2</span>*pi*c_speed/<span class="keyword">lambda</span>(l);</span><br><span class="line"></span><br><span class="line">eps5a = lorentzAg(w); % silver</span><br><span class="line">eps5a_real = real(eps5a);</span><br><span class="line">eps5a_im = imag(eps5a);</span><br><span class="line"></span><br><span class="line">eps5g = lorentzTDBC(w); % TDBC</span><br><span class="line">eps5g_real = real(eps5g);</span><br><span class="line">eps5g_im = imag(eps5g);</span><br><span class="line"></span><br><span class="line">% Indice de refracci髇 Ag y TBDC</span><br><span class="line">nAg = (eps5a_real+1i*eps5a_im)^<span class="number">0.5</span>;</span><br><span class="line">nTDBC = (eps5g_real+1i*eps5g_im)^<span class="number">0.5</span>;</span><br><span class="line">n_silica = sellmeier(<span class="keyword">lambda</span>(l));</span><br><span class="line">%% 相邻两个界面的入射角度 theta</span><br><span class="line">theta_Entrada = theta(k); % 入射角度</span><br><span class="line">theta_Ag1 = asin((nEntrada/nAg)*sin(theta_Entrada));</span><br><span class="line">theta_silicaL = asin((nAg/n_silica)*sin(theta_Ag1));</span><br><span class="line">theta_TDBC = asin((n_silica/nTDBC)*sin(theta_silicaL));</span><br><span class="line">theta_silicaR = asin((nTDBC/n_silica)*sin(theta_TDBC));</span><br><span class="line">theta_Ag2 = asin((n_silica/nAg)*sin(theta_silicaR));</span><br><span class="line">theta_S = asin((nAg/nS)*sin(theta_Ag2)); % </span><br><span class="line">%% delta</span><br><span class="line">deltaEntrada = <span class="number">2</span>*pi*nEntrada*dEntrada*cos(theta_Entrada)/<span class="keyword">lambda</span>(l);</span><br><span class="line">deltaAg1 = <span class="number">2</span>*pi*nAg*dAg*cos(theta_Ag1)/<span class="keyword">lambda</span>(l);</span><br><span class="line">delta_silicaL = <span class="number">2</span>*pi*n_silica*dSilicaL*cos(theta_silicaL)/<span class="keyword">lambda</span>(l);</span><br><span class="line">deltaTDBC = <span class="number">2</span>*pi*nTDBC*dTDBC*cos(theta_TDBC)/<span class="keyword">lambda</span>(l);</span><br><span class="line">delta_silicaR = <span class="number">2</span>*pi*n_silica*dSilicaR*cos(theta_silicaR)/<span class="keyword">lambda</span>(l);</span><br><span class="line">deltaAg2 = <span class="number">2</span>*pi*nAg*dAg*cos(theta_Ag2)/<span class="keyword">lambda</span>(l);</span><br><span class="line">deltaS = <span class="number">2</span>*pi*nS*dS*cos(theta_S)/<span class="keyword">lambda</span>(l);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> polarizacion == <span class="number">1</span></span><br><span class="line">        etaEntrada = nEntrada/cos(theta_Entrada);        etaAg1 = nAg/cos(theta_Ag1);</span><br><span class="line">        eta_silicaL = n_silica/cos(theta_silicaL);        etaTDBC = nTDBC/cos(theta_TDBC);</span><br><span class="line">        eta_silicaR = n_silica/cos(theta_silicaR);        etaAg2 = nAg/cos(theta_Ag2);</span><br><span class="line">        etaS = nS/cos(theta_S);        eta_m = etaS;        eta_o = n0/cos(theta(k));</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        etaEntrada = nEntrada*cos(theta_Entrada);</span><br><span class="line">        etaAg1 = nAg*cos(theta_Ag1);</span><br><span class="line">        eta_silicaL = n_silica*cos(theta_silicaL);</span><br><span class="line">        etaTDBC = nTDBC*cos(theta_TDBC);</span><br><span class="line">        eta_silicaR = n_silica*cos(theta_silicaR);</span><br><span class="line">        etaAg2 = nAg*cos(theta_Ag2);</span><br><span class="line">        etaS = nS*cos(theta_S);</span><br><span class="line">        eta_m = etaS;</span><br><span class="line">        eta_o = n0*cos(theta(k));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">delta = [deltaEntrada deltaAg1 delta_silicaL deltaTDBC delta_silicaR deltaAg2 deltaS];</span><br><span class="line">eta   = [etaEntrada   etaAg1   eta_silicaL   etaTDBC   eta_silicaR   etaAg2   etaS  ];</span><br><span class="line"></span><br><span class="line">%% Matriz de matrices de cada capa</span><br><span class="line">Matrices = [cos(deltaEntrada),-1i*sin(deltaEntrada)/etaEntrada; % primera matriz</span><br><span class="line">           -1i*etaEntrada*sin(deltaEntrada),cos(deltaEntrada)];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i=<span class="number">2</span>:Ncapas</span><br><span class="line">        Matrices(:,:,i) = [cos(delta(i)),-1i*sin(delta(i))/eta(i); % matrices de cada capa</span><br><span class="line">                           -1i*eta(i)*sin(delta(i)),cos(delta(i))];</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">%% Matriz <span class="keyword">del</span> conjunto</span><br><span class="line">Mcon = Matrices(:,:,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i=<span class="number">2</span>:Ncapas</span><br><span class="line">        Mcon = Mcon*Matrices(:,:,i); % matriz <span class="keyword">del</span> conjunto</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">%% C醠culo de la reflectancia</span><br><span class="line">BC = Mcon*[<span class="number">1</span>;etaS];</span><br><span class="line">Y = BC(<span class="number">2</span>)/BC(<span class="number">1</span>);</span><br><span class="line">R(k,l) = ((eta_o-Y)/(eta_o+Y))*conj((eta_o-Y)/(eta_o+Y)); % reflectancia</span><br><span class="line">A(k,l) = (<span class="number">4</span>*eta_o*real(BC(<span class="number">1</span>)*conj(BC(<span class="number">2</span>))-eta_m))   /((eta_o*BC(<span class="number">1</span>)+BC(<span class="number">2</span>))*conj(eta_o*BC(<span class="number">1</span>)+BC(<span class="number">2</span>))); % absorbancia</span><br><span class="line">T(k,l) = (<span class="number">4</span>*eta_o*real(eta_m))/((eta_o*BC(<span class="number">1</span>)+BC(<span class="number">2</span>))*conj(eta_o*BC(<span class="number">1</span>)+BC(<span class="number">2</span>))); % transmitancia</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%% Curva de Dispersi髇</span><br><span class="line">% figure (<span class="number">1</span>)</span><br><span class="line">% E = repmat(E,x,<span class="number">1</span>);</span><br><span class="line">% h = surf(kx<span class="string">&#x27;,E,R);</span></span><br><span class="line"><span class="string">% colormap (parula)</span></span><br><span class="line"><span class="string">% ylim([Emin,Emax])</span></span><br><span class="line"><span class="string">% xlim([0,max(max(kx))+0.05*max(max(kx))])</span></span><br><span class="line"><span class="string">% set(h,&#x27;</span>edgecolo<span class="string">r&#x27;,&#x27;</span>none<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">% colorbar</span></span><br><span class="line"><span class="string">% view(2)</span></span><br><span class="line"><span class="string">% ylabel(&#x27;</span>Energ韆 [eV]<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">% xlabel(&#x27;</span>k_&#123;||&#125; [nm^&#123;-<span class="number">1</span>&#125;]<span class="string">&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% Reflectancia, Absortancia y Transmitancia</span></span><br><span class="line"><span class="string">figure (2)</span></span><br><span class="line"><span class="string">plot(lambda,T(1,:),&#x27;</span><span class="string">b&#x27;,lambda,R(1,:),&#x27;</span><span class="string">r&#x27;,lambda,A(1,:),&#x27;</span>g<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">legend(&#x27;</span>\fontsize&#123;<span class="number">9</span>&#125;Transmitancia<span class="string">&#x27;,&#x27;</span>\fontsize&#123;<span class="number">9</span>&#125;Reflectancia<span class="string">&#x27;,&#x27;</span>\fontsize&#123;<span class="number">9</span>&#125;Absortancia<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">xlabel(&#x27;</span>Longitud de onda [nm]<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">ylabel(&#x27;</span>Intensidad<span class="string">&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><h2 id="MATLAB绘制不同Ra-距离不同的图片"><a href="#MATLAB绘制不同Ra-距离不同的图片" class="headerlink" title="MATLAB绘制不同Ra 距离不同的图片"></a>MATLAB绘制不同Ra 距离不同的图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ix = <span class="number">1</span>:length(a(:,<span class="number">1</span>)) % 最里面</span><br><span class="line">        plot(a(ix,<span class="number">1</span>),a(ix,<span class="number">2</span>),<span class="string">&#x27;ko&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">1.2</span>);hold on; </span><br><span class="line">end  </span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;times new Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">20</span>);</span><br><span class="line">axis([<span class="number">0</span>,<span class="number">0.9</span>,<span class="number">0</span>,<span class="number">0.9</span>]);box on;hold on;</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;linewidth&#x27;</span>,<span class="number">1.2</span>);</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;XTick&#x27;</span>,[<span class="number">0</span>:<span class="number">0.3</span>:<span class="number">0.9</span>]);   % 修改x轴坐标间隔</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;YTick&#x27;</span>,[<span class="number">0</span>:<span class="number">0.3</span>:<span class="number">0.9</span>]);   % 修改x轴坐标间隔</span><br><span class="line">% xlabel(<span class="string">&#x27;Δ&#123;\itA&#125;_&#123;sol&#125;&#x27;</span>); ylabel(<span class="string">&#x27;&#123;\itT&#125;_&#123;lum&#125;&#x27;</span>);</span><br><span class="line"><span class="keyword">for</span> ix = <span class="number">1</span>:length(a(:,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> a(ix,<span class="number">4</span>) &lt;= <span class="number">0.02</span></span><br><span class="line">        plot(a(ix,<span class="number">1</span>),a(ix,<span class="number">2</span>),<span class="string">&#x27;ro&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">0.6</span>);hold on;</span><br><span class="line">    end</span><br><span class="line">    <span class="keyword">if</span> a(ix,<span class="number">4</span>) &lt;= <span class="number">0.04</span> &amp;&amp; a(ix,<span class="number">4</span>) &gt; <span class="number">0.02</span></span><br><span class="line">        plot(a(ix,<span class="number">1</span>),a(ix,<span class="number">2</span>),<span class="string">&#x27;b.&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">0.1</span>);hold on;</span><br><span class="line">    end</span><br><span class="line">    <span class="keyword">if</span> a(ix,<span class="number">4</span>) &gt; <span class="number">0.04</span></span><br><span class="line">        plot(a(ix,<span class="number">1</span>),a(ix,<span class="number">2</span>),<span class="string">&#x27;k.&#x27;</span>,<span class="string">&#x27;LineWidth&#x27;</span>,<span class="number">0.05</span>);hold on;</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">end</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;FontName&#x27;</span>,<span class="string">&#x27;times new Roman&#x27;</span>,<span class="string">&#x27;FontSize&#x27;</span>,<span class="number">20</span>);</span><br><span class="line">axis([<span class="number">0</span>,<span class="number">0.9</span>,<span class="number">0</span>,<span class="number">0.9</span>]);box on;hold on;</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;linewidth&#x27;</span>,<span class="number">1.2</span>);</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;XTick&#x27;</span>,[<span class="number">0</span>:<span class="number">0.3</span>:<span class="number">0.9</span>]);   % 修改x轴坐标间隔</span><br><span class="line"><span class="built_in">set</span>(gca,<span class="string">&#x27;YTick&#x27;</span>,[<span class="number">0</span>:<span class="number">0.3</span>:<span class="number">0.9</span>]);   % 修改x轴坐标间隔</span><br><span class="line">% xlabel(<span class="string">&#x27;Δ&#123;\itA&#125;_&#123;sol&#125;&#x27;</span>); ylabel(<span class="string">&#x27;&#123;\itT&#125;_&#123;lum&#125;&#x27;</span>);</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="TETM折射率有效折射率"><a href="#TETM折射率有效折射率" class="headerlink" title="TETM折射率有效折射率"></a>TETM折射率有效折射率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">a1 = <span class="number">3.47</span>;</span><br><span class="line">a2 = <span class="number">1.45</span>;</span><br><span class="line">f = <span class="number">0</span>:<span class="number">0.01</span>:<span class="number">1</span>;</span><br><span class="line">t = <span class="number">0</span>;</span><br><span class="line">Py = <span class="number">400</span>;</span><br><span class="line">lam = <span class="number">1550</span>;</span><br><span class="line"><span class="keyword">for</span>   ix = f</span><br><span class="line">    t = t+<span class="number">1</span>;</span><br><span class="line">    te = (    ix/a2^<span class="number">2</span>+(<span class="number">1</span>-ix)/a1^<span class="number">2</span>   )^-<span class="number">0.5</span>;</span><br><span class="line">    n_TE(t,<span class="number">1</span>) = te;</span><br><span class="line">    tm = (    ix*a2^<span class="number">2</span>+(<span class="number">1</span>-ix)*a1^<span class="number">2</span>   )^<span class="number">0.5</span>;</span><br><span class="line">    n_TM(t,<span class="number">1</span>) = tm;</span><br><span class="line">    te2 = te*(<span class="number">1</span>+pi^<span class="number">2</span>/<span class="number">3</span>*(Py/lam)^<span class="number">2</span>*ix^<span class="number">2</span>*(<span class="number">1</span>-ix)^<span class="number">2</span>*( a1^<span class="number">2</span>-a2^<span class="number">2</span>)^<span class="number">2</span>* tm^<span class="number">2</span>*(te/a1/a2)^<span class="number">4</span>    )^<span class="number">0.5</span>;</span><br><span class="line">    %     te*(<span class="number">1</span>+pi*pi/<span class="number">3</span>*(Py/lam)^<span class="number">2</span>*f^<span class="number">2</span>*(<span class="number">1</span>-f)^<span class="number">2</span>*( a1^<span class="number">2</span>-a1^<span class="number">2</span>)^<span class="number">2</span>* tm^<span class="number">2</span>*(te/a1/a2)^<span class="number">4</span>    )^<span class="number">0.5</span>;</span><br><span class="line">    n_TE2(t,<span class="number">1</span>) = te2;</span><br><span class="line">    tm2 = tm*(<span class="number">1</span>+pi^<span class="number">2</span>/<span class="number">3</span>*(Py/lam)^<span class="number">2</span>*ix^<span class="number">2</span>*(<span class="number">1</span>-ix)^<span class="number">2</span>*( a1^<span class="number">2</span>-a2^<span class="number">2</span>)^<span class="number">2</span>/tm^<span class="number">2</span>   )^<span class="number">0.5</span>;</span><br><span class="line">    n_TM2(t,<span class="number">1</span>) = tm2;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">plot(f,n_TE);hold on;plot(f,n_TM);      hold on;plot(f,n_TE2);hold on;plot(f,n_TM2);</span><br></pre></td></tr></table></figure><h2 id="chen-shuo神经网络生成-代码"><a href="#chen-shuo神经网络生成-代码" class="headerlink" title="chen_shuo神经网络生成_代码"></a>chen_shuo神经网络生成_代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">% clc;clear <span class="built_in">all</span>;</span><br><span class="line">% close <span class="built_in">all</span>;</span><br><span class="line">t1=clock;</span><br><span class="line">%%</span><br><span class="line">% wave_range = [<span class="number">300</span>:<span class="number">10</span>:<span class="number">2000</span>, <span class="number">2100</span>:<span class="number">100</span>:<span class="number">10000</span>];</span><br><span class="line">wave_range = <span class="number">400</span>:<span class="number">5</span>:<span class="number">1300</span>; save(<span class="string">&#x27;wave_range.mat&#x27;</span>,<span class="string">&#x27;wave_range&#x27;</span>);</span><br><span class="line">M_mat = nk_data(wave_range);</span><br><span class="line">shoulian = <span class="number">1</span>;save(<span class="string">&#x27;shoulian.mat&#x27;</span>,<span class="string">&#x27;shoulian&#x27;</span>);</span><br><span class="line">%%</span><br><span class="line">% <span class="number">1</span>  MgF2     % <span class="number">11</span> W</span><br><span class="line">% <span class="number">2</span>  SiO2     % <span class="number">12</span> Mo</span><br><span class="line">% <span class="number">3</span>  Al2O3    % <span class="number">13</span> Cr</span><br><span class="line">% <span class="number">4</span>  Si3N4    % <span class="number">14</span> Fe</span><br><span class="line">% <span class="number">5</span>  Si3N41直 % <span class="number">15</span> Ti</span><br><span class="line">% <span class="number">6</span>  TiO2     % <span class="number">16</span> Ag</span><br><span class="line">% <span class="number">7</span>  SiN      % <span class="number">17</span> Al</span><br><span class="line">% <span class="number">8</span>  Ge       % <span class="number">18</span> Cu-<span class="number">0.29</span>-SiO2</span><br><span class="line">% <span class="number">9</span>  Au</span><br><span class="line">% <span class="number">10</span> Cu</span><br><span class="line">load(<span class="string">&#x27;matlab.mat&#x27;</span>); % 结构<span class="number">2</span>--尺寸</span><br><span class="line">% <span class="keyword">for</span> ink = <span class="number">9</span>:<span class="number">17</span>     cool(<span class="number">1</span>,<span class="number">1</span>) = ink   ;</span><br><span class="line"></span><br><span class="line">lb = cool(:,<span class="number">2</span>)<span class="string">&#x27;;st1 = cool(:,3)&#x27;</span>;ub = cool(:,<span class="number">4</span>)<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">nk_M = M_mat(:,cool(1,1)); for nx = 2:length(cool(:,1))  nk_M = [nk_M,M_mat(:,cool(nx,1))]; end</span></span><br><span class="line"><span class="string">nk_M = [nk_M M_mat(:,2)];   save(&#x27;</span>nk_M.mat<span class="string">&#x27;,&#x27;</span>nk_M<span class="string">&#x27;);    % save nk_M;    % W基底</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PopulationSize_Data = 0.1*10000 ;  Generations = 8; StallGenerations = Generations; InitialPopulation=st1; nvars = length(lb); % number of variables</span></span><br><span class="line"><span class="string">[x,Aa,exitflag,output,population,score] = GA_code(nvars,lb,ub,PopulationSize_Data,Generations,StallGenerations,InitialPopulation);</span></span><br><span class="line"><span class="string">Aa = -roundn(Aa,-4);   t2=clock;tc(t2,t1);x=roundn(x&#x27;</span>,-<span class="number">0</span>);    wave_range = wave_range<span class="string">&#x27;;</span></span><br><span class="line"><span class="string">%% plot</span></span><br><span class="line"><span class="string">% for dd=5:2:30</span></span><br><span class="line"><span class="string">%     x(3,1)=dd;</span></span><br><span class="line"><span class="string">%%</span></span><br><span class="line"><span class="string">cs1 = 2:2:20;</span></span><br><span class="line"><span class="string">cs2 = 10:10:300;</span></span><br><span class="line"><span class="string">cs3 = 2:2:20;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[j1,j2,j3]=ndgrid(cs1,cs2,cs3);</span></span><br><span class="line"><span class="string">com=[j1(:),j2(:),j3(:)];clear cs1 cs2 cs3 j1 j2 j3</span></span><br><span class="line"><span class="string">for ix = 1:length(com)</span></span><br><span class="line"><span class="string">    x = com(ix,:)&#x27;</span>;</span><br><span class="line">    shoulian = <span class="number">2</span>;save(<span class="string">&#x27;shoulian.mat&#x27;</span>,<span class="string">&#x27;shoulian&#x27;</span>);</span><br><span class="line">    RTA = <span class="built_in">round</span>(objectivefunction(x<span class="string">&#x27;),4); </span></span><br><span class="line"><span class="string">    R1(:,ix) = RTA(:,1);</span></span><br><span class="line"><span class="string">    T1(:,ix) = RTA(:,2);</span></span><br><span class="line"><span class="string">    A1(:,ix) = RTA(:,3);</span></span><br><span class="line"><span class="string">    R2(:,ix) = RTA(:,4);</span></span><br><span class="line"><span class="string">    T2(:,ix) = RTA(:,5);</span></span><br><span class="line"><span class="string">    A2(:,ix) = RTA(:,6);</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">%  Tt(:,ink) = RTA(:,3);end</span></span><br><span class="line"><span class="string">figure(&#x27;</span>colo<span class="string">r&#x27;,[1 1 1]); </span></span><br><span class="line"><span class="string">plot(wave_range,RTA(:,1),&#x27;</span><span class="string">b&#x27;,  wave_range,RTA(:,2),&#x27;</span><span class="string">r&#x27;,  wave_range,RTA(:,3),&#x27;</span>g<span class="string">&#x27;,  &#x27;</span>LineWidth<span class="string">&#x27;,2);hold on;</span></span><br><span class="line"><span class="string">plot(wave_range,RTA(:,4),&#x27;</span>b--<span class="string">&#x27;,wave_range,RTA(:,5),&#x27;</span>r--<span class="string">&#x27;,wave_range,RTA(:,6),&#x27;</span>g--<span class="string">&#x27;,&#x27;</span>LineWidth<span class="string">&#x27;,2);hold on;</span></span><br><span class="line"><span class="string">legend(&#x27;</span>\fontsize&#123;<span class="number">20</span>&#125;<span class="string">R&#x27;,&#x27;</span>\fontsize&#123;<span class="number">20</span>&#125;T<span class="string">&#x27;,&#x27;</span>\fontsize&#123;<span class="number">20</span>&#125;A<span class="string">&#x27;); xlabel(&#x27;</span>wavelength (nm)<span class="string">&#x27;);ylabel(&#x27;</span>Intensity<span class="string">&#x27;);</span></span><br><span class="line"><span class="string">set(gca,&#x27;</span>FontSize<span class="string">&#x27;,20);axis([min(wave_range),max(wave_range),0,1]);</span></span><br><span class="line"><span class="string">% aZ = [cool(:,1),x];% z_1 = [x ;-fval*100; A_spectrum];% A_avg(mx)=Aa; % print(gcf,&#x27;</span>-dpng<span class="string">&#x27;,&#x27;</span><span class="number">1.</span>png<span class="string">&#x27;);picture(1);  semilogx(wave_range,a,&#x27;</span>k<span class="string">&#x27;,&#x27;</span>LineWidth<span class="string">&#x27;,1);axis([300 20000 0 1]);</span></span><br><span class="line"><span class="string">clear st1 t1 t2 ub lb score population nx exitflag shoulian CrossoverFraction_Data InitialPopulationMatrix_Data MaxGenerations_Data;clear MaxStallGenerations_Data PopulationSize_Data nvars output ans</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 博客图片 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>latex相关</title>
      <link href="/2024/04/419e.html"/>
      <url>/2024/04/419e.html</url>
      
        <content type="html"><![CDATA[<h2 id="latex相关"><a href="#latex相关" class="headerlink" title="latex相关"></a>latex相关</h2><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">  \centering</span><br><span class="line">  \includegraphics[width=11cm]&#123;图片文件名&#125;</span><br><span class="line">  \caption&#123;注释&#125;</span><br><span class="line">  \label&#123;标签&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>引用图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~\ref&#123;标签&#125;~</span><br><span class="line">h：表示“here”，告诉LaTeX尽可能将图片放置在代码所在的位置。</span><br><span class="line">t：表示“top”，告诉LaTeX将图片放置在页面的顶部。</span><br><span class="line">b：表示“bottom”，告诉LaTeX将图片放置在页面的底部。</span><br><span class="line">p：表示“page”，告诉LaTeX将图片放置在一页独立成页，而不与文本混排。</span><br></pre></td></tr></table></figure><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>快捷键：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\label&#123;公式标签&#125;</span><br><span class="line">aaaaaaa</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><p>引用公式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\eqref&#123;公式标签&#125;</span><br></pre></td></tr></table></figure><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\section&#123;电致变色窗&#125;</span><br><span class="line">\subsection&#123;电致变色窗&#125;</span><br><span class="line">\subsubsection&#123;电致变色窗&#125;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\cite&#123;BUPT_Thesis_Format_2014&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dim$_&#123;\mathrm&#123;<span class="built_in">input</span>&#125;&#125;$</span><br><span class="line">CNN$_&#123;<span class="number">3</span>\times32\times32&#125;$和CNN$_&#123;<span class="number">6</span>\times8\times8&#125;$</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo个人博客搭建</title>
      <link href="/2024/04/hexo_build.html"/>
      <url>/2024/04/hexo_build.html</url>
      
        <content type="html"><![CDATA[<hr><h1 id="Hexo个人博客搭建"><a href="#Hexo个人博客搭建" class="headerlink" title="Hexo个人博客搭建"></a>Hexo个人博客搭建</h1><hr><ol><li>安装nodejs、git</li></ol><p>首先、需要安装 Node.js(<a href="http://nodejs.cn/download/">http://nodejs.cn/download/</a>) 和 Git(<a href="https://git-scm.com/downloads)%EF%BC%8C%E5%A4%A7%E5%AE%B6%E5%B0%BD%E9%87%8F%E5%8E%BB%E4%B8%8B%E8%BD%BD.exe%E6%89%A9%E5%B1%95%E5%90%8D%E7%9A%84%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%EF%BC%8C%E8%BF%99%E6%A0%B7%E7%9A%84%E5%A5%BD%E5%A4%84%E6%98%AF%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E3%80%81%E7%9C%81%E5%8E%BB%E4%BA%86%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E3%80%82%E5%AE%89%E8%A3%85%E7%89%88%E6%9C%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%9A%84%E7%89%88%E6%9C%AC%E3%80%82%E5%A6%82%E6%9E%9C%E6%9C%89%E9%97%AE%E9%A2%98%E5%8D%B8%E8%BD%BD%E5%AE%89%E8%A3%85%E6%97%A7%E7%89%88%E3%80%82">https://git-scm.com/downloads)，大家尽量去下载.exe扩展名的可执行文件，这样的好处是一键安装、省去了一些配置。安装版本也可以安装最新的版本。如果有问题卸载安装旧版。</a> </p><p><code>验证安装是否成功</code>，打开CMD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">验证Node.js的方法</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line">输入后能够显示版本说明安装成功</span><br></pre></td></tr></table></figure><ol><li><p>在文件夹中右击 <code>Git Bash Here</code> </p></li><li><p>使用npm命令安装Hexo，输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli <span class="comment"># 安装hexo可略过</span></span><br><span class="line">hexo init</span><br><span class="line">hexo clean <span class="comment"># 清除标签</span></span><br><span class="line">hexo new 我的新博客</span><br><span class="line">hexo g <span class="comment">#生成</span></span><br><span class="line">hexo s <span class="comment">#启动服务预览</span></span><br><span class="line">git config --<span class="keyword">global</span> user.name <span class="string">&quot;wbupt&quot;</span></span><br><span class="line">git config --<span class="keyword">global</span> user.email <span class="string">&quot;34@qq.com&quot;</span></span><br><span class="line">npm install hexo-deployer-git --save <span class="comment"># 安装上传文件</span></span><br><span class="line">npm install hexo-renderer-pug hexo-renderer-stylus <span class="comment"># 安装 butterfly插件</span></span><br><span class="line">npm install hexo-generator-index-pin-top --save <span class="comment"># 文章置顶插件</span></span><br><span class="line">npm install hexo-hide-posts --save <span class="comment"># 隐藏文章</span></span><br><span class="line">npm install hexo-math --save <span class="comment"># 安装公式</span></span><br><span class="line">npm install hexo-wordcount --save <span class="comment"># 安装字数统计</span></span><br><span class="line"></span><br><span class="line">hexo d <span class="comment">#部署</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="配置文件设置："><a href="#配置文件设置：" class="headerlink" title="配置文件设置："></a>配置文件设置：</h2><p><code>_config.yml</code> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">skip_render: <span class="comment">#跳过文件</span></span><br><span class="line">  - <span class="string">&quot;HTML/*&quot;</span></span><br><span class="line">  - <span class="string">&quot;about/*&quot;</span></span><br><span class="line">banner_img: <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Hexo个人博客搭建</span><br><span class="line">tags: [标签,标签<span class="number">2</span>,标签<span class="number">3</span>] <span class="comment"># 标题</span></span><br><span class="line">category: 沧州植物鱼<span class="comment"># 分类</span></span><br><span class="line">isTop: true</span><br><span class="line">abbrlink: <span class="number">20929</span><span class="comment"># 文章地址</span></span><br><span class="line">date: <span class="number">2023</span>-03-<span class="number">14</span> <span class="number">10</span>:<span class="number">19</span>:08<span class="comment"># 写作时间</span></span><br><span class="line">feature:</span><br><span class="line">sticky: <span class="number">101</span></span><br><span class="line">top: <span class="number">101</span></span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>刘玉敏签名</p><p><img src="https://s2.loli.net/2023/12/06/iNYf4C2aOPX3dK1.png" alt="刘玉敏签名"></p><p><img src="https://s2.loli.net/2024/04/25/e2gSuhyKQYfwT13.png" alt="刘玉敏"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北邮地图</title>
      <link href="/2024/04/7ed0.html"/>
      <url>/2024/04/7ed0.html</url>
      
        <content type="html"><![CDATA[<!DOCTYPE html><html lang="en"><head>     <meta charset="UTF-8">     <meta name="viewport" content="width=device-width, initial-scale=1.0">     <meta http-equiv="X-UA-Compatible" content="ie=edge">     <title>北邮地图</title>     <style>          :root {               --sushe-color: #d2a8ef;               --lvse-color: #62be9d;               --color-jiao_xue_lou: #cae270;               --color-shi_tang: #e36297;               --color-sports: #b7c2ee;               --entrance-color: #e2e8cd;               --jiashuqu-color: #09f5f5;               --color-li-fa: #bbf1d4; /* 理发店*/               /* 北边的路*/               --d: 20;               --a: 0.7;               --x_distance: var(--d);               --y_distance: 100;               --x_scale: var(--a);               --y_scale: var(--a);               --x_scale_little: var(--a);               --y_scale_little: var(--a);               /* 字体大小 */               --font_size_first: 10;               --font_size_second: 18;               --font_size_third: 5;          }          /* 1设置弹框 */          .modal {               display: none;               position: fixed;               z-index: 1;               left: 1px;               top: 30px;               width: 100%;               height: 300%;               overflow: auto;               background-color: rgba(0, 0, 0, 0.4);          }          .modal-content {               background-color: #fefefe;               margin: 10% auto;               padding: 20px;               border: 0 solid #888;               width: 50%;               border-radius: 15px;          }          .close {               color: #aaa;               float: right;               font-size: 28px;               font-weight: bold;               border-radius: 15px;          }          .close:hover,          .close:focus {               color: black;               text-decoration: none;               cursor: pointer;          }          /* 全局整体 */          .box {               color: black;               position: absolute;               letter-spacing: 0; /* 文字间水平距离 */               line-height: calc((var(--font_size_first) + 2 ) * 1px); /* 文字间垂直距离 */               display: flex;  /* 设置为flex布局 */               align-items: center; /* 垂直居中 */               justify-content: center; /* 水平居中 */               font-size: calc(var(--font_size_first) * 1px);               text-align: center;               top: 0px;               left: var(--d);               width: 300px;               height: 200px;          }          .beiyoukeji {               color: black;               background-color: #c75779;               z-index: 1;               left: calc((var(--x_distance) + 0)*var(--x_scale)*1px);               top: calc((var(--y_distance) + 100)*var(--y_scale)*1px);               width: calc(75 * var(--x_scale_little)*1px);               height: calc(130 * var(--y_scale_little)*1px);          }          .xue11 {               color: black;               background-color: var(--sushe-color);               left: calc((var(--x_distance) + 75)*var(--x_scale)*1px);               top: calc((var(--y_distance) + 100)*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little)*1px);               height: calc(45 * var(--y_scale_little)*1px);               z-index: 1;          }          .xue9 {               background-color: var(--sushe-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((190 + var(--y_distance))*var(--y_scale)*1px);               width: calc(73 * var(--x_scale_little)*1px);               height: calc(45 * var(--y_scale_little)*1px);               z-index: 1;          }          .j9 {               background-color: var(--color-jiao_xue_lou);               z-index: 1;               left: calc((148 + var(--x_distance))*var(--x_scale)*1px);               top: calc((190 + var(--y_distance))*var(--y_scale)*1px);               width: calc(27 * var(--x_scale_little) * 1px);               height: calc(45 * var(--y_scale_little) * 1px);          }          .x10 {               background-color: var(--sushe-color);               z-index: 1;               left: calc((191 + var(--x_distance))*var(--x_scale)*1px);               top: calc((130 + var(--y_distance))*var(--y_scale)*1px);               width: calc(105 * var(--x_scale_little) * 1px);               height: calc(100 * var(--y_scale_little) * 1px);          }          .jg {               background-color: var(--sushe-color);               left: calc((var(--x_distance) + 350)*var(--x_scale)*1px);               top: calc((var(--y_distance) + 130)*var(--y_scale)*1px);               width: calc(95 * var(--x_scale_little)*1px);               height: calc(60 * var(--y_scale_little)*1px);               z-index: 1;          }          .zhong_you {               background-color: #eec0c0;               line-height: calc((var(--font_size_first) + 0 ) * 1px);               font-size: calc(var(--font_size_first) * 1px);               left: calc((var(--x_distance) + 390)*var(--x_scale)*1px);               top: calc((var(--y_distance) + 110)*var(--y_scale)*1px);               width: calc(55 * var(--x_scale_little)*1px);               height: calc(30 * var(--y_scale_little)*1px);               z-index: 1;          }          .xh {               letter-spacing: 0;               background-color: var(--sushe-color);               left: calc((var(--x_distance) + 350)*var(--x_scale)*1px);               top: calc((var(--y_distance) + 208)*var(--y_scale)*1px);               width: calc(93 * var(--x_scale_little)*1px);               height: calc(100 * var(--y_scale_little)*1px);               z-index: 1;          }          .x6 {               background-color: var(--sushe-color);               left: calc((460 + var(--x_distance))*var(--x_scale)*1px);               top: calc((130 + var(--y_distance))*var(--y_scale)*1px);               width: calc(45 * var(--x_scale_little) * 1px);               height: calc(100 * var(--y_scale_little) * 1px);               z-index: 1;          }          .x61 {               background-color: var(--sushe-color);               left: calc((505 + var(--x_distance))*var(--x_scale)*1px);               top: calc((130 + var(--y_distance))*var(--y_scale)*1px);               width: calc(65 * var(--x_scale_little) * 1px);               height: calc(37 * var(--y_scale_little) * 1px);               z-index: 1;          }          .x62 {               background-color: var(--sushe-color);               left: calc((505 + var(--x_distance))*var(--x_scale)*1px);               top: calc((195 + var(--y_distance))*var(--y_scale)*1px);               width: calc(35 * var(--x_scale_little) * 1px);               height: calc(35 * var(--y_scale_little) * 1px);               z-index: 1;          }          .xi_hong_dian1 {               background-color: red;               left: calc((500 + var(--x_distance))*var(--x_scale)*1px);               top: calc((165 + var(--y_distance))*var(--y_scale)*1px);               width: calc(17 * var(--x_scale_little) * 1px);               height: calc(17 * var(--y_scale_little) * 1px);               z-index: 1;          }          .zao_tang {               background-color: #7be7ad;               font-size: calc((var(--font_size_first) + 0 )* 1px);               line-height: calc(((--font_size_first) + 0 ) * 1px);               left: calc((458 + var(--x_distance))*var(--x_scale)*1px);               top: calc((245 + var(--y_distance))*var(--y_scale)*1px);               width: calc(28 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);               z-index: 1;          }          .ge_ge_li_fa_dian {               background-color: var(--color-li-fa);               left: calc((486 + var(--x_distance))*var(--x_scale)*1px);               top: calc((245 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_first) + 0 )* 1px);               line-height: calc(((--font_size_first) + 0 ) * 1px);               z-index: 1;          }          .tai_yang_neng_shui_fang {               background-color: #96e3ba;               left: calc((516 + var(--x_distance))*var(--x_scale)*1px);               top: calc((245 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);               font-size: calc(var(--font_size_third) * 1px);               line-height: calc((var(--font_size_third) + 7 ) * 1px); /* 文字间垂直距离 */          }          .guo_lu_fang {               background-color: #78d7db;               left: calc((546 + var(--x_distance))*var(--x_scale)*1px);               top: calc((245 + var(--y_distance))*var(--y_scale)*1px);               width: calc(28 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);               font-size: calc(var(--font_size_third) * 1px);               line-height: calc((var(--font_size_third) + 7 ) * 1px);          }          .jsq {               background-color: var(--jiashuqu-color);               left: calc((597 + var(--x_distance))*var(--x_scale)*1px);               top: calc((129 + var(--y_distance))*var(--y_scale)*1px);               width: calc(215 * var(--x_scale_little) * 1px);               height: calc(69 * var(--y_scale_little) * 1px);          }          .qing_nian_guo_yu {               line-height: 15px;               background-color: #6aacd5;               left: calc((710 + var(--x_distance))*var(--x_scale)*1px);               top: calc((170 + var(--y_distance))*var(--y_scale)*1px);               width: calc(102 * var(--x_scale_little) * 1px);               height: calc(28 * var(--y_scale_little) * 1px);          }          .ke_yan_lou {               background-color: var(--color-jiao_xue_lou);               left: calc((579 + var(--x_distance))*var(--x_scale)*1px);               top: calc((218 + var(--y_distance))*var(--y_scale)*1px);               width: calc(102 * var(--x_scale_little) * 1px);               height: calc(94 * var(--y_scale_little) * 1px);          }          .Jia_shu_qu2 {               background-color: var(--jiashuqu-color);               left: calc((712 + var(--x_distance))*var(--x_scale)*1px);               top: calc((218 + var(--y_distance))*var(--y_scale)*1px);               width: calc(102 * var(--x_scale_little) * 1px);               height: calc(83 * var(--y_scale_little) * 1px);          }          .qngy {               background-color: var(--sushe-color);               left: calc((0 + var(--x_distance))*var(--x_scale)*1px);               top: calc((238 + var(--y_distance))*var(--y_scale)*1px);               width: calc(45 * var(--x_scale_little) * 1px);               height: calc(90 * var(--y_scale_little) * 1px);               line-height: calc((var(--font_size_second) + 0 ) * 1px);          }          .lxs {               background-color: var(--sushe-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((255 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(41 * var(--y_scale_little) * 1px);               line-height: calc((var(--font_size_first) + 0 ) * 1px);          }          .xin_shi_tang {               background-color: var(--color-shi_tang);               left: calc((193 + var(--x_distance))*var(--x_scale)*1px);               top: calc((250 + var(--y_distance))*var(--y_scale)*1px);               width: calc(105 * var(--x_scale_little) * 1px);               height: calc(52 * var(--y_scale_little) * 1px);          }          .x13 {               background-color: var(--sushe-color);               left: calc((0 + var(--x_distance))*var(--x_scale)*1px);               top: calc((344 + var(--y_distance))*var(--y_scale)*1px);               width: calc(47 * var(--x_scale_little) * 1px);               height: calc(168 * var(--y_scale_little) * 1px);          }          .x5 {               background-color: var(--sushe-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((320 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(42 * var(--y_scale_little) * 1px);          }          .cd1 {               background-color: var(--lvse-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((373 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }          .x3 {               background-color: var(--sushe-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((435 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(40 * var(--y_scale_little) * 1px);          }          .x8 {               background-color: var(--sushe-color);               left: calc((192 + var(--x_distance))*var(--x_scale)*1px);               top: calc((320 + var(--y_distance))*var(--y_scale)*1px);               width: calc(108 * var(--x_scale_little) * 1px);               height: calc(42 * var(--y_scale_little) * 1px);          }          .cd2 {               background-color: var(--lvse-color);               left: calc((192 + var(--x_distance))*var(--x_scale)*1px);               top: calc((372 + var(--y_distance))*var(--y_scale)*1px);               width: calc(108 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }          .x4 {               background-color: var(--sushe-color);               left: calc((192 + var(--x_distance))*var(--x_scale)*1px);               top: calc((435 + var(--y_distance))*var(--y_scale)*1px);               width: calc(108 * var(--x_scale_little) * 1px);               height: calc(40 * var(--y_scale_little) * 1px);          }          .cd3 {               background-color: var(--lvse-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((478 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(45 * var(--y_scale_little) * 1px);          }          .cd4 {               background-color: var(--lvse-color);               left: calc((192 + var(--x_distance))*var(--x_scale)*1px);               top: calc((478 + var(--y_distance))*var(--y_scale)*1px);               width: calc(108 * var(--x_scale_little) * 1px);               height: calc(45 * var(--y_scale_little) * 1px);          }          .x1 {               background-color: var(--sushe-color);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((527 + var(--y_distance))*var(--y_scale)*1px);               width: calc(100 * var(--x_scale_little) * 1px);               height: calc(38 * var(--y_scale_little) * 1px);          }          .x2 {               background-color: var(--sushe-color);               left: calc((192 + var(--x_distance))*var(--x_scale)*1px);               top: calc((527 + var(--y_distance))*var(--y_scale)*1px);               width: calc(108 * var(--x_scale_little) * 1px);               height: calc(38 * var(--y_scale_little) * 1px);          }          .kd1 {               background-color: var(--sushe-color);               left: calc((342 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(37 * var(--x_scale_little) * 1px);               height: calc(74 * var(--y_scale_little) * 1px);          }          .mai_dang_lao {               background-color: var(--sushe-color);               left: calc((410 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(25 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_third) + 0 )* 1px);               line-height: calc(((--font_size_third) + 0 ) * 1px);          }          .wu_mei {               background-color: #eeb1b1;               left: calc((435 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(50 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);          }          .mlt {               background-color: #eaf2b6;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((480 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(27 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);          }          .shui_guo_dian {               background-color: #e35770;                    left: calc((507 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(27 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_third) + 0 )* 1px);               line-height: calc(((--font_size_third) + 0 ) * 1px);          }          .da_yin_dian {               background-color: #e26f47;               left: calc((534 + var(--x_distance))*var(--x_scale)*1px);               top: calc((327 + var(--y_distance))*var(--y_scale)*1px);               width: calc(39 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_third) + 0 )* 1px);               line-height: calc(((--font_size_third) - 5 ) * 1px);          }          .jiao_gong_can_ting {               background-color: var(--color-shi_tang);               left: calc((470 + var(--x_distance))*var(--x_scale)*1px);               top: calc((347 + var(--y_distance))*var(--y_scale)*1px);               width: calc(103 * var(--x_scale_little) * 1px);               height: calc(30 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_third) + 4 )* 1px);               line-height: calc(((--font_size_third) + 2 ) * 1px);          }          .xue_sheng_shu_wu {               background-color: #bbc660;               left: calc((490 + var(--x_distance))*var(--x_scale)*1px);               top: calc((377 + var(--y_distance))*var(--y_scale)*1px);               width: calc(83 * var(--x_scale_little) * 1px);               height: calc(22 * var(--y_scale_little) * 1px);          }          .xue_sheng_chu {               background-color: #efbaba;               left: calc((435 + var(--x_distance))*var(--x_scale)*1px);               top: calc((377 + var(--y_distance))*var(--y_scale)*1px);               width: calc(72 * var(--x_scale_little) * 1px);               height: calc(22 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_third) + 0 )* 1px);               line-height: calc(((--font_size_third) + 0 ) * 1px);          }          .lao_shi_tang {               background-color: var(--color-shi_tang);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((598 + var(--x_distance))*var(--x_scale)*1px);               top: calc((337 + var(--y_distance))*var(--y_scale)*1px);               width: calc(80 * var(--x_scale_little) * 1px);               height: calc(92 * var(--y_scale_little) * 1px);          }          .jsq2 {               background-color:  var(--jiashuqu-color);               left: calc((688 + var(--x_distance))*var(--x_scale)*1px);               top: calc((337 + var(--y_distance))*var(--y_scale)*1px);               width: calc(45 * var(--x_scale_little) * 1px);               height: calc(92 * var(--y_scale_little) * 1px);          }          .bwi {               background-color: #a4d45b;               left: calc((748 + var(--x_distance))*var(--x_scale)*1px);               top: calc((317 + var(--y_distance))*var(--y_scale)*1px);               width: calc(65 * var(--x_scale_little) * 1px);               height: calc(112 * var(--y_scale_little) * 1px);          }          .tug {               background-color: var(--sushe-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((457 + var(--x_distance))*var(--x_scale)*1px);               top: calc((427 + var(--y_distance))*var(--y_scale)*1px);               width: calc(112 * var(--x_scale_little) * 1px);               height: calc(140 * var(--y_scale_little) * 1px);          }          .lan_qiu_chang {               background-color: var(--color-sports);               left: calc((598 + var(--x_distance))*var(--x_scale)*1px);               top: calc((453 + var(--y_distance))*var(--y_scale)*1px);               width: calc(80 * var(--x_scale_little) * 1px);               height: calc(112 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_second) - 2 )* 1px);               line-height: calc((var(--font_size_second) + 0 ) * 1px);          }          .wqi {               background-color: #68e37b;               left: calc((678 + var(--x_distance))*var(--x_scale)*1px);               top: calc((453 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(112 * var(--y_scale_little) * 1px);          }          .pqi {               background-color: #67e2d0;               left: calc((708 + var(--x_distance))*var(--x_scale)*1px);               top: calc((453 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(112 * var(--y_scale_little) * 1px);          }          .x29 {               background-color: var(--sushe-color);               left: calc((758 + var(--x_distance))*var(--x_scale)*1px);               top: calc((453 + var(--y_distance))*var(--y_scale)*1px);               width: calc(55 * var(--x_scale_little) * 1px);               height: calc(112 * var(--y_scale_little) * 1px);          }          .ti_yu_guan {               background-color: var(--color-sports);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((707 + var(--x_distance))*var(--x_scale)*1px);               top: calc((600 + var(--y_distance))*var(--y_scale)*1px);               width: calc(77 * var(--x_scale_little) * 1px);               height: calc(126 * var(--y_scale_little) * 1px);          }          .ygr {               background-color: #deb476;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((784 + var(--x_distance))*var(--x_scale)*1px);               top: calc((600 + var(--y_distance))*var(--y_scale)*1px);               width: calc(37 * var(--x_scale_little) * 1px);               height: calc(189 * var(--y_scale_little) * 1px);          }          .cao_chang {               background-color: var(--color-sports);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((714 + var(--x_distance))*var(--x_scale)*1px);               top: calc((805 + var(--y_distance))*var(--y_scale)*1px);               width: calc(110 * var(--x_scale_little) * 1px);               height: calc(259 * var(--y_scale_little) * 1px);          }          .qmjs {               background-color: #ded684;               left: calc((714 + var(--x_distance))*var(--x_scale)*1px);               top: calc((750 + var(--y_distance))*var(--y_scale)*1px);               width: calc(53 * var(--x_scale_little) * 1px);               height: calc(49 * var(--y_scale_little) * 1px);          }          .dks {               background-color: var(--lvse-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((598 + var(--x_distance))*var(--x_scale)*1px);               top: calc((600 + var(--y_distance))*var(--y_scale)*1px);               width: calc(89 * var(--x_scale_little) * 1px);               height: calc(126 * var(--y_scale_little) * 1px);          }          .j1 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((348 + var(--x_distance))*var(--x_scale)*1px);               top: calc((670 + var(--y_distance))*var(--y_scale)*1px);               width: calc(222 * var(--x_scale_little) * 1px);               height: calc(56 * var(--y_scale_little) * 1px);          }          .j11 {               background-color: #cae270;               left: calc((348 + var(--x_distance))*var(--x_scale)*1px);               top: calc((648 + var(--y_distance))*var(--y_scale)*1px);               width: calc(48 * var(--x_scale_little) * 1px);               height: calc(92 * var(--y_scale_little) * 1px);          }          .j12 {               background-color: #cae270;               left: calc((523 + var(--x_distance))*var(--x_scale)*1px);               top: calc((648 + var(--y_distance))*var(--y_scale)*1px);               width: calc(46 * var(--x_scale_little) * 1px);               height: calc(92 * var(--y_scale_little) * 1px);          }          .vl {               background-color: #cae270;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((475 + var(--x_distance))*var(--x_scale)*1px);               top: calc((760 + var(--y_distance))*var(--y_scale)*1px);               width: calc(98 * var(--x_scale_little) * 1px);               height: calc(128 * var(--y_scale_little) * 1px);          }          .kxht {               background-color: #cae270;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((597 + var(--x_distance))*var(--x_scale)*1px);               top: calc((760 + var(--y_distance))*var(--y_scale)*1px);               width: calc(92 * var(--x_scale_little) * 1px);               height: calc(108 * var(--y_scale_little) * 1px);          }          .j2 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((351 + var(--x_distance))*var(--x_scale)*1px);               top: calc((950 + var(--y_distance))*var(--y_scale)*1px);               width: calc(209 * var(--x_scale_little) * 1px);               height: calc(58 * var(--y_scale_little) * 1px);          }          .j21 {               background-color: #cae270;               left: calc((351 + var(--x_distance))*var(--x_scale)*1px);               top: calc((920 + var(--y_distance))*var(--y_scale)*1px);               width: calc(45 * var(--x_scale_little) * 1px);               height: calc(115 * var(--y_scale_little) * 1px);          }          .j22 {               background-color: #cae270;               left: calc((515 + var(--x_distance))*var(--x_scale)*1px);               top: calc((920 + var(--y_distance))*var(--y_scale)*1px);               width: calc(45 * var(--x_scale_little) * 1px);               height: calc(115 * var(--y_scale_little) * 1px);          }          .bm {               background-color: var(--entrance-color);               font-size: calc((var(--font_size_first) + 5) * 1px);               line-height: calc((var(--font_size_first) + 5 ) * 1px);               left: calc((290 + var(--x_distance))*var(--x_scale)*1px);               top: calc((100 + var(--y_distance))*var(--y_scale)*1px);               width: calc(65 * var(--x_scale_little) * 1px);               height: calc(15 * var(--y_scale_little) * 1px);          }          .xi_men {               background-color: var(--entrance-color);               font-size: calc((var(--font_size_first) + 5) * 1px);               line-height: calc((var(--font_size_first) + 5 ) * 1px);               left: calc((0 + var(--x_distance))*var(--x_scale)*1px);               top: calc((782 + var(--y_distance))*var(--y_scale)*1px);               width: calc(20 * var(--x_scale_little) * 1px);               height: calc(90 * var(--y_scale_little) * 1px);          }          .dong_men {               background-color: var(--entrance-color);               font-size: calc((var(--font_size_first) + 5) * 1px);               line-height: calc((var(--font_size_first) + 5 ) * 1px);               left: calc((832 + var(--x_distance))*var(--x_scale)*1px);               top: calc((542 + var(--y_distance))*var(--y_scale)*1px);               width: calc(20 * var(--x_scale_little) * 1px);               height: calc(76 * var(--y_scale_little) * 1px);          }          .j4 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((640 + var(--y_distance))*var(--y_scale)*1px);               width: calc(222 * var(--x_scale_little) * 1px);               height: calc(55 * var(--y_scale_little) * 1px);          }          .j41 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((610 + var(--y_distance))*var(--y_scale)*1px);               width: calc(49 * var(--x_scale_little) * 1px);               height: calc(110 * var(--y_scale_little) * 1px);          }          .j42 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((250 + var(--x_distance))*var(--x_scale)*1px);               top: calc((610 + var(--y_distance))*var(--y_scale)*1px);               width: calc(49 * var(--x_scale_little) * 1px);               height: calc(110 * var(--y_scale_little) * 1px);          }          .j3 {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((75 + var(--x_distance))*var(--x_scale)*1px);               top: calc((920 + var(--y_distance))*var(--y_scale)*1px);               width: calc(225 * var(--x_scale_little) * 1px);               height: calc(110 * var(--y_scale_little) * 1px);          }          .j3_da_yin_dian {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_third) *2 ) * 1px);              font-size: calc(var(--font_size_third) *2* 1px);               left: calc((245 + var(--x_distance))*var(--x_scale)*1px);               top: calc((960 + var(--y_distance))*var(--y_scale)*1px);               width: calc(25 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }          .xyy {               background-color: #63e21a;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((79 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1065 + var(--y_distance))*var(--y_scale)*1px);               width: calc(220 * var(--x_scale_little) * 1px);               height: calc(40 * var(--y_scale_little) * 1px);          }          .zhong_men {               background-color: var(--entrance-color);               font-size: calc((var(--font_size_first) + 5) * 1px);               line-height: calc((var(--font_size_first) + 5 ) * 1px);               left: calc((308 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1090 + var(--y_distance))*var(--y_scale)*1px);               width: calc(60 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);          }          .nan_qu_chao_shi {               background-color: #2abd65;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((191 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1130 + var(--y_distance))*var(--y_scale)*1px);               width: calc(106 * var(--x_scale_little) * 1px);               height: calc(56 * var(--y_scale_little) * 1px);          }          .qmjs2 {               background-color: #2abd65;               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((365 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1132 + var(--y_distance))*var(--y_scale)*1px);               width: calc(240 * var(--x_scale_little) * 1px);               height: calc(36 * var(--y_scale_little) * 1px);          }          .gan_xi_feng_ren_dian {               background-color: #e3d4c4;               line-height: calc((var(--font_size_second)  ) *0.7* 1px);               font-size: calc(var(--font_size_second)*0.7 * 1px);               left: calc((455 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1070 + var(--y_distance))*var(--y_scale)*1px);               width: calc(40 * var(--x_scale_little) * 1px);               height: calc(36 * var(--y_scale_little) * 1px);          }          .bwyey {               background-color: #2abd65;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((610 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1132 + var(--y_distance))*var(--y_scale)*1px);               width: calc(95 * var(--x_scale_little) * 1px);               height: calc(132 * var(--y_scale_little) * 1px);          }          .njuq {               background-color: var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((389 + var(--x_distance))*var(--x_scale)*1px);               top: calc((1059 + var(--y_distance))*var(--y_scale)*1px);               width: calc(300 * var(--x_scale_little) * 1px);               height: calc(48 * var(--y_scale_little) * 1px);          }          .njuq1 {               background-color:  var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((730 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1072 + var(--y_distance))*var(--y_scale)*1px);               width: calc(85 * var(--x_scale_little) * 1px);               height: calc(85 * var(--y_scale_little) * 1px);          }          .lsvw1 {               background-color: var(--lvse-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((75 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((760 + var(--y_distance))*var(--y_scale)*1px);               width: calc(106 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);          }          .lsvw2 {               background-color: var(--lvse-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((200 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((760 + var(--y_distance))*var(--y_scale)*1px);               width: calc(102 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);          }          .lsvw3 {               background-color: var(--lvse-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((75 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((830 + var(--y_distance))*var(--y_scale)*1px);               width: calc(106 * var(--x_scale_little) * 1px);               height: calc(65 * var(--y_scale_little) * 1px);          }          .lsvw4 {               background-color: var(--lvse-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((200 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((830 + var(--y_distance))*var(--y_scale)*1px);               width: calc(102 * var(--x_scale_little) * 1px);               height: calc(65 * var(--y_scale_little) * 1px);          }          .zxx {               background-color: #c3d1ef;               font-size: calc((var(--font_size_first) + 2 )* 1px);               line-height: calc(((--font_size_first) + 2 ) * 1px);               left: calc((235 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((795 + var(--y_distance))*var(--y_scale)*1px);               width: calc(36 * var(--x_scale_little) * 1px);               height: calc(67 * var(--y_scale_little) * 1px);          }          .xiao_xun_shi {               background-color: #b9cef5;               font-size: calc((var(--font_size_first) + 2 )* 1px);               line-height: calc(((--font_size_first) + 2 ) * 1px);               left: calc((175 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((799 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);          }          .ke_xin_wang_luo {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_first) + 5 ) * 1px);               font-size: calc((var(--font_size_first) + 4) * 1px);               left: calc((577 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((920 + var(--y_distance))*var(--y_scale)*1px);               width: calc(113 * var(--x_scale_little) * 1px);               height: calc(115 * var(--y_scale_little) * 1px);          }          .hong_tong_lou {               background-color: var(--color-jiao_xue_lou);               font-size: calc((var(--font_size_second) - 2 )* 1px);               line-height: calc((var(--font_size_second) - 2 ) * 1px);               left: calc(( var(--d))*var(--x_scale)*1px);               top: calc((565 + var(--y_distance))*var(--y_scale)*1px);               width: calc(47 * var(--x_scale_little) * 1px);               height: calc(65 * var(--y_scale_little) * 1px);          }          .xsl {               background-color: palevioletred;               left: calc((356 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((427 + var(--y_distance))*var(--y_scale)*1px);               width: calc(79 * var(--x_scale_little) * 1px);               height: calc(40 * var(--y_scale_little) * 1px);          }          .shi_guang_guang_chang {               background-color: palevioletred;               left: calc((356 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((529 + var(--y_distance))*var(--y_scale)*1px);               width: calc(79 * var(--x_scale_little) * 1px);               height: calc(38 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_second) - 0 )* 1px);               line-height: calc((var(--font_size_second) - 0 ) * 1px);          }          .xing_zheng_ban_gong_lou {               background-color: palevioletred;               /*line-height: calc((var(--font_size_second) + 2 ) * 1px);*/               /*font-size: calc(var(--font_size_second) * 1px);*/               left: calc((430 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((593 + var(--y_distance))*var(--y_scale)*1px);               width: calc(79 * var(--x_scale_little) * 1px);               height: calc(77 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_second) - 5 )* 1px);               line-height: calc(((--font_size_second) - 5 ) * 1px);          }          .cai_wu_chu {               background-color: palevioletred;               left: calc((350 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((593 + var(--y_distance))*var(--y_scale)*1px);               width: calc(70 * var(--x_scale_little) * 1px);               height: calc(25 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_second) - 7 )* 1px);               line-height: calc(((--font_size_second) + 2 ) * 1px);          }          .hou_qin_lou {               background-color: #a48ce8;               left: calc((350 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((617 + var(--y_distance))*var(--y_scale)*1px);               width: calc(70 * var(--x_scale_little) * 1px);               height: calc(25 * var(--y_scale_little) * 1px);               font-size: calc((var(--font_size_second) - 7 )* 1px);               line-height: calc(((--font_size_second) + 2 ) * 1px);          }          .bj {               background-color: #5dd0db;               left: calc((580 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1278 + var(--y_distance))*var(--y_scale)*1px);               width: calc(155 * var(--x_scale_little) * 1px);               height: calc(54 * var(--y_scale_little) * 1px);               line-height: calc((var(--font_size_second) - 1 ) * 1px);               font-size: calc((var(--font_size_second) - 1 )  * 1px);          }          .jsq3 {               background-color:  var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((460 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1278 + var(--y_distance))*var(--y_scale)*1px);               width: calc(115 * var(--x_scale_little) * 1px);               height: calc(54 * var(--y_scale_little) * 1px);          }          .jsq4 {               background-color:  var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);               font-size: calc(var(--font_size_second) * 1px);               left: calc((365 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1174 + var(--y_distance))*var(--y_scale)*1px);               width: calc(240 * var(--x_scale_little) * 1px);               height: calc(89 * var(--y_scale_little) * 1px);          }          .jsq5 {               background-color:  var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((80 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1180 + var(--y_distance))*var(--y_scale)*1px);               width: calc(175 * var(--x_scale_little) * 1px);               height: calc(78 * var(--y_scale_little) * 1px);          }          .sd {               background-color: #7e8def;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((255 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1180 + var(--y_distance))*var(--y_scale)*1px);               width: calc(43 * var(--x_scale_little) * 1px);               height: calc(78 * var(--y_scale_little) * 1px);          }          .xue_yuan_li_fa {               background-color: #e4f5bf;               line-height: calc((var(--font_size_third) + 5 ) * 1px);              font-size: calc(var(--font_size_third) * 2px);               left: calc((255 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1200 + var(--y_distance))*var(--y_scale)*1px);               width: calc(43 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);          }          .nan_men_shuo_guo {               background-color: #e3d6d3;               line-height: calc((var(--font_size_third) + 5 ) * 1px);              font-size: calc(var(--font_size_third) * 2px);               left: calc((255 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1183 + var(--y_distance))*var(--y_scale)*1px);               width: calc(43 * var(--x_scale_little) * 1px);               height: calc(20 * var(--y_scale_little) * 1px);          }          .jsq6 {               background-color: var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);              font-size: calc(var(--font_size_second) * 1px);               left: calc((80 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1265 + var(--y_distance))*var(--y_scale)*1px);               width: calc(183 * var(--x_scale_little) * 1px);               height: calc(90 * var(--y_scale_little) * 1px);          }          .jsq7 {               background-color: var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((260 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1305 + var(--y_distance))*var(--y_scale)*1px);               width: calc(97 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }          .hcp {               background-color: #765de7;               line-height: calc((var(--font_size_second) + 2 ) * 1px);              font-size: calc(var(--font_size_second) * 1px);               left: calc((362 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1305 + var(--y_distance))*var(--y_scale)*1px);               width: calc(52 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }           .nan_men {                background-color: var(--entrance-color);                font-size: calc((var(--font_size_first) + 0) * 1px);                line-height: calc((var(--font_size_first) - 5 ) * 1px);                left: calc((420 + var(--x_distance)) * var(--x_scale) * 1px);                top: calc((1345 + var(--y_distance))*var(--y_scale)*1px);                width: calc(60 * var(--x_scale_little) * 1px);                height: calc(20 * var(--y_scale_little) * 1px);          }          .ming_guang_lou {               background-color: var(--color-jiao_xue_lou);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((743 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1178 + var(--y_distance))*var(--y_scale)*1px);               width: calc(70 * var(--x_scale_little) * 1px);               height: calc(183 * var(--y_scale_little) * 1px);          }          .jsq8 {               background-color:  var(--jiashuqu-color);               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc(( var(--d))*var(--x_scale)*1px);               top: calc((1065 + var(--y_distance))*var(--y_scale)*1px);               width: calc(68 * var(--x_scale_little) * 1px);               height: calc(290 * var(--y_scale_little) * 1px);          }          .shang_pu {               background-color: #eebcf6;               line-height: calc((var(--font_size_second) + 2 ) * 1px);font-size: calc(var(--font_size_second) * 1px);               left: calc((80 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1130 + var(--y_distance))*var(--y_scale)*1px);               width: calc(110 * var(--x_scale_little) * 1px);               height: calc(50 * var(--y_scale_little) * 1px);          }          .sheng_shi_hao {               background-color: var(--color-li-fa);               line-height: calc((var(--font_size_second) *0.7 ) * 1px);              font-size: calc(var(--font_size_second) *0.6* 1px);               left: calc((40 + var(--x_distance)) * var(--x_scale) * 1px);               top: calc((1120 + var(--y_distance))*var(--y_scale)*1px);               width: calc(30 * var(--x_scale_little) * 1px);               height: calc(60 * var(--y_scale_little) * 1px);          }     </style></head><body>     <p style="color: white;font-size:500px;line-height: 1.0">.</p>     <p style="color: white;font-size:500px;line-height: 1.0">.</p>     <div class="box beiyoukeji"><p style="color: black">北邮科技酒店</p></div>     <div class="box xue11"><p style="color: black">学11</p></div>     <div class="box xue9"><p style="color: black">学9</p></div>     <div class="box j9"><p style="color: black">教9</p></div>     <div class="box x10"><p style="color: black">学10</p></div>     <div class="box jg"><p style="color: black">经管楼</p></div>     <div class="box xh"><p style="color: black">学生活动中心</p></div>     <div class="box x6"><p style="color: black">学6</p></div>     <div class="box x61"><p style="color: black"></p></div>     <div class="box x62"><p style="color: black"></p></div>     <button class="box xi_hong_dian1" onclick="openModal('modal6')">洗烘店</button>     <div id="modal6" class="modal"><div class="modal-content">         <span class="close" onclick="closeModal('modal6')">&times;&times;&times;</span>         <p>24小时开放</p>         <a href="https://www.bupt.life/hong_xi_dian">具体详情</a>     </div></div>     <button class="box zao_tang" onclick="openModal('moda20')">澡堂</button>     <div id="moda20" class="modal"><div class="modal-content">         <span class="close" onclick="closeModal('moda20')">&times;&times;&times;</span>         <p>澡堂开放:15:00-23:30</p></div></div>     <button class="box ge_ge_li_fa_dian" onclick="openModal('modal19')">理发店</button>     <div id="modal19" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal19')">&times;&times;&times;</span>          <p>格格理发店 营业时间 08:00-20:00</p></div></div>     <div class="box tai_yang_neng_shui_fang"><a style="color: black" href="#" title="开放:08:00-22:00">水房</a></div>     <div class="box guo_lu_fang"><p style="color: black">锅炉房</p></div>     <div class="box jsq"><p style="color: black">家属区</p></div>     <div class="box qing_nian_guo_yu"><p style="color: black"></p></div>     <div class="box ke_yan_lou"><p style="color: black">科研楼</p></div>     <div class="box Jia_shu_qu2"><p style="color: black">家属区</p></div>     <div class="box qngy"><p style="color: black">青年公寓</p></div>     <div class="box lxs"><p style="color: black">留学生公寓</p></div>     <div class="box xin_shi_tang"><p style="color: black">新食堂</p></div>     <button class="box xin_shi_tang" onclick="openModal('modal7')">新食堂</button><div id="modal7" class="modal"><div class="modal-content">    <span class="close" onclick="closeModal('modal7')">&times;&times;&times;</span>    <p>新食堂：待添加</p></div></div>     <div class="box x13"><p style="color: black">学13</p></div>     <div class="box x5"><p style="color: black">学5</p></div>     <div class="box cd1"><p style="color: black">草地</p></div>     <div class="box x3"><p style="color: black">学3</p></div>     <div class="box x8"><p style="color: black">学8</p></div>     <div class="box cd2"><p style="color: black">草地</p></div>     <div class="box x4"><p style="color: black">学4</p></div>     <div class="box cd3"><p style="color: black">草地</p></div>     <div class="box cd4"><p style="color: black">草地</p></div>     <div class="box x1"><p style="color: black">学1</p></div>     <div class="box x2"><p style="color: black">学2</p></div>     <div class="box kd1"><p style="color: black"></p></div>     <button class="box mai_dang_lao" onclick="openModal('modal8')">麦当劳</button>     <div id="modal8" class="modal"><div class="modal-content">         <span class="close" onclick="closeModal('modal8')">&times;&times;&times;</span>         <p>麦当劳 拟开业</p></div></div>     <button class="box wu_mei" onclick="openModal('modal9')">物美</button>     <div id="modal9" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal9')">&times;&times;&times;</span>          <p>物美 待添加</p></div></div>     <div class="box mlt"><p style="color: black"></p></div>     <div class="box shui_guo_dian"><a style="color: black" href="#" title="开放:14:00-16:00">水果店</a></div>     <div class="box da_yin_dian"><p style="color: black">打印店</p></div>     <button class="box jiao_gong_can_ting" onclick="openModal('modal10')">教工餐厅</button>     <div id="modal10" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal10')">&times;&times;&times;</span>          <p>教工餐厅开放:08:00-21:00</p></div></div>     <div class="box xue_sheng_shu_wu"><p style="color: black"></p></div> <!-- 学苑书屋-->     <div class="box xue_sheng_chu"><p style="color: black">学生处</p></div>     <button class="box lao_shi_tang" onclick="openModal('modal11')">老食堂</button>     <div id="modal11" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal11')">&times;&times;&times;</span>          <p>老食堂开放:08:00-21:00</p></div></div>     <div class="box jsq2"><p style="color: black">家属区</p></div>     <div class="box bwi"><p style="color: black">保卫处&基建处</p></div>     <button class="box tug" onclick="openModal('modal3')">图书馆</button>     <div id="modal3" class="modal">          <div class="modal-content">               <span class="close" onclick="closeModal('modal3')">&times;&times;&times;</span>               <p>图书馆放时间:08:00-21:00</p></div></div>     <button class="box lan_qiu_chang" onclick="openModal('modal1')">篮球场</button>     <div id="modal1" class="modal">          <div class="modal-content">               <span class="close" onclick="closeModal('modal1')">&times;&times;&times;</span>               <p>篮球场开放时间</p>               <a href="https://www.bupt.life/lan_qiu_chang">具体详情</a>               <p>08:00-21:00</p></div></div>     <div class="box wqi"><p style="color: black">网球场</p></div>     <div class="box pqi"><p style="color: black">排球场</p></div>     <div class="box x29"><p style="color: black">学29</p></div><!--     <div class="box ti_yu_guan"><p style="color: black">体育馆</p></div>-->     <button class="box ti_yu_guan" onclick="openModal('modal12')">体育馆</button>     <div id="modal12" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal12')">&times;&times;&times;</span>          <p>体育馆 待添加</p></div></div>     <button class="box ygr" onclick="openModal('modal13')">游泳馆</button>     <div id="modal13" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal13')">&times;&times;&times;</span>          <p>游泳馆 待添加</p></div></div>     <button class="box cao_chang" onclick="openModal('modal2')">操场</button>     <div id="modal2" class="modal">          <div class="modal-content">               <span class="close" onclick="closeModal('modal2')">&times;&times;&times;</span>               <p>操场开放时间:08:00-21:00</p></div></div>     <div class="box qmjs"><p style="color: black">全民健身</p></div>     <div class="box dks"><p style="color: black">大空地</p></div>     <div class="box j1"><p style="color: black">教一楼</p></div>     <div class="box j11"><p style="color: black"></p></div>     <div class="box j12"><p style="color: black"></p></div>     <div class="box vl"><p style="color: black">主楼</p></div>     <div class="box kxht"><p style="color: black">科学会堂</p></div>     <div class="box j2"><p style="color: black">教二楼</p></div>     <div class="box j21"><p style="color: black"></p></div>     <div class="box j22"><p style="color: black"></p></div>     <button class="box bm" onclick="openModal('modal5')">北门</button>     <div id="modal5" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal5')">&times;&times;&times;</span>          <p>北门已开</p>          <p>开放时间：06:00 - 24:00</p>         <a href="https://cdn.staticaly.com/gh/yangmulao/blogcdn@master/img/20230331191350.png">北门最新图片</a>     </div></div>     <button class="box xi_men" onclick="openModal('modal4')">西门</button>     <div id="modal4" class="modal">          <div class="modal-content">               <span class="close" onclick="closeModal('modal4')">&times;&times;&times;</span>               <p>西门开放时间:早8:00 - 晚11:00</p></div></div>     <button class="box dong_men" onclick="openModal('modal14')">东门</button>     <div id="modal14" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal14')">&times;&times;&times;</span>          <p>东门 24小时开放</p>     </div></div>     <div class="box j4"><p style="color: black">教四楼</p></div>     <div class="box j41"><p style="color: black"></p></div>     <div class="box j42"><p style="color: black"></p></div>     <div class="box j3"><p style="color: black">教三楼</p></div>      <button class="box j3_da_yin_dian" onclick="openModal('modal20')">打印店</button>     <div id="modal20" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal20')">&times;&times;&times;</span>          <p>教三 打印店</p>          <p>营业时间:早8:00 - 晚9:30，节假日不休</p>          <p>价格和其他家一样</p>     </div></div>     <button class="box xyy" onclick="openModal('modal15')">校医院</button>     <div id="modal15" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal15')">&times;&times;&times;</span>          <p>校医院 待添加</p></div></div>     <button class="box zhong_men" onclick="openModal('modal16')">中门</button>     <div id="modal16" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal16')">&times;&times;&times;</span>          <p>中门开放时间:早8:00 - 晚11:00</p></div></div>     <div class="box nan_qu_chao_shi"><p style="color: black">超市</p></div>     <div class="box qmjs2"><p style="color: black"></p></div>  <!-- 家属南区 全民健身-->               <div class="box bwyey"><p style="color: black">北邮幼儿园</p></div>     <div class="box njuq"><p style="color: black">家属区</p></div>  <!-- 家属区教二下边-->     <div class="box njuq1"><p style="color: black">家属区</p></div>  <!-- 家属区操场下面-->     <div class="box lsvw1"><p style="color: black"></p></div>     <div class="box lsvw2"><p style="color: black"></p></div>     <div class="box lsvw3"><p style="color: black"></p></div>     <div class="box lsvw4"><p style="color: black"></p></div>     <div class="box zxx"><p style="color: black">主席像</p></div>     <div class="box xiao_xun_shi"><p style="color: black">校训石</p></div>     <div class="box ke_xin_wang_luo"><p style="color: black">可信网络通信协同创新中心</p></div>     <div class="box hong_tong_lou"><p style="color: black">鸿通楼</p></div>     <div class="box xsl"><p style="color: black">小松林</p></div>     <div class="box shi_guang_guang_chang"><p style="color: black">时光广场</p></div>     <div class="box xing_zheng_ban_gong_lou"><p style="color: black">行政办公楼</p></div>     <div class="box cai_wu_chu"><p style="color: black">财务处</p></div>     <div class="box hou_qin_lou"><p style="color: black">后勤楼</p></div>     <div class="box bj"><p style="color: black">北交附属中学</p></div>     <div class="box jsq3"><p style="color: black">家属区</p></div>     <div class="box jsq4"><p style="color: black">家属区</p></div>     <div class="box jsq5"><p style="color: black">家属区</p></div>  <!-- 家属区商铺下面-->     <div class="box sd"><p style="color: black"></p></div>     <button class="box xue_yuan_li_fa" onclick="openModal('modal22')">理发</button>     <div id="modal22" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal22')">&times;&times;&times;</span>     <p>学苑造型理发店 营业时间：早8:00 - 晚10:00</p><p>男士35，女士40</p>     </div></div>      <button class="box nan_men_shuo_guo" onclick="openModal('modal23')">水果</button>     <div id="modal23" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal23')">&times;&times;&times;</span>        <p>北邮南区便民菜站</p>        <p>营业时间：早6:00-晚10:00</p>        <p>卖应季水果和日常水果</p>         <a href="https://www.bupt.life/nan_men_shui_guo">具体详情</a>     </div></div>     <button class="box gan_xi_feng_ren_dian" onclick="openModal('modal21')">干洗店</button>     <div id="modal21" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal21')">&times;&times;&times;</span>          <p>可缝纫 干洗 换拉链 修电瓶车、自行车</p>          <a href="https://www.bupt.life/nan_men_gan_xi_dian">具体详情</a>     </div></div>     <div class="box jsq6"><p style="color: black"></p></div>  <!-- 家属区商铺下下面1-->     <div class="box jsq7"><p style="color: black"></p></div>  <!-- 家属区南门左边2-->     <div class="box hcp"><p style="color: black"></p></div>     <div class="box nan_men"><p style="color: black">南门</p></div>     <div class="box ming_guang_lou"><p style="color: black">明光楼</p></div>     <div class="box jsq8"><p style="color: black"></p></div>   <!-- 左下边家属区-->     <div class="box shang_pu"><p style="color: black">商铺</p></div>     <button class="box sheng_shi_hao" onclick="openModal('modal18')">圣士豪</button>     <div id="modal18" class="modal"><div class="modal-content">          <span class="close" onclick="closeModal('modal18')">&times;&times;&times;</span>          <p>圣士豪理发店 营业时间：早10:00 - 晚9:30</p>            <a href="https://www.bupt.life/sheng_shi_hao">具体详情</a>     </div></div>     <button class="box zhong_you" onclick="openModal('modal24')">快递</button>     <div id="modal24" class="modal">          <div class="modal-content">               <span class="close" onclick="closeModal('modal24')">&times;&times;&times;</span>               <p>中邮快递</p> <p>营业时间:早8:00-晚8:00</p>          </div></div>     <script>          // 1打开弹窗          function openModal(modalId) { var modal = document.getElementById(modalId); modal.style.display = "block"; }          // 2关闭弹窗          function closeModal(modalId) { var modal = document.getElementById(modalId); modal.style.display = "none"; }          // 3点击模态框外部区域也可以关闭弹窗          window.onclick = function (event) {               var modals = document.getElementsByClassName("modal");               for (var i = 0; i < modals.length; i++) { if (event.target === modals[i]) { modals[i].style.display = "none"; } }          }     </script></body></html>]]></content>
      
      
      
        <tags>
            
            <tag> 北邮地图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自己的文章</title>
      <link href="/2024/04/my_paper.html"/>
      <url>/2024/04/my_paper.html</url>
      
        <content type="html"><![CDATA[<h1 id="自己的文章"><a href="#自己的文章" class="headerlink" title="自己的文章"></a>自己的文章</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[1] Wang X, Chen S, Liu Y, et al. Design of asymmetric-structured metasurfaces for smart windows[J]. Results in Materials, 2023, 17: 100363.</span><br><span class="line">[2] Wang X, Chen S, Liu Y, et al. Design of independent and radiation modulation enhanced electrochromic windows in visible and infrared ranges[J]. Optics &amp; Laser Technology, 2023, 167: 109774.</span><br><span class="line">[3] Wang X, Chen S, Chen L, et al. A hybrid MLP-CNN model based on positional encoding for daytime radiative cooler[J]. Optics Communications, 2024, 560: 130448.</span><br><span class="line">[4] Wang X, Liu Y. Design of Electrochromic Asymmetric Multilayered Structure for Smart Windows[C]//2022 Asia Communications and Photonics Conference (ACP). IEEE, 2022: 2107-2110.</span><br></pre></td></tr></table></figure><h2 id="参加的项目"><a href="#参加的项目" class="headerlink" title="参加的项目"></a>参加的项目</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">科研启动项目：超构表面的热管理调控的理论和实验研究</span><br><span class="line">校级项目：面向智能窗热管理的应用研究</span><br></pre></td></tr></table></figure><h2 id="1-Result-in-Materials"><a href="#1-Result-in-Materials" class="headerlink" title="1 Result in Materials"></a>1 Result in Materials</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Design of asymmetric-structured metasurfaces <span class="keyword">for</span> smart windows</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.sciencedirect.com/science/article/pii/S2590048X23000018?via%3Dihub</span><br></pre></td></tr></table></figure><div>  <iframe src="/pdf/1. Result in Materials 王学玉.pdf" width="100%" height="500px" frameborder="0"></iframe></div> <h2 id="2-Optics-Laser-Technology"><a href="#2-Optics-Laser-Technology" class="headerlink" title="2 Optics &amp; Laser Technology"></a>2 Optics &amp; Laser Technology</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Design of independent <span class="keyword">and</span> radiation modulation enhanced electrochromic windows <span class="keyword">in</span> visible <span class="keyword">and</span> infrared ranges</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.sciencedirect.com/science/article/<span class="built_in">abs</span>/pii/S0030399223006679</span><br></pre></td></tr></table></figure><div>  <iframe src="/pdf/2. OLT 王学玉.pdf" width="100%" height="500px" frameborder="0"></iframe></div> <h2 id="3-Optics-Communications"><a href="#3-Optics-Communications" class="headerlink" title="3 Optics Communications"></a>3 Optics Communications</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A hybrid MLP-CNN model based on positional encoding <span class="keyword">for</span> daytime radiative cooler</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.sciencedirect.com/science/article/pii/S0030401824001858?dgcid=coauthor</span><br></pre></td></tr></table></figure><div>  <iframe src="/pdf/3. OC 王学玉.pdf" width="100%" height="500px" frameborder="0"></iframe></div> <h2 id="4-ACP"><a href="#4-ACP" class="headerlink" title="4 ACP"></a>4 ACP</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Design of Electrochromic Asymmetric Multilayered Structure <span class="keyword">for</span> Smart Windows</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2022</span> Asia Communications <span class="keyword">and</span> Photonics Conference (ACP)</span><br><span class="line">https://ieeexplore.ieee.org/abstract/document/<span class="number">10088484</span></span><br></pre></td></tr></table></figure><div>  <iframe src="/pdf/4. ACP 王学玉.pdf" width="100%" height="500px" frameborder="0"></iframe></div> ]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FDTD 相关</title>
      <link href="/2023/03/f386.html"/>
      <url>/2023/03/f386.html</url>
      
        <content type="html"><![CDATA[<h2 id="常用代码"><a href="#常用代码" class="headerlink" title="常用代码"></a>常用代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#mesh1</span></span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,<span class="number">0</span>);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;x span&#x27;</span>,y2);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,<span class="number">0</span>);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;y span&#x27;</span>,y2);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;z max&#x27;</span>,<span class="number">0</span>-h_1);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;z min&#x27;</span>,-h1-h2-h3-h_1);</span><br><span class="line"></span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;dx&#x27;</span>,mesh_a);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;dy&#x27;</span>,mesh_a);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh1&#x27;</span>,<span class="string">&#x27;dz&#x27;</span>,mesh_z);</span><br><span class="line"></span><br><span class="line"><span class="comment">#mesh2</span></span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,<span class="number">0</span>);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;x span&#x27;</span>,w);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,<span class="number">0</span>);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;y span&#x27;</span>,w);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;z min&#x27;</span>,-t_sub+h_2);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;z max&#x27;</span>,-t_sub+h6+h5+h4+h_2);</span><br><span class="line"></span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;dx&#x27;</span>,mesh_a);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;dy&#x27;</span>,mesh_a);</span><br><span class="line">setnamed(<span class="string">&#x27;mesh2&#x27;</span>,<span class="string">&#x27;dz&#x27;</span>,mesh_z);</span><br></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">deleteall;</span><br><span class="line">addrect;</span><br><span class="line">set(<span class="string">&quot;x&quot;</span>,<span class="number">0</span>);</span><br><span class="line">set(<span class="string">&quot;x span&quot;</span>,(h1) * <span class="number">2</span>* scale);</span><br><span class="line">set(<span class="string">&quot;y max&quot;</span>,<span class="number">0</span>); # y</span><br><span class="line">set(<span class="string">&quot;y min&quot;</span>, -thick); </span><br><span class="line">set(<span class="string">&quot;material&quot;</span>, material); set(<span class="string">&quot;index&quot;</span>, nk);</span><br><span class="line"></span><br><span class="line">w = [<span class="number">50</span>,    <span class="number">14</span>,    <span class="number">12</span>,    <span class="number">68</span>,    <span class="number">38</span>,    <span class="number">74</span>,    <span class="number">20</span>,    <span class="number">52</span>,    <span class="number">32</span>,    <span class="number">38</span>,     <span class="number">2</span>]*<span class="number">1e-9</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ( <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">1</span>:<span class="number">4</span> )</span><br><span class="line">&#123;</span><br><span class="line">    addrect;    </span><br><span class="line">    set(<span class="string">&quot;x&quot;</span>,(sum(w(<span class="number">1</span>:<span class="number">2</span>*<span class="built_in">i</span>))+w(<span class="number">2</span>*<span class="built_in">i</span>+<span class="number">1</span>)/<span class="number">2</span>)*scale);</span><br><span class="line">    set(<span class="string">&quot;x span&quot;</span>,w(<span class="number">2</span>*<span class="built_in">i</span>+<span class="number">1</span>)*scale);</span><br><span class="line">    set(<span class="string">&quot;y max&quot;</span>,<span class="number">0</span>); set(<span class="string">&quot;y min&quot;</span>, -thick); </span><br><span class="line">    set(<span class="string">&quot;material&quot;</span>, material); set(<span class="string">&quot;index&quot;</span>, nk);</span><br><span class="line">    #set(<span class="string">&quot;x&quot;</span>,(h1+h2+h3/<span class="number">2</span>)*scale);     </span><br><span class="line">    addrect;</span><br><span class="line">    set(<span class="string">&quot;x&quot;</span>,-(sum(w(<span class="number">1</span>:<span class="number">2</span>*<span class="built_in">i</span>))+w(<span class="number">2</span>*<span class="built_in">i</span>+<span class="number">1</span>)/<span class="number">2</span>)*scale);</span><br><span class="line">    set(<span class="string">&quot;x span&quot;</span>,w(<span class="number">2</span>*<span class="built_in">i</span>+<span class="number">1</span>)*scale);</span><br><span class="line">    set(<span class="string">&quot;y max&quot;</span>,<span class="number">0</span>); set(<span class="string">&quot;y min&quot;</span>, -thick); </span><br><span class="line">    set(<span class="string">&quot;material&quot;</span>, material); set(<span class="string">&quot;index&quot;</span>, nk);  </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.staticaly.com/gh/yangmulao/blogcdn@master/img/image-20230506110907422.png" alt="image-20230506110907422"></p><h2 id="绘制图形"><a href="#绘制图形" class="headerlink" title="绘制图形"></a>绘制图形</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot_func(<span class="number">3</span>,[<span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">0</span>],  <span class="number">1</span>:length(field(<span class="number">1</span>,:)),<span class="number">1</span>:length(field(:,<span class="number">1</span>)), field, <span class="number">15</span>,<span class="number">1.5</span>,  <span class="number">100</span>,<span class="number">100</span>,<span class="number">1000</span>,<span class="number">400</span>,<span class="string">&#x27;X&#x27;</span>,<span class="string">&#x27;Z&#x27;</span>,<span class="string">&#x27;|Ez|^2 &amp; FOM&#x27;</span>);</span><br><span class="line"><span class="built_in">set</span>(gca, <span class="string">&#x27;YDir&#x27;</span>, <span class="string">&#x27;reverse&#x27;</span>); </span><br><span class="line">title(sprintf(<span class="string">&#x27;|Ez|^2 &amp; FOM = %4f&#x27;</span>, -FOM ));</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> FDTD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习总结</title>
      <link href="/2023/03/40131.html"/>
      <url>/2023/03/40131.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>I:\PyTorch深度学习实践</p></blockquote><h3 id="经典代码"><a href="#经典代码" class="headerlink" title="经典代码"></a>经典代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> optimizer</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># from main_09 import criterion</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># Important</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)  <span class="comment"># Important</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;device=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(device))  <span class="comment"># Important</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])  <span class="comment"># 均值 标准差</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;mnist&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;mnist&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Flatten data from (n, 1, 28, 28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># flatten</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct criterion and optimizer</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)  <span class="comment"># 冲量0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward + backward + update</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            inputs, target = data</span><br><span class="line">            inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %% [%d/%d]&#x27;</span> % (<span class="number">100</span> * correct / total, correct, total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            test()</span><br><span class="line">            <span class="keyword">import</span> time</span><br><span class="line">            end_time = time.time()  <span class="comment"># calculate time</span></span><br><span class="line">            <span class="keyword">if</span> end_time - start_time &gt;= <span class="number">3600</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; h&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time) / <span class="number">3600</span>, <span class="number">2</span>)))</span><br><span class="line">            <span class="keyword">elif</span> end_time - start_time &gt;= <span class="number">60</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; min&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time) / <span class="number">60</span>, <span class="number">2</span>)))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; s&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>((end_time - start_time), <span class="number">1</span>)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 总结 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
